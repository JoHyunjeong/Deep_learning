{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8affb2db",
   "metadata": {},
   "source": [
    "# CHAPTER 1 신경망 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53999d49",
   "metadata": {},
   "source": [
    "## 1.1 수학과 파이썬 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b65721",
   "metadata": {},
   "source": [
    "### 1.1.1 벡터와 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1d3d8",
   "metadata": {},
   "source": [
    "- 벡터 : 크기와 방향을 가진 양  \n",
    "    벡터는 숫자가 일렬로 늘어선 집합으로 표현할 수 있음  \n",
    "    파이썬에서는 1차원 배열로 취급  \n",
    "<br> \n",
    "\n",
    "- 행렬 : 숫자가 2차원 형태(사각형 형상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644083d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "x.__class__ # 클래스 이름 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01e3ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 형상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1577d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim # 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf21d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb08a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94ca9b",
   "metadata": {},
   "source": [
    "### 1.1.2 행렬의 원소별 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecb0a0",
   "metadata": {},
   "source": [
    "원소별(element-wise) 연산 : 서로 대응하는 원소끼리 (각 원소가 독립적으로) 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d79935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5],\n",
       "       [ 7,  9, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "X = np.array([[0,1,2],[3,4,5]])\n",
    "W + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5eea5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  6],\n",
       "       [12, 20, 30]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W * X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c66fa",
   "metadata": {},
   "source": [
    "### 1.1.3 브로드캐스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab3b16",
   "metadata": {},
   "source": [
    "<img src='./img/1/broadcast.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778cea1",
   "metadata": {},
   "source": [
    "배열의 형상이 달라도 자동적으로 확장하여 형상이 다른 배열끼리의 연산을 수행할 수 있게 하는 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a410f",
   "metadata": {},
   "source": [
    "### 1.1.4 벡터의 내적과 행렬의 곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e13a9d",
   "metadata": {},
   "source": [
    "- 벡터의 내적 : 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것  \n",
    "<br>\n",
    "\n",
    "<center>$x\\cdot y=x_1y_1+x_2y_2+\\cdots +x_ny_n$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd04eb",
   "metadata": {},
   "source": [
    "- 행렬의 곱 : 왼쪽 행렬의 행벡터(가로 방향)와 오른쪽 행렬의 열벡터(세로 방향)의 내적(원소별 곱의 합)으로 계산  \n",
    "    계산 결과는 새로운 행렬의 대응하는 원소에 저장됨  \n",
    "<br>\n",
    "<img src='./img/1/matrix_product_1.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51038964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7e3d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬의 곱\n",
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[5,6],[7,8]])\n",
    "np.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76593a7",
   "metadata": {},
   "source": [
    "### 1.1.5 행렬 형상 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9798a8",
   "metadata": {},
   "source": [
    "행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜야 함  \n",
    "<br>\n",
    "<img src='./img/1/matrix_product_2.png' width=300>  \n",
    "<br>\n",
    "행렬 C의 형상은 A의 행 수와 B의 열 수가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba82e6",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdaa352",
   "metadata": {},
   "source": [
    "### 1.2.1 신경망 추론 전체 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fa07e",
   "metadata": {},
   "source": [
    "ex) 입력층에 뉴런 2개, 은닉층에 뉴런 4개, 출력층에 뉴런 3개를 둔 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efec5e",
   "metadata": {},
   "source": [
    "<img src='./img/1/example.png' width=300>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f167",
   "metadata": {},
   "source": [
    "가중치와 뉴런의 값을 각각 곱해서 그 합이 다음 뉴런의 입력이 됨  \n",
    "(정확하게는 그 합에 활성화 함수(activation function)을 적용한 값이 다음 뉴런의 입력이 됨)  \n",
    "또, 이때 각 층에서는 이전 뉴런의 값에 영향받지 않은 '정수'인 편향(bias)도 더해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42923c2",
   "metadata": {},
   "source": [
    "입력층의 데이터를 $(x_1, x_2)$, 가중치는 $(w_{11}, w_{21})$, 편향은 $b_1$으로 쓴다면,  \n",
    "은닉층 중 첫 번째 뉴런 $h_1$는 다음과 같이 계산할 수 있다.  \n",
    "  \n",
    "$h_1=x_1w_{11}+x_2w_{21}+b_1$  \n",
    "  \n",
    "위와 같이 은닉층의 뉴런은 가중치의 합으로 계산됨  \n",
    "이 값은 행렬의 곱으로 한꺼번에 계산할 수 있음  \n",
    "  \n",
    "  \n",
    "$(h_1,h_2,h_3,h_4)=(x_1,x_2)\\begin{pmatrix}w_{11}&w_{12}&w_{13}&w_{14} \\\\ w_{21}&w_{22}&w_{23}&w_{24} \\end{pmatrix}+(b_1,b_2,b_3,b_4)$  \n",
    "  \n",
    "$h=xW+b$  \n",
    "  \n",
    "- x : 입력(1,2)\n",
    "- h : 은닉층의 뉴런(1,4)\n",
    "- W : 가중치(2,4)\n",
    "- b : 편향(4,)  \n",
    "  \n",
    "\\* 여기서, 편향 b1은 브로드캐스트 되어 더해짐\n",
    "  \n",
    "미니배치로 한번에 N개 데이터를 처리하는 경우   \n",
    "  \n",
    "- x : 입력(N,2)\n",
    "- h : 은닉층의 뉴런(N,4)\n",
    "- W : 가중치(2,4)\n",
    "- b : 편향(4,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a5f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W1 = np.random.randn(2,4) # 가중치\n",
    "b1 = np.random.randn(4) # 편향\n",
    "x = np.random.randn(10,2) # 입력\n",
    "h = np.matmul(x, W1) + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b0967",
   "metadata": {},
   "source": [
    "완전연결계층에 의한 변환은 '선형' 변환  \n",
    "여기에 '비선형'효과를 부여하는 것이 활성화 함수  \n",
    "비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있음  \n",
    "  \n",
    "ex) 시그모이드 함수(sigmoid function)  \n",
    "  \n",
    "$\\sigma(x)=\\frac{1}{1+exp(-x)}$  \n",
    "  \n",
    "시그모이드 함수는 알파벳 'S'자 모양의 곡선 함수  \n",
    "임의의 실수를 입력받아 0에서 1 사이의 실수를 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e39f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9192d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수로 비선형 변환\n",
    "a = sigmoid(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea797e",
   "metadata": {},
   "source": [
    "활성화 함수의 출력을 활성화(activation)이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16dd6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10,2)\n",
    "W1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4,3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.matmul(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.matmul(a, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1b59f",
   "metadata": {},
   "source": [
    "위 신경망 예시는 3차원 데이터를 출력합  \n",
    "각 차원의 값을 이용하여 3 클래스 분류를 할 수 있음  \n",
    "이 경우, 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 '점수(score)'가 됨  \n",
    "  \n",
    "\\* 점수란 '확률'이 되기 전의 값, 점수가 높을수록 그 뉴런에 해당하는 클래스의 확률도 높아짐  \n",
    "\\* 점수를 소프트맥스 함수(softmax function)에 입력하면 확률을 얻을 수 있음  \n",
    "  \n",
    "분류를 하면 출력층에서 가장 큰 값을 내뱉는 뉴런에 해당하는 클래스가 예측 결과가 됨  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4266051",
   "metadata": {},
   "source": [
    "### 1.2.2 계층으로 클래스화 및 순전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dce199",
   "metadata": {},
   "source": [
    "- 순전파(forward propagation) : 신경망 추론 과정에서 신경망을 구성하는 각 계층이 입력으로부터 출력 방향으로 처리 결과를 차례로 전파  \n",
    "- 역전파(backward propagation) : 신경망 학습에서 데이터(기울기)를 순전파와는 반대 방향으로 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42758c",
   "metadata": {},
   "source": [
    "#### sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "576d1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c4720",
   "metadata": {},
   "source": [
    "#### Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a099dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b81612",
   "metadata": {},
   "source": [
    "#### 신경망 구현( X(입력)  →  Affine  →  Sigmoid  →  Affine  →  S(점수) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f33dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "\n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fad97",
   "metadata": {},
   "source": [
    "#### 신경망의 추론 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e736d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9979ba",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dde39",
   "metadata": {},
   "source": [
    "- 추론 : 문제의 답을 구하는 작업  \n",
    "- 학습 : 최적의 매개변수 값을 찾는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cc8d3",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efcfd6",
   "metadata": {},
   "source": [
    "신경망 학습에서는 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 손실(loss)를 사용  \n",
    "손실은 학습 데이터(학습 시 주어진 정답 데이터)와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값(스칼라)  \n",
    "  \n",
    "신경망의 손실은 손실 함수(loss function)을 사용해 구함  \n",
    "다중 클래스 분류(multi-class classification) 신경망에서는 손실 함수로 흔히 교차 엔트로피 오차(Cross Entropy Error)를 사용  \n",
    "  \n",
    "Softmax 계층의 출력은 확률이 되고,  다음 계층인 Cross Entropy Error 계층에는 확률과 정답 레이블이 입력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba4ce1",
   "metadata": {},
   "source": [
    "- __소프트맥스 함수__  \n",
    "  \n",
    "    $y_k=\\frac{exp(s_k}{\\sum_{i=1}^nexp(s_i)}$  \n",
    "  \n",
    "소프트맥스 함수의 분자는 점수 $S_k$의 지수 함수이고, 분모는 모든 입력 신호의 지수 함수의 총합  \n",
    "소프트맥스 함수의 출력의 각 원소는 0.0 이상 1.0 이하의 실수  \n",
    "그리고 그 원소를 모두 더하면 1.0이 됨  \n",
    "따라서 소프트맥스 함수의 출력을 '확률'로 해석할 수 있음  \n",
    "  \n",
    "- __교차 엔트로피 오차__  \n",
    "  \n",
    "    $L=-\\sum_kt_klogy_k$   \n",
    "  \n",
    "여기서 t는 원핫 벡터로 표기한 정답레이블임  \n",
    "따라서, L은 정답 레이블이 1의 원소에 해당하는 출력의 자연로그  \n",
    "  \n",
    "- __교차 엔트로피 오차(미니배치)__  \n",
    "  \n",
    "    $L=-\\frac{1}{N}\\sum_N\\sum_kt_{nk}logy_{nk}$  \n",
    "  \n",
    "'평균 손실 함수'를 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475958e5",
   "metadata": {},
   "source": [
    "### 1.3.2 미분과 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970ad09",
   "metadata": {},
   "source": [
    "신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것  \n",
    "  \n",
    "$y=f(x)$가 있을 때, x에 관한 y의 미분은 $\\frac{\\partial{y}}{\\partial{x}}$  \n",
    "  \n",
    "여기서, $\\frac{\\partial{y}}{\\partial{x}}$가 의미하는 것은 x의 값을 '조금' 변화시켰을 때(더 정확하게는 그 '조금의 변화'를 극한까지 줄일 때)  \n",
    "y의 값이 얼마나 변하는가 하는 '변화의 정도'  \n",
    "이는 함수의 '기울기'에 해당함  \n",
    "  \n",
    "-----------------\n",
    "L은 스칼라, x는 벡터인 함수 $L=f(x)$인 경우,  \n",
    "$x_i$에 대한 L의 미분은 다음과 같다.\n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x}}=(\\frac{\\partial{L}}{\\partial{x_1}},\\frac{\\partial{L}}{\\partial{x_2}},\\cdots,\\frac{\\partial{L}}{\\partial{x_n}})$  \n",
    "  \n",
    "이처럼 벡터의 각 원소에 대한 미분을 정리한 것이 기울기(gradient)  \n",
    "  \n",
    "-----------------\n",
    "W가 $m\\times n$ 행렬이라면 $L=g(W)$함수의 기울기는 다음과 같다.  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{W}} =\n",
    " \\begin{pmatrix}\n",
    "  \\frac{\\partial{L}}{\\partial{w_{11}}} & \\dots & \\frac{\\partial{L}}{\\partial{w_{1n}}} \\\\\n",
    "  \\vdots & \\ddots &  \\\\\n",
    "  \\frac{\\partial{L}}{\\partial{w_{m1}}}  &   & \\frac{\\partial{L}}{\\partial{w_{mn}}}  \n",
    " \\end{pmatrix}$  \n",
    "  \n",
    "이처럼 L의 W에 대한 기울기를 행렬로 정리할 수 있음  \n",
    "  \n",
    "W와 $\\frac{\\partial{L}}{\\partial{W}}$의 형상이 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9730a",
   "metadata": {},
   "source": [
    "### 1.3.3 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a740191",
   "metadata": {},
   "source": [
    "연쇄 법칙(chain rule) : 합성 함수에 대한 미분의 법칙  \n",
    "  \n",
    "ex) $y=f(x), z=g(y)$라는 두 함수가 있을 때, $z=g(f(x))$가 되고, 이 합성 함수의 미분(x에 대한 z의 미분)은 다음과 같다.\n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}}$  \n",
    "  \n",
    "즉, x에 대한 z의 미분은 $y=f(x)$의 미분과 $z=g(y)$의 미분을 곱하여 구할 수 있음   \n",
    "  \n",
    "따라서 각 함수의 국소적인 미분을 계산할 수 있다면 그 값들을 곱해서 전체의 미분을 구할 수 있음  \n",
    "오차역전파법에서 각 매개변수에 대한 손실의 기울기를 구할 때 연쇄 법칙을 이용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68787955",
   "metadata": {},
   "source": [
    "### 1.3.4 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08978d5",
   "metadata": {},
   "source": [
    "계산 그래프는 계산 과정을 시각적으로 보여줌  \n",
    "  \n",
    "- 순전파 : 처리 결과가 순서대로(왼쪽에서 오른쪽으로) 흐름\n",
    "- 역전파 : 순전파와 반대 방향으로 전파됨  \n",
    "    전파되는 값은 최종 출력의 각 변수에 대한 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c18efb",
   "metadata": {},
   "source": [
    "#### 덧셈노드  \n",
    "  \n",
    "<img src='./img/1/sum_node.png' width=250>  \n",
    "  \n",
    "상류로부터 받은 값에 1을 곱하여 하류로 기울기를 전파  \n",
    "  \n",
    "#### 곱셈노드  \n",
    "  \n",
    "<img src='./img/1/mul_node.png' width=250>  \n",
    "  \n",
    "상류로부터 받은 기울기에 '순전파 시의 입력을 서로 바꾼 값'을 곱함  \n",
    "  \n",
    "#### 분기노드  \n",
    "  \n",
    "<img src='./img/1/repeat_node_1.png' width=250>  \n",
    "  \n",
    "분기 노드는 같은 값이 복제되어 분기  \n",
    "분기 노드의 역전파는 상류에서 온 기울기들의 '합'이 됨  \n",
    "  \n",
    "#### Repeat 노드  \n",
    "  \n",
    "<img src='./img/1/repeat_node_2.png' width=250>  \n",
    "\n",
    "N개로의 분기(복제) 노드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52a943",
   "metadata": {},
   "source": [
    "ex) 길이가 D인 배열을 N개로 복제하는 예  \n",
    "이 역전파는 N개의 기울기를 모두 더해 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88e23936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D) # 입력\n",
    "y = np.repeat(x, N, axis=0) # 순전파\n",
    "\n",
    "dy = np.random.randn(N, D) # 무작위 기울기\n",
    "dx = np.sum(dy, axis=0, keepdims=True) # 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbf9bb",
   "metadata": {},
   "source": [
    "#### Sum 노드  \n",
    "  \n",
    "Sum 노드의 역전파는 상류로부터의 기울기를 모든 화살표에 분배  \n",
    "Sum 노드와 Repeat 노드는 서로 '반대 관계'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10f2900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(N, D) # 입력\n",
    "y = np.sum(x, axis=0, keepdims=True) # 순전파\n",
    "\n",
    "dy = np.random.randn(1, D) # 무작위 기울기\n",
    "dx = np.repeat(dy, N, axis=0) # 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a0627",
   "metadata": {},
   "source": [
    "#### MatMul 노드  \n",
    "  \n",
    "행렬의 곱셈 'Matrix Multiply'의 약자  \n",
    "  \n",
    "<img src='./img/1/matmul_node.png' width=250>  \n",
    "  \n",
    "$x$의 i번째 원소에 대한 미분  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}$  \n",
    "  \n",
    "이 식의 $\\frac{\\partial{L}}{\\partial{x_i}}$은 $x_i$를 변화시켰을 때 L이 얼마나 변할 것인가라는 '변화의 정도'를 나타냄  \n",
    "여기서 $x_i$를 변화시키면 벡터 y의 모든 원소가 변하고, 그로 인해 L이 변하게 됨  \n",
    "$x_i$에서 L에 이르는 연쇄 법칙의 경로는 여러 개가 있으며, 그 총합은 $\\frac{\\partial{L}}{\\partial{x_i}}$이 됨  \n",
    "  \n",
    "$\\frac{\\partial{y_j}}{\\partial{x_i}}=W_{ij}$이므로,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}W_{ij}$  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}$는 벡터 $\\frac{\\partial{L}}{\\partial{y}}$와 W의 i행 벡터의 내적으로 구해짐을 알 수 있음  \n",
    "  \n",
    "따라서,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{y}}W^T$  \n",
    "  \n",
    "마찬가지로,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{W}}=X^T\\frac{\\partial{L}}{\\partial{y}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb00b7f",
   "metadata": {},
   "source": [
    "< 참고 >  \n",
    "  \n",
    "~~~python\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "~~~\n",
    "  \n",
    "'a = b'와 'a[...] = b' 모두 a에는 [4,5,6]이 할당되나  \n",
    "두 경우 a가 가리키는 메모리 위치는 서로 다름  \n",
    "\n",
    "<img src='./img/1/deepcopy.png' width=500>  \n",
    "  \n",
    "a = b에서는 a가 가리키는 메모리 위치가 b가 가리키는 위치와 같아짐  \n",
    "실제 데이터 (4,5,6)은 복제되지 않는 다는 뜻으로 이를 '얕은 복사'라고 함  \n",
    "  \n",
    "한편, a[...] = b일 때는, a의 메모리 위치는 변하지 않고, a가 가리키는 메모리에 b의 원소가 복제됨  \n",
    "실제 데이터가 복제된다는 뜻에서 '깊은 복사'라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d82edd",
   "metadata": {},
   "source": [
    "### 1.3.5 기울기 도출과 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a5f60",
   "metadata": {},
   "source": [
    "#### Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97825319",
   "metadata": {},
   "source": [
    "시그모이드 함수  \n",
    "  \n",
    "$y=\\frac{1}{1+exp(-x}$  \n",
    "  \n",
    "위 식을 미분하면,  \n",
    "  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=y(1-y)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e52960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a889a35",
   "metadata": {},
   "source": [
    "#### Affine 계층(완전연결층)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3714ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d67a7",
   "metadata": {},
   "source": [
    "#### Softmax with Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d0ef4",
   "metadata": {},
   "source": [
    "softmax 계층은 입력 ($a_1, a_2, a_3)$를 정규화하여 ($y_1, y_2, y_3$)을 출력  \n",
    "Cross Entropy Error 계층은 Softmax의 출력 ($y_1, y_2, y_3$)과 정답 레이블($t_1, t_2, t_3)$을 받고, 이 데이터로부터 손실 L을 구해 출력  \n",
    "  \n",
    "Softmax 계층의 역전파는 ($y_1-t_1, y_2-t_2, y_3-t_3$)인 출력과 정답 레이블의 차이를 출력  \n",
    "신경망의 역전파는 이 차이(오차)를 앞 계층에 전해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54788e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61192419",
   "metadata": {},
   "source": [
    "### 1.3.6 가중치 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d850",
   "metadata": {},
   "source": [
    "오차역전파법으로 기울기를 구했으면, 그 기울기를 사용하여 신경망의 매개변수를 갱신함  \n",
    "  \n",
    "신경망의 학습은 다음 순서로 수행  \n",
    "  \n",
    "1. 미니배치  \n",
    "    훈련 데이터 중에서 무작위로 다수의 데이터를 골라낸다.\n",
    "2. 기울기 계산\n",
    "    오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기르 ㄹ구한다.\n",
    "3. 매개변수 갱신\n",
    "    기울기를 사용하여 가중치 매개변수를 갱신한다.\n",
    "4. 반복\n",
    "    1 ~ 3 단계를 필요한 만큼 반복한다.  \n",
    "  \n",
    "  \n",
    "기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킴  \n",
    "따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있음  \n",
    "이것이 바로 __경사하강법(Gradient Descent)__  \n",
    "  \n",
    "3단계에서 매개변수를 갱신하는 방법의 종류는 다양함  \n",
    "  \n",
    "그 중 확률적경사하강법(Stochastic Gradient Descent, SGD)는 (현재의) 가중치를 기울기 방향으로 일정한 거리만큼 갱신  \n",
    "  \n",
    "$W\\leftarrow W-\\eta\\frac{\\partial{L}}{\\partial{W}}$  \n",
    "  \n",
    "- W : 갱신하는 가중치 매개변수\n",
    "- $\\frac{\\partial{L}}{\\partial{W}}$ : W에 대한 손실 함수의 기울기\n",
    "- $\\eta$ : 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "379fa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b4132",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# 신경망의 매개변수 갱신(예시)\n",
    "\n",
    "model = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, t_batch = get_mini_batch(...) # 미니배치 획득\n",
    "    loss = model.forward(x_batch, t_batch)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)\n",
    "    ...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e1d6c",
   "metadata": {},
   "source": [
    "## 1.4 신경망으로 문제를 풀다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4858b46",
   "metadata": {},
   "source": [
    "### 1.4.1 스파이럴 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b161e",
   "metadata": {},
   "source": [
    "<img src='./img/1/spiral_dataset.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e11b7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape)\n",
    "print('t', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62dc5f",
   "metadata": {},
   "source": [
    "입력은 2차원 데이터, 분류할 클래스 수는 3개  \n",
    "위 그래프를 보면 직선만으로는 클래스들을 분리할 수 없음을 알 수 있음  \n",
    "비선형 분리를 학습해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7559897",
   "metadata": {},
   "source": [
    "### 1.4.2 신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00123f68",
   "metadata": {},
   "source": [
    "은닉층이 하나인 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd688bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    # 추론 수행\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # 순전파\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "    \n",
    "    # 역전파\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaeffa0",
   "metadata": {},
   "source": [
    "### 1.4.3 학습용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e51c0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 2 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 3 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 4 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 5 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 6 | 반복 10 / 10 | 손실 1.14 |\n",
      "| 에폭 7 | 반복 10 / 10 | 손실 1.16 |\n",
      "| 에폭 8 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 9 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 10 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 11 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 12 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 13 | 반복 10 / 10 | 손실 1.09 |\n",
      "| 에폭 14 | 반복 10 / 10 | 손실 1.08 |\n",
      "| 에폭 15 | 반복 10 / 10 | 손실 1.04 |\n",
      "| 에폭 16 | 반복 10 / 10 | 손실 1.03 |\n",
      "| 에폭 17 | 반복 10 / 10 | 손실 0.96 |\n",
      "| 에폭 18 | 반복 10 / 10 | 손실 0.92 |\n",
      "| 에폭 19 | 반복 10 / 10 | 손실 0.92 |\n",
      "| 에폭 20 | 반복 10 / 10 | 손실 0.87 |\n",
      "| 에폭 21 | 반복 10 / 10 | 손실 0.85 |\n",
      "| 에폭 22 | 반복 10 / 10 | 손실 0.82 |\n",
      "| 에폭 23 | 반복 10 / 10 | 손실 0.79 |\n",
      "| 에폭 24 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 25 | 반복 10 / 10 | 손실 0.82 |\n",
      "| 에폭 26 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 27 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 28 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 29 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 30 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 31 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 32 | 반복 10 / 10 | 손실 0.77 |\n",
      "| 에폭 33 | 반복 10 / 10 | 손실 0.77 |\n",
      "| 에폭 34 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 35 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 36 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 37 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 38 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 39 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 40 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 41 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 42 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 43 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 44 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 45 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 46 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 47 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 48 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 49 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 50 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 51 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 52 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 53 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 54 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 55 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 56 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 57 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 58 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 59 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 60 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 61 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 62 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 63 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 64 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 65 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 66 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 67 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 68 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 69 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 70 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 71 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 72 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 73 | 반복 10 / 10 | 손실 0.67 |\n",
      "| 에폭 74 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 75 | 반복 10 / 10 | 손실 0.67 |\n",
      "| 에폭 76 | 반복 10 / 10 | 손실 0.66 |\n",
      "| 에폭 77 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 78 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 79 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 80 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 81 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 82 | 반복 10 / 10 | 손실 0.66 |\n",
      "| 에폭 83 | 반복 10 / 10 | 손실 0.62 |\n",
      "| 에폭 84 | 반복 10 / 10 | 손실 0.62 |\n",
      "| 에폭 85 | 반복 10 / 10 | 손실 0.61 |\n",
      "| 에폭 86 | 반복 10 / 10 | 손실 0.60 |\n",
      "| 에폭 87 | 반복 10 / 10 | 손실 0.60 |\n",
      "| 에폭 88 | 반복 10 / 10 | 손실 0.61 |\n",
      "| 에폭 89 | 반복 10 / 10 | 손실 0.59 |\n",
      "| 에폭 90 | 반복 10 / 10 | 손실 0.58 |\n",
      "| 에폭 91 | 반복 10 / 10 | 손실 0.56 |\n",
      "| 에폭 92 | 반복 10 / 10 | 손실 0.56 |\n",
      "| 에폭 93 | 반복 10 / 10 | 손실 0.54 |\n",
      "| 에폭 94 | 반복 10 / 10 | 손실 0.53 |\n",
      "| 에폭 95 | 반복 10 / 10 | 손실 0.53 |\n",
      "| 에폭 96 | 반복 10 / 10 | 손실 0.52 |\n",
      "| 에폭 97 | 반복 10 / 10 | 손실 0.51 |\n",
      "| 에폭 98 | 반복 10 / 10 | 손실 0.50 |\n",
      "| 에폭 99 | 반복 10 / 10 | 손실 0.48 |\n",
      "| 에폭 100 | 반복 10 / 10 | 손실 0.48 |\n",
      "| 에폭 101 | 반복 10 / 10 | 손실 0.46 |\n",
      "| 에폭 102 | 반복 10 / 10 | 손실 0.45 |\n",
      "| 에폭 103 | 반복 10 / 10 | 손실 0.45 |\n",
      "| 에폭 104 | 반복 10 / 10 | 손실 0.44 |\n",
      "| 에폭 105 | 반복 10 / 10 | 손실 0.44 |\n",
      "| 에폭 106 | 반복 10 / 10 | 손실 0.41 |\n",
      "| 에폭 107 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 108 | 반복 10 / 10 | 손실 0.41 |\n",
      "| 에폭 109 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 110 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 111 | 반복 10 / 10 | 손실 0.38 |\n",
      "| 에폭 112 | 반복 10 / 10 | 손실 0.38 |\n",
      "| 에폭 113 | 반복 10 / 10 | 손실 0.36 |\n",
      "| 에폭 114 | 반복 10 / 10 | 손실 0.37 |\n",
      "| 에폭 115 | 반복 10 / 10 | 손실 0.35 |\n",
      "| 에폭 116 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 117 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 118 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 119 | 반복 10 / 10 | 손실 0.33 |\n",
      "| 에폭 120 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 121 | 반복 10 / 10 | 손실 0.32 |\n",
      "| 에폭 122 | 반복 10 / 10 | 손실 0.32 |\n",
      "| 에폭 123 | 반복 10 / 10 | 손실 0.31 |\n",
      "| 에폭 124 | 반복 10 / 10 | 손실 0.31 |\n",
      "| 에폭 125 | 반복 10 / 10 | 손실 0.30 |\n",
      "| 에폭 126 | 반복 10 / 10 | 손실 0.30 |\n",
      "| 에폭 127 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 128 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 129 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 130 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 131 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 132 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 133 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 134 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 135 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 136 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 137 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 138 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 139 | 반복 10 / 10 | 손실 0.25 |\n",
      "| 에폭 140 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 141 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 142 | 반복 10 / 10 | 손실 0.25 |\n",
      "| 에폭 143 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 144 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 145 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 146 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 147 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 148 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 149 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 150 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 151 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 152 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 153 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 154 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 155 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 156 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 157 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 158 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 159 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 160 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 161 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 162 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 163 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 164 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 165 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 166 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 167 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 168 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 169 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 170 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 171 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 172 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 173 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 174 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 175 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 176 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 177 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 178 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 179 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 180 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 181 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 182 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 183 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 184 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 185 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 186 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 187 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 188 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 189 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 190 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 191 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 192 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 193 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 194 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 195 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 196 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 197 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 198 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 199 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 200 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 201 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 202 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 203 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 204 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 205 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 206 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 207 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 208 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 209 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 210 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 211 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 212 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 213 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 214 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 215 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 216 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 217 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 218 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 219 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 220 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 221 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 222 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 223 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 224 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 225 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 226 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 227 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 228 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 229 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 230 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 231 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 232 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 233 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 234 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 235 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 236 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 237 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 238 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 239 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 240 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 241 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 242 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 243 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 244 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 245 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 246 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 247 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 248 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 249 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 250 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 251 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 252 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 253 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 254 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 255 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 256 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 257 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 258 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 259 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 260 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 261 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 262 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 263 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 264 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 265 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 266 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 267 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 268 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 269 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 270 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 271 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 272 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 273 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 274 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 275 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 276 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 277 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 278 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 279 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 280 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 281 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 282 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 283 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 284 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 285 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 286 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 287 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 288 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 289 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 290 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 291 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 292 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 293 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 294 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 295 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 296 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 297 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 298 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 299 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 300 | 반복 10 / 10 | 손실 0.11 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 2. 데이터 읽기, 모델과 옵티마이저 생성\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr = learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 3. 데이터 뒤섞기\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        \n",
    "        # 4. 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        # 5. 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d | 반복 %d / %d | 손실 %.2f |' % (epoch+1, iters+1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67320512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### 학습에 따른 loss의 변화 시각화\n",
    "\n",
    "# 학습 결과 플롯\n",
    "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
    "plt.xlabel('x10')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "### 결정 경계(decision boundary) 시각화\n",
    "\n",
    "# 경계 영역 플롯\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "score = model.predict(X)\n",
    "predict_cls = np.argmax(score, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('off')\n",
    "\n",
    "# 데이터점 플롯\n",
    "x, t = spiral.load_data()\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c2338",
   "metadata": {},
   "source": [
    "### 1.4.4 Trainer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e23db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from common.np import *  # import numpy as np\n",
    "from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa0ac7",
   "metadata": {},
   "source": [
    "- x : 입력 데이터\n",
    "- t : 정답 레이블\n",
    "- max_epoch(=10) : 학습을 수행하는 에폭 수\n",
    "- batch_size(=32) : 미니배치 크기\n",
    "- eval_interval(=20) : 결과(평균 손실 등)를 출력하는 간격\n",
    "- max_grad(=None) : 기울기 최대 노름(norm)\n",
    "    기울기 노름이 이 값을 넘어서면 기울기를 줄인다(이를 기울기 클리핑이라고 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c804f",
   "metadata": {},
   "source": [
    "Trainer 클래스를 활용한 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "280fac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
      "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from dataset import spiral\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3370c6",
   "metadata": {},
   "source": [
    "## 1.5 계산 고속화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316753b",
   "metadata": {},
   "source": [
    "### 1.5.1 비트 정밀도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9e74130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac6930",
   "metadata": {},
   "source": [
    "넘파이의 64비트 부동소수점 수를 표준으로 사용  \n",
    "그러나,  \n",
    "- 신경망의 추론과 학습은 32비트 부동소수점 수로도 인식률을 거의 떨어뜨리는 일 없이 수행할 수 있음\n",
    "- 메모리 관점에서 항상 64비트에 비해 43비트가 더 조흥ㅁ\n",
    "- 신경망 계산 시 데이터를 전송하는 '버스 대역 폭(bus bandwidth)'이 병목되는 경우가 종종 있으므로, 데이터 타입이 작은게 좋음\n",
    "- 계산 속도 측면에서도 32비트 부동소수점 수가 일반적으로 더 빠름  \n",
    "  \n",
    "넘파이에서 32비트 부동소수점 수를 사용하려면 다음과 같이 데이터 타입을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbcd27d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(3).astype(np.float32)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "152e403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.randn(3).astype('f')\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea4806",
   "metadata": {},
   "source": [
    "신경망 추론으로 한정하면, 16비트 부동소수점 수를 사용해도 인식률이 거의 떨어지지 않음  \n",
    "하지만, 일반적으로 CPU와 GPU는 연산 자체를 32비트로 수행하므로 16비트로 변환하더라도 계산 자체는 32비트로 이뤄짐  \n",
    "학습된 가중치를 저장할 때는 16비트 부동소수점 수가 여전히 유효(32비트에 비해 절반의 용량만 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19fa95",
   "metadata": {},
   "source": [
    "### 1.5.2 GPU(쿠파이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771da967",
   "metadata": {},
   "source": [
    "딥러닝의 계산은 대량의 곱하기 연산으로 구성  \n",
    "따라서 CPU보다 GPU가 유리  \n",
    "  \n",
    "쿠파이 : GPU를 이용해 병렬 계산을 수행해주는 라이브러리  \n",
    "    넘파이와 호환되는 API를 제공함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
