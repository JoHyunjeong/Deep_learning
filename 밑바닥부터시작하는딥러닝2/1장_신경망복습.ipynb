{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8affb2db",
   "metadata": {},
   "source": [
    "# CHAPTER 1 신경망 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53999d49",
   "metadata": {},
   "source": [
    "## 1.1 수학과 파이썬 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b65721",
   "metadata": {},
   "source": [
    "### 1.1.1 벡터와 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1d3d8",
   "metadata": {},
   "source": [
    "- 벡터 : 크기와 방향을 가진 양  \n",
    "    벡터는 숫자가 일렬로 늘어선 집합으로 표현할 수 있음  \n",
    "    파이썬에서는 1차원 배열로 취급  \n",
    "<br> \n",
    "\n",
    "- 행렬 : 숫자가 2차원 형태(사각형 형상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644083d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "x.__class__ # 클래스 이름 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01e3ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 형상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1577d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim # 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf21d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb08a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94ca9b",
   "metadata": {},
   "source": [
    "### 1.1.2 행렬의 원소별 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eecb0a0",
   "metadata": {},
   "source": [
    "원소별(element-wise) 연산 : 서로 대응하는 원소끼리 (각 원소가 독립적으로) 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d79935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5],\n",
       "       [ 7,  9, 11]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2,3],[4,5,6]])\n",
    "X = np.array([[0,1,2],[3,4,5]])\n",
    "W + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5eea5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  6],\n",
       "       [12, 20, 30]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W * X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c66fa",
   "metadata": {},
   "source": [
    "### 1.1.3 브로드캐스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab3b16",
   "metadata": {},
   "source": [
    "<img src='./img/1/broadcast.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778cea1",
   "metadata": {},
   "source": [
    "배열의 형상이 달라도 자동적으로 확장하여 형상이 다른 배열끼리의 연산을 수행할 수 있게 하는 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a410f",
   "metadata": {},
   "source": [
    "### 1.1.4 벡터의 내적과 행렬의 곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e13a9d",
   "metadata": {},
   "source": [
    "- 벡터의 내적 : 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것  \n",
    "<br>\n",
    "\n",
    "<center>$x\\cdot y=x_1y_1+x_2y_2+\\cdots +x_ny_n$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd04eb",
   "metadata": {},
   "source": [
    "- 행렬의 곱 : 왼쪽 행렬의 행벡터(가로 방향)와 오른쪽 행렬의 열벡터(세로 방향)의 내적(원소별 곱의 합)으로 계산  \n",
    "    계산 결과는 새로운 행렬의 대응하는 원소에 저장됨  \n",
    "<br>\n",
    "<img src='./img/1/matrix_product_1.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51038964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7e3d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬의 곱\n",
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[5,6],[7,8]])\n",
    "np.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76593a7",
   "metadata": {},
   "source": [
    "### 1.1.5 행렬 형상 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9798a8",
   "metadata": {},
   "source": [
    "행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜야 함  \n",
    "<br>\n",
    "<img src='./img/1/matrix_product_2.png' width=300>  \n",
    "<br>\n",
    "행렬 C의 형상은 A의 행 수와 B의 열 수가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba82e6",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdaa352",
   "metadata": {},
   "source": [
    "### 1.2.1 신경망 추론 전체 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fa07e",
   "metadata": {},
   "source": [
    "ex) 입력층에 뉴런 2개, 은닉층에 뉴런 4개, 출력층에 뉴런 3개를 둔 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efec5e",
   "metadata": {},
   "source": [
    "<img src='./img/1/example.png' width=300>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f167",
   "metadata": {},
   "source": [
    "가중치와 뉴런의 값을 각각 곱해서 그 합이 다음 뉴런의 입력이 됨  \n",
    "(정확하게는 그 합에 활성화 함수(activation function)을 적용한 값이 다음 뉴런의 입력이 됨)  \n",
    "또, 이때 각 층에서는 이전 뉴런의 값에 영향받지 않은 '정수'인 편향(bias)도 더해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42923c2",
   "metadata": {},
   "source": [
    "입력층의 데이터를 $(x_1, x_2)$, 가중치는 $(w_{11}, w_{21})$, 편향은 $b_1$으로 쓴다면,  \n",
    "은닉층 중 첫 번째 뉴런 $h_1$는 다음과 같이 계산할 수 있다.  \n",
    "  \n",
    "$h_1=x_1w_{11}+x_2w_{21}+b_1$  \n",
    "  \n",
    "위와 같이 은닉층의 뉴런은 가중치의 합으로 계산됨  \n",
    "이 값은 행렬의 곱으로 한꺼번에 계산할 수 있음  \n",
    "  \n",
    "  \n",
    "$(h_1,h_2,h_3,h_4)=(x_1,x_2)\\begin{pmatrix}w_{11}&w_{12}&w_{13}&w_{14} \\\\ w_{21}&w_{22}&w_{23}&w_{24} \\end{pmatrix}+(b_1,b_2,b_3,b_4)$  \n",
    "  \n",
    "$h=xW+b$  \n",
    "  \n",
    "- x : 입력(1,2)\n",
    "- h : 은닉층의 뉴런(1,4)\n",
    "- W : 가중치(2,4)\n",
    "- b : 편향(4,)  \n",
    "  \n",
    "\\* 여기서, 편향 b1은 브로드캐스트 되어 더해짐\n",
    "  \n",
    "미니배치로 한번에 N개 데이터를 처리하는 경우   \n",
    "  \n",
    "- x : 입력(N,2)\n",
    "- h : 은닉층의 뉴런(N,4)\n",
    "- W : 가중치(2,4)\n",
    "- b : 편향(4,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a5f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W1 = np.random.randn(2,4) # 가중치\n",
    "b1 = np.random.randn(4) # 편향\n",
    "x = np.random.randn(10,2) # 입력\n",
    "h = np.matmul(x, W1) + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b0967",
   "metadata": {},
   "source": [
    "완전연결계층에 의한 변환은 '선형' 변환  \n",
    "여기에 '비선형'효과를 부여하는 것이 활성화 함수  \n",
    "비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있음  \n",
    "  \n",
    "ex) 시그모이드 함수(sigmoid function)  \n",
    "  \n",
    "$\\sigma(x)=\\frac{1}{1+exp(-x)}$  \n",
    "  \n",
    "시그모이드 함수는 알파벳 'S'자 모양의 곡선 함수  \n",
    "임의의 실수를 입력받아 0에서 1 사이의 실수를 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e39f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9192d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수로 비선형 변환\n",
    "a = sigmoid(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea797e",
   "metadata": {},
   "source": [
    "활성화 함수의 출력을 활성화(activation)이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a16dd6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10,2)\n",
    "W1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4,3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.matmul(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.matmul(a, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1b59f",
   "metadata": {},
   "source": [
    "위 신경망 예시는 3차원 데이터를 출력합  \n",
    "각 차원의 값을 이용하여 3 클래스 분류를 할 수 있음  \n",
    "이 경우, 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 '점수(score)'가 됨  \n",
    "  \n",
    "\\* 점수란 '확률'이 되기 전의 값, 점수가 높을수록 그 뉴런에 해당하는 클래스의 확률도 높아짐  \n",
    "\\* 점수를 소프트맥스 함수(softmax function)에 입력하면 확률을 얻을 수 있음  \n",
    "  \n",
    "분류를 하면 출력층에서 가장 큰 값을 내뱉는 뉴런에 해당하는 클래스가 예측 결과가 됨  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4266051",
   "metadata": {},
   "source": [
    "### 1.2.2 계층으로 클래스화 및 순전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dce199",
   "metadata": {},
   "source": [
    "- 순전파(forward propagation) : 신경망 추론 과정에서 신경망을 구성하는 각 계층이 입력으로부터 출력 방향으로 처리 결과를 차례로 전파  \n",
    "- 역전파(backward propagation) : 신경망 학습에서 데이터(기울기)를 순전파와는 반대 방향으로 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42758c",
   "metadata": {},
   "source": [
    "#### sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576d1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c4720",
   "metadata": {},
   "source": [
    "#### Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a099dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b81612",
   "metadata": {},
   "source": [
    "#### 신경망 구현( X(입력)  →  Affine  →  Sigmoid  →  Affine  →  S(점수) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f33dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "\n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fad97",
   "metadata": {},
   "source": [
    "#### 신경망의 추론 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e736d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9979ba",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dde39",
   "metadata": {},
   "source": [
    "- 추론 : 문제의 답을 구하는 작업  \n",
    "- 학습 : 최적의 매개변수 값을 찾는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cc8d3",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efcfd6",
   "metadata": {},
   "source": [
    "신경망 학습에서는 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 손실(loss)를 사용  \n",
    "손실은 학습 데이터(학습 시 주어진 정답 데이터)와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값(스칼라)  \n",
    "  \n",
    "신경망의 손실은 손실 함수(loss function)을 사용해 구함  \n",
    "다중 클래스 분류(multi-class classification) 신경망에서는 손실 함수로 흔히 교차 엔트로피 오차(Cross Entropy Error)를 사용  \n",
    "  \n",
    "Softmax 계층의 출력은 확률이 되고,  다음 계층인 Cross Entropy Error 계층에는 확률과 정답 레이블이 입력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba4ce1",
   "metadata": {},
   "source": [
    "- __소프트맥스 함수__  \n",
    "  \n",
    "    $y_k=\\frac{exp(s_k}{\\sum_{i=1}^nexp(s_i)}$  \n",
    "  \n",
    "소프트맥스 함수의 분자는 점수 $S_k$의 지수 함수이고, 분모는 모든 입력 신호의 지수 함수의 총합  \n",
    "소프트맥스 함수의 출력의 각 원소는 0.0 이상 1.0 이하의 실수  \n",
    "그리고 그 원소를 모두 더하면 1.0이 됨  \n",
    "따라서 소프트맥스 함수의 출력을 '확률'로 해석할 수 있음  \n",
    "  \n",
    "- __교차 엔트로피 오차__  \n",
    "  \n",
    "    $L=-\\sum_kt_klogy_k$   \n",
    "  \n",
    "여기서 t는 원핫 벡터로 표기한 정답레이블임  \n",
    "따라서, L은 정답 레이블이 1의 원소에 해당하는 출력의 자연로그  \n",
    "  \n",
    "- __교차 엔트로피 오차(미니배치)__  \n",
    "  \n",
    "    $L=-\\frac{1}{N}\\sum_N\\sum_kt_{nk}logy_{nk}$  \n",
    "  \n",
    "'평균 손실 함수'를 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475958e5",
   "metadata": {},
   "source": [
    "### 1.3.2 미분과 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970ad09",
   "metadata": {},
   "source": [
    "신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것  \n",
    "  \n",
    "$y=f(x)$가 있을 때, x에 관한 y의 미분은 $\\frac{\\partial{y}}{\\partial{x}}$  \n",
    "  \n",
    "여기서, $\\frac{\\partial{y}}{\\partial{x}}$가 의미하는 것은 x의 값을 '조금' 변화시켰을 때(더 정확하게는 그 '조금의 변화'를 극한까지 줄일 때)  \n",
    "y의 값이 얼마나 변하는가 하는 '변화의 정도'  \n",
    "이는 함수의 '기울기'에 해당함  \n",
    "  \n",
    "-----------------\n",
    "L은 스칼라, x는 벡터인 함수 $L=f(x)$인 경우,  \n",
    "$x_i$에 대한 L의 미분은 다음과 같다.\n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x}}=(\\frac{\\partial{L}}{\\partial{x_1}},\\frac{\\partial{L}}{\\partial{x_2}},\\cdots,\\frac{\\partial{L}}{\\partial{x_n}})$  \n",
    "  \n",
    "이처럼 벡터의 각 원소에 대한 미분을 정리한 것이 기울기(gradient)  \n",
    "  \n",
    "-----------------\n",
    "W가 $m\\times n$ 행렬이라면 $L=g(W)$함수의 기울기는 다음과 같다.  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{W}} =\n",
    " \\begin{pmatrix}\n",
    "  \\frac{\\partial{L}}{\\partial{w_{11}}} & \\dots & \\frac{\\partial{L}}{\\partial{w_{1n}}} \\\\\n",
    "  \\vdots & \\ddots &  \\\\\n",
    "  \\frac{\\partial{L}}{\\partial{w_{m1}}}  &   & \\frac{\\partial{L}}{\\partial{w_{mn}}}  \n",
    " \\end{pmatrix}$  \n",
    "  \n",
    "이처럼 L의 W에 대한 기울기를 행렬로 정리할 수 있음  \n",
    "  \n",
    "W와 $\\frac{\\partial{L}}{\\partial{W}}$의 형상이 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9730a",
   "metadata": {},
   "source": [
    "### 1.3.3 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a740191",
   "metadata": {},
   "source": [
    "연쇄 법칙(chain rule) : 합성 함수에 대한 미분의 법칙  \n",
    "  \n",
    "ex) $y=f(x), z=g(y)$라는 두 함수가 있을 때, $z=g(f(x))$가 되고, 이 합성 함수의 미분(x에 대한 z의 미분)은 다음과 같다.\n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}}$  \n",
    "  \n",
    "즉, x에 대한 z의 미분은 $y=f(x)$의 미분과 $z=g(y)$의 미분을 곱하여 구할 수 있음   \n",
    "  \n",
    "따라서 각 함수의 국소적인 미분을 계산할 수 있다면 그 값들을 곱해서 전체의 미분을 구할 수 있음  \n",
    "오차역전파법에서 각 매개변수에 대한 손실의 기울기를 구할 때 연쇄 법칙을 이용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68787955",
   "metadata": {},
   "source": [
    "### 1.3.4 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08978d5",
   "metadata": {},
   "source": [
    "계산 그래프는 계산 과정을 시각적으로 보여줌  \n",
    "  \n",
    "- 순전파 : 처리 결과가 순서대로(왼쪽에서 오른쪽으로) 흐름\n",
    "- 역전파 : 순전파와 반대 방향으로 전파됨  \n",
    "    전파되는 값은 최종 출력의 각 변수에 대한 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c18efb",
   "metadata": {},
   "source": [
    "#### 덧셈노드  \n",
    "  \n",
    "<img src='./img/1/sum_node.png' width=250>  \n",
    "  \n",
    "상류로부터 받은 값에 1을 곱하여 하류로 기울기를 전파  \n",
    "  \n",
    "#### 곱셈노드  \n",
    "  \n",
    "<img src='./img/1/mul_node.png' width=250>  \n",
    "  \n",
    "상류로부터 받은 기울기에 '순전파 시의 입력을 서로 바꾼 값'을 곱함  \n",
    "  \n",
    "#### 분기노드  \n",
    "  \n",
    "<img src='./img/1/repeat_node_1.png' width=250>  \n",
    "  \n",
    "분기 노드는 같은 값이 복제되어 분기  \n",
    "분기 노드의 역전파는 상류에서 온 기울기들의 '합'이 됨  \n",
    "  \n",
    "#### Repeat 노드  \n",
    "  \n",
    "<img src='./img/1/repeat_node_2.png' width=250>  \n",
    "\n",
    "N개로의 분기(복제) 노드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52a943",
   "metadata": {},
   "source": [
    "ex) 길이가 D인 배열을 N개로 복제하는 예  \n",
    "이 역전파는 N개의 기울기를 모두 더해 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e23936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D) # 입력\n",
    "y = np.repeat(x, N, axis=0) # 순전파\n",
    "\n",
    "dy = np.random.randn(N, D) # 무작위 기울기\n",
    "dx = np.sum(dy, axis=0, keepdims=True) # 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbf9bb",
   "metadata": {},
   "source": [
    "#### Sum 노드  \n",
    "  \n",
    "Sum 노드의 역전파는 상류로부터의 기울기를 모든 화살표에 분배  \n",
    "Sum 노드와 Repeat 노드는 서로 '반대 관계'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10f2900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(N, D) # 입력\n",
    "y = np.sum(x, axis=0, keepdims=True) # 순전파\n",
    "\n",
    "dy = np.random.randn(1, D) # 무작위 기울기\n",
    "dx = np.repeat(dy, N, axis=0) # 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a0627",
   "metadata": {},
   "source": [
    "#### MatMul 노드  \n",
    "  \n",
    "행렬의 곱셈 'Matrix Multiply'의 약자  \n",
    "  \n",
    "<img src='./img/1/matmul_node.png' width=250>  \n",
    "  \n",
    "$x$의 i번째 원소에 대한 미분  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}$  \n",
    "  \n",
    "이 식의 $\\frac{\\partial{L}}{\\partial{x_i}}$은 $x_i$를 변화시켰을 때 L이 얼마나 변할 것인가라는 '변화의 정도'를 나타냄  \n",
    "여기서 $x_i$를 변화시키면 벡터 y의 모든 원소가 변하고, 그로 인해 L이 변하게 됨  \n",
    "$x_i$에서 L에 이르는 연쇄 법칙의 경로는 여러 개가 있으며, 그 총합은 $\\frac{\\partial{L}}{\\partial{x_i}}$이 됨  \n",
    "  \n",
    "$\\frac{\\partial{y_j}}{\\partial{x_i}}=W_{ij}$이므로,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}W_{ij}$  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}$는 벡터 $\\frac{\\partial{L}}{\\partial{y}}$와 W의 i행 벡터의 내적으로 구해짐을 알 수 있음  \n",
    "  \n",
    "따라서,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{y}}W^T$  \n",
    "  \n",
    "마찬가지로,  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{W}}=X^T\\frac{\\partial{L}}{\\partial{y}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e3e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb00b7f",
   "metadata": {},
   "source": [
    "< 참고 >  \n",
    "  \n",
    "~~~python\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "~~~\n",
    "  \n",
    "'a = b'와 'a[...] = b' 모두 a에는 [4,5,6]이 할당되나  \n",
    "두 경우 a가 가리키는 메모리 위치는 서로 다름  \n",
    "\n",
    "<img src='./img/1/deepcopy.png' width=500>  \n",
    "  \n",
    "a = b에서는 a가 가리키는 메모리 위치가 b가 가리키는 위치와 같아짐  \n",
    "실제 데이터 (4,5,6)은 복제되지 않는 다는 뜻으로 이를 '얕은 복사'라고 함  \n",
    "  \n",
    "한편, a[...] = b일 때는, a의 메모리 위치는 변하지 않고, a가 가리키는 메모리에 b의 원소가 복제됨  \n",
    "실제 데이터가 복제된다는 뜻에서 '깊은 복사'라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d82edd",
   "metadata": {},
   "source": [
    "### 1.3.5 기울기 도출과 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a5f60",
   "metadata": {},
   "source": [
    "#### Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97825319",
   "metadata": {},
   "source": [
    "시그모이드 함수  \n",
    "  \n",
    "$y=\\frac{1}{1+exp(-x}$  \n",
    "  \n",
    "위 식을 미분하면,  \n",
    "  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=y(1-y)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e52960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a889a35",
   "metadata": {},
   "source": [
    "#### Affine 계층(완전연결층)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3714ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d67a7",
   "metadata": {},
   "source": [
    "#### Softmax with Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d0ef4",
   "metadata": {},
   "source": [
    "softmax 계층은 입력 ($a_1, a_2, a_3)$를 정규화하여 ($y_1, y_2, y_3$)을 출력  \n",
    "Cross Entropy Error 계층은 Softmax의 출력 ($y_1, y_2, y_3$)과 정답 레이블($t_1, t_2, t_3)$을 받고, 이 데이터로부터 손실 L을 구해 출력  \n",
    "  \n",
    "Softmax 계층의 역전파는 ($y_1-t_1, y_2-t_2, y_3-t_3$)인 출력과 정답 레이블의 차이를 출력  \n",
    "신경망의 역전파는 이 차이(오차)를 앞 계층에 전해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54788e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61192419",
   "metadata": {},
   "source": [
    "### 1.3.6 가중치 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d850",
   "metadata": {},
   "source": [
    "오차역전파법으로 기울기를 구했으면, 그 기울기를 사용하여 신경망의 매개변수를 갱신함  \n",
    "  \n",
    "신경망의 학습은 다음 순서로 수행  \n",
    "  \n",
    "1. 미니배치  \n",
    "    훈련 데이터 중에서 무작위로 다수의 데이터를 골라낸다.\n",
    "2. 기울기 계산\n",
    "    오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기르 ㄹ구한다.\n",
    "3. 매개변수 갱신\n",
    "    기울기를 사용하여 가중치 매개변수를 갱신한다.\n",
    "4. 반복\n",
    "    1 ~ 3 단계를 필요한 만큼 반복한다.  \n",
    "  \n",
    "  \n",
    "기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킴  \n",
    "따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있음  \n",
    "이것이 바로 __경사하강법(Gradient Descent)__  \n",
    "  \n",
    "3단계에서 매개변수를 갱신하는 방법의 종류는 다양함  \n",
    "  \n",
    "그 중 확률적경사하강법(Stochastic Gradient Descent, SGD)는 (현재의) 가중치를 기울기 방향으로 일정한 거리만큼 갱신  \n",
    "  \n",
    "$W\\leftarrow W-\\eta\\frac{\\partial{L}}{\\partial{W}}$  \n",
    "  \n",
    "- W : 갱신하는 가중치 매개변수\n",
    "- $\\frac{\\partial{L}}{\\partial{W}}$ : W에 대한 손실 함수의 기울기\n",
    "- $\\eta$ : 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "379fa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b4132",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# 신경망의 매개변수 갱신(예시)\n",
    "\n",
    "model = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "    ...\n",
    "    x_batch, t_batch = get_mini_batch(...) # 미니배치 획득\n",
    "    loss = model.forward(x_batch, t_batch)\n",
    "    model.backward()\n",
    "    optimizer.update(model.params, model.grads)\n",
    "    ...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e1d6c",
   "metadata": {},
   "source": [
    "## 1.4 신경망으로 문제를 풀다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4858b46",
   "metadata": {},
   "source": [
    "### 1.4.1 스파이럴 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b161e",
   "metadata": {},
   "source": [
    "<img src='./img/1/spiral_dataset.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11b7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape)\n",
    "print('t', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62dc5f",
   "metadata": {},
   "source": [
    "입력은 2차원 데이터, 분류할 클래스 수는 3개  \n",
    "위 그래프를 보면 직선만으로는 클래스들을 분리할 수 없음을 알 수 있음  \n",
    "비선형 분리를 학습해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7559897",
   "metadata": {},
   "source": [
    "### 1.4.2 신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00123f68",
   "metadata": {},
   "source": [
    "은닉층이 하나인 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd688bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    # 추론 수행\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # 순전파\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "    \n",
    "    # 역전파\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaeffa0",
   "metadata": {},
   "source": [
    "### 1.4.3 학습용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e51c0893",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 2 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 3 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 4 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 5 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 6 | 반복 10 / 10 | 손실 1.14 |\n",
      "| 에폭 7 | 반복 10 / 10 | 손실 1.16 |\n",
      "| 에폭 8 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 9 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 10 | 반복 10 / 10 | 손실 1.13 |\n",
      "| 에폭 11 | 반복 10 / 10 | 손실 1.12 |\n",
      "| 에폭 12 | 반복 10 / 10 | 손실 1.11 |\n",
      "| 에폭 13 | 반복 10 / 10 | 손실 1.09 |\n",
      "| 에폭 14 | 반복 10 / 10 | 손실 1.08 |\n",
      "| 에폭 15 | 반복 10 / 10 | 손실 1.04 |\n",
      "| 에폭 16 | 반복 10 / 10 | 손실 1.03 |\n",
      "| 에폭 17 | 반복 10 / 10 | 손실 0.96 |\n",
      "| 에폭 18 | 반복 10 / 10 | 손실 0.92 |\n",
      "| 에폭 19 | 반복 10 / 10 | 손실 0.92 |\n",
      "| 에폭 20 | 반복 10 / 10 | 손실 0.87 |\n",
      "| 에폭 21 | 반복 10 / 10 | 손실 0.85 |\n",
      "| 에폭 22 | 반복 10 / 10 | 손실 0.82 |\n",
      "| 에폭 23 | 반복 10 / 10 | 손실 0.79 |\n",
      "| 에폭 24 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 25 | 반복 10 / 10 | 손실 0.82 |\n",
      "| 에폭 26 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 27 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 28 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 29 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 30 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 31 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 32 | 반복 10 / 10 | 손실 0.77 |\n",
      "| 에폭 33 | 반복 10 / 10 | 손실 0.77 |\n",
      "| 에폭 34 | 반복 10 / 10 | 손실 0.78 |\n",
      "| 에폭 35 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 36 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 37 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 38 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 39 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 40 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 41 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 42 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 43 | 반복 10 / 10 | 손실 0.76 |\n",
      "| 에폭 44 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 45 | 반복 10 / 10 | 손실 0.75 |\n",
      "| 에폭 46 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 47 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 48 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 49 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 50 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 51 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 52 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 53 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 54 | 반복 10 / 10 | 손실 0.74 |\n",
      "| 에폭 55 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 56 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 57 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 58 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 59 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 60 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 61 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 62 | 반복 10 / 10 | 손실 0.72 |\n",
      "| 에폭 63 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 64 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 65 | 반복 10 / 10 | 손실 0.73 |\n",
      "| 에폭 66 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 67 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 68 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 69 | 반복 10 / 10 | 손실 0.70 |\n",
      "| 에폭 70 | 반복 10 / 10 | 손실 0.71 |\n",
      "| 에폭 71 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 72 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 73 | 반복 10 / 10 | 손실 0.67 |\n",
      "| 에폭 74 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 75 | 반복 10 / 10 | 손실 0.67 |\n",
      "| 에폭 76 | 반복 10 / 10 | 손실 0.66 |\n",
      "| 에폭 77 | 반복 10 / 10 | 손실 0.69 |\n",
      "| 에폭 78 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 79 | 반복 10 / 10 | 손실 0.68 |\n",
      "| 에폭 80 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 81 | 반복 10 / 10 | 손실 0.64 |\n",
      "| 에폭 82 | 반복 10 / 10 | 손실 0.66 |\n",
      "| 에폭 83 | 반복 10 / 10 | 손실 0.62 |\n",
      "| 에폭 84 | 반복 10 / 10 | 손실 0.62 |\n",
      "| 에폭 85 | 반복 10 / 10 | 손실 0.61 |\n",
      "| 에폭 86 | 반복 10 / 10 | 손실 0.60 |\n",
      "| 에폭 87 | 반복 10 / 10 | 손실 0.60 |\n",
      "| 에폭 88 | 반복 10 / 10 | 손실 0.61 |\n",
      "| 에폭 89 | 반복 10 / 10 | 손실 0.59 |\n",
      "| 에폭 90 | 반복 10 / 10 | 손실 0.58 |\n",
      "| 에폭 91 | 반복 10 / 10 | 손실 0.56 |\n",
      "| 에폭 92 | 반복 10 / 10 | 손실 0.56 |\n",
      "| 에폭 93 | 반복 10 / 10 | 손실 0.54 |\n",
      "| 에폭 94 | 반복 10 / 10 | 손실 0.53 |\n",
      "| 에폭 95 | 반복 10 / 10 | 손실 0.53 |\n",
      "| 에폭 96 | 반복 10 / 10 | 손실 0.52 |\n",
      "| 에폭 97 | 반복 10 / 10 | 손실 0.51 |\n",
      "| 에폭 98 | 반복 10 / 10 | 손실 0.50 |\n",
      "| 에폭 99 | 반복 10 / 10 | 손실 0.48 |\n",
      "| 에폭 100 | 반복 10 / 10 | 손실 0.48 |\n",
      "| 에폭 101 | 반복 10 / 10 | 손실 0.46 |\n",
      "| 에폭 102 | 반복 10 / 10 | 손실 0.45 |\n",
      "| 에폭 103 | 반복 10 / 10 | 손실 0.45 |\n",
      "| 에폭 104 | 반복 10 / 10 | 손실 0.44 |\n",
      "| 에폭 105 | 반복 10 / 10 | 손실 0.44 |\n",
      "| 에폭 106 | 반복 10 / 10 | 손실 0.41 |\n",
      "| 에폭 107 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 108 | 반복 10 / 10 | 손실 0.41 |\n",
      "| 에폭 109 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 110 | 반복 10 / 10 | 손실 0.40 |\n",
      "| 에폭 111 | 반복 10 / 10 | 손실 0.38 |\n",
      "| 에폭 112 | 반복 10 / 10 | 손실 0.38 |\n",
      "| 에폭 113 | 반복 10 / 10 | 손실 0.36 |\n",
      "| 에폭 114 | 반복 10 / 10 | 손실 0.37 |\n",
      "| 에폭 115 | 반복 10 / 10 | 손실 0.35 |\n",
      "| 에폭 116 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 117 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 118 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 119 | 반복 10 / 10 | 손실 0.33 |\n",
      "| 에폭 120 | 반복 10 / 10 | 손실 0.34 |\n",
      "| 에폭 121 | 반복 10 / 10 | 손실 0.32 |\n",
      "| 에폭 122 | 반복 10 / 10 | 손실 0.32 |\n",
      "| 에폭 123 | 반복 10 / 10 | 손실 0.31 |\n",
      "| 에폭 124 | 반복 10 / 10 | 손실 0.31 |\n",
      "| 에폭 125 | 반복 10 / 10 | 손실 0.30 |\n",
      "| 에폭 126 | 반복 10 / 10 | 손실 0.30 |\n",
      "| 에폭 127 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 128 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 129 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 130 | 반복 10 / 10 | 손실 0.28 |\n",
      "| 에폭 131 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 132 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 133 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 134 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 135 | 반복 10 / 10 | 손실 0.27 |\n",
      "| 에폭 136 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 137 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 138 | 반복 10 / 10 | 손실 0.26 |\n",
      "| 에폭 139 | 반복 10 / 10 | 손실 0.25 |\n",
      "| 에폭 140 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 141 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 142 | 반복 10 / 10 | 손실 0.25 |\n",
      "| 에폭 143 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 144 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 145 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 146 | 반복 10 / 10 | 손실 0.24 |\n",
      "| 에폭 147 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 148 | 반복 10 / 10 | 손실 0.23 |\n",
      "| 에폭 149 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 150 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 151 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 152 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 153 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 154 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 155 | 반복 10 / 10 | 손실 0.22 |\n",
      "| 에폭 156 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 157 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 158 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 159 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 160 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 161 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 162 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 163 | 반복 10 / 10 | 손실 0.21 |\n",
      "| 에폭 164 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 165 | 반복 10 / 10 | 손실 0.20 |\n",
      "| 에폭 166 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 167 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 168 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 169 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 170 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 171 | 반복 10 / 10 | 손실 0.19 |\n",
      "| 에폭 172 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 173 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 174 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 175 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 176 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 177 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 178 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 179 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 180 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 181 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 182 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 183 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 184 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 185 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 186 | 반복 10 / 10 | 손실 0.18 |\n",
      "| 에폭 187 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 188 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 189 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 190 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 191 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 192 | 반복 10 / 10 | 손실 0.17 |\n",
      "| 에폭 193 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 194 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 195 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 196 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 197 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 198 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 199 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 200 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 201 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 202 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 203 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 204 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 205 | 반복 10 / 10 | 손실 0.16 |\n",
      "| 에폭 206 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 207 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 208 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 209 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 210 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 211 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 212 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 213 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 214 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 215 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 216 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 217 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 218 | 반복 10 / 10 | 손실 0.15 |\n",
      "| 에폭 219 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 220 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 221 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 222 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 223 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 224 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 225 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 226 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 227 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 228 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 229 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 230 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 231 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 232 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 233 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 234 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 235 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 236 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 237 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 238 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 239 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 240 | 반복 10 / 10 | 손실 0.14 |\n",
      "| 에폭 241 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 242 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 243 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 244 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 245 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 246 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 247 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 248 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 249 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 250 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 251 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 252 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 253 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 254 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 255 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 256 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 257 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 258 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 259 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 260 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 261 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 262 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 263 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 264 | 반복 10 / 10 | 손실 0.13 |\n",
      "| 에폭 265 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 266 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 267 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 268 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 269 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 270 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 271 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 272 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 273 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 274 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 275 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 276 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 277 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 278 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 279 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 280 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 281 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 282 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 283 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 284 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 285 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 286 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 287 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 288 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 289 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 290 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 291 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 292 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 293 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 294 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 295 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 296 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 297 | 반복 10 / 10 | 손실 0.12 |\n",
      "| 에폭 298 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 299 | 반복 10 / 10 | 손실 0.11 |\n",
      "| 에폭 300 | 반복 10 / 10 | 손실 0.11 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 2. 데이터 읽기, 모델과 옵티마이저 생성\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr = learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 3. 데이터 뒤섞기\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        \n",
    "        # 4. 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        # 5. 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d | 반복 %d / %d | 손실 %.2f |' % (epoch+1, iters+1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67320512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "\n",
    "### 학습에 따른 loss의 변화 시각화\n",
    "\n",
    "# 학습 결과 플롯\n",
    "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
    "plt.xlabel('반복x10')\n",
    "plt.ylabel('손실')\n",
    "plt.show()\n",
    "\n",
    "### 결정 경계(decision boundary) 시각화\n",
    "\n",
    "# 경계 영역 플롯\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "score = model.predict(X)\n",
    "predict_cls = np.argmax(score, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('off')\n",
    "\n",
    "# 데이터점 플롯\n",
    "x, t = spiral.load_data()\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c2338",
   "metadata": {},
   "source": [
    "### 1.4.4 Trainer 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9e23db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from common.np import *  # import numpy as np\n",
    "from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa0ac7",
   "metadata": {},
   "source": [
    "- x : 입력 데이터\n",
    "- t : 정답 레이블\n",
    "- max_epoch(=10) : 학습을 수행하는 에폭 수\n",
    "- batch_size(=32) : 미니배치 크기\n",
    "- eval_interval(=20) : 결과(평균 손실 등)를 출력하는 간격\n",
    "- max_grad(=None) : 기울기 최대 노름(norm)\n",
    "    기울기 노름이 이 값을 넘어서면 기울기를 줄인다(이를 기울기 클리핑이라고 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c804f",
   "metadata": {},
   "source": [
    "Trainer 클래스를 활용한 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "280fac23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
      "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
      "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
      "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
      "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from dataset import spiral\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3370c6",
   "metadata": {},
   "source": [
    "## 1.5 계산 고속화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316753b",
   "metadata": {},
   "source": [
    "### 1.5.1 비트 정밀도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9e74130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac6930",
   "metadata": {},
   "source": [
    "넘파이의 64비트 부동소수점 수를 표준으로 사용  \n",
    "그러나,  \n",
    "- 신경망의 추론과 학습은 32비트 부동소수점 수로도 인식률을 거의 떨어뜨리는 일 없이 수행할 수 있음\n",
    "- 메모리 관점에서 항상 64비트에 비해 43비트가 더 조흥ㅁ\n",
    "- 신경망 계산 시 데이터를 전송하는 '버스 대역 폭(bus bandwidth)'이 병목되는 경우가 종종 있으므로, 데이터 타입이 작은게 좋음\n",
    "- 계산 속도 측면에서도 32비트 부동소수점 수가 일반적으로 더 빠름  \n",
    "  \n",
    "넘파이에서 32비트 부동소수점 수를 사용하려면 다음과 같이 데이터 타입을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbcd27d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(3).astype(np.float32)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "152e403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.randn(3).astype('f')\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea4806",
   "metadata": {},
   "source": [
    "신경망 추론으로 한정하면, 16비트 부동소수점 수를 사용해도 인식률이 거의 떨어지지 않음  \n",
    "하지만, 일반적으로 CPU와 GPU는 연산 자체를 32비트로 수행하므로 16비트로 변환하더라도 계산 자체는 32비트로 이뤄짐  \n",
    "학습된 가중치를 저장할 때는 16비트 부동소수점 수가 여전히 유효(32비트에 비해 절반의 용량만 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19fa95",
   "metadata": {},
   "source": [
    "### 1.5.2 GPU(쿠파이)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771da967",
   "metadata": {},
   "source": [
    "딥러닝의 계산은 대량의 곱하기 연산으로 구성  \n",
    "따라서 CPU보다 GPU가 유리  \n",
    "  \n",
    "쿠파이 : GPU를 이용해 병렬 계산을 수행해주는 라이브러리  \n",
    "    넘파이와 호환되는 API를 제공함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
