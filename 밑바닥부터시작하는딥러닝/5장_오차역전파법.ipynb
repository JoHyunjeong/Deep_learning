{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c0ca41",
   "metadata": {},
   "source": [
    "4장에서는 신경망의 가중치 매개변수의 기울기(정확히는 가중치 매개변수에 대한 손실 함수의 기울기)를 수치 미분을 사용해 구함  \n",
    "수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점  \n",
    "  \n",
    "따라서 이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 __'오차역전파법 backpropagation'__을 다룸  \n",
    "* 오차역전파법 : 오차를 역(반대 방향)으로 전파하는 방법(backward propagation of errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aba3e",
   "metadata": {},
   "source": [
    "### 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1729a2",
   "metadata": {},
   "source": [
    "계산그래프(computational graph)는 계산 과정을 그래프로 나타낸 것  \n",
    "복수의 노드(node)와 에지(edge, 노드 사이의 직선)으로 표현  \n",
    "  \n",
    "노드는 원(o)으로 표기하고 원 안에 연산 내용을 적음  \n",
    "계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982511c5",
   "metadata": {},
   "source": [
    "<img src=\"./img/계산그래프_1.png"\ >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2f356",
   "metadata": {},
   "source": [
    "여기서 '계산을 왼쪽에서 오른쪽으로 진행'하는 단계를 순전파(forward propagation)이라고 함  \n",
    "역전파(backward propagation)은 반대방향으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1e765",
   "metadata": {},
   "source": [
    "### 계산 그래프의 이점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb00cc0",
   "metadata": {},
   "source": [
    "1. '국소적 계산'을 전파함으로써 최종 결과를 얻는다는 점  \n",
    "    국소적이란 '자신과 직접 관련된 작은 범위'라는 뜻  \n",
    "    복잡한 계산도 분해하면 단순한 계산으로 구성됨  \n",
    "    전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 할 수 있음 \n",
    "  \n",
    "2. 중간 계산 결과를 모두 보관할 수 있음\n",
    "3. 순전파와 역전파를 활용하여 각 변수의 미분을 효율적으로 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8a8b1",
   "metadata": {},
   "source": [
    "<img src=\"./img/계산그래프_2.png\" width =500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d58de",
   "metadata": {},
   "source": [
    "### 연쇄법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939fe743",
   "metadata": {},
   "source": [
    "연쇄법칙 : 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다. \n",
    "  \n",
    "ex)  \n",
    "$z = t^2$  \n",
    "$t = x + y$  \n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}$(x에 대한 z의 미분)은 $\\frac{\\partial{z}}{\\partial{t}}$(t에 대한 z의 미분)과 $\\frac{\\partial{t}}{\\partial{x}}$(x에 대한 t의 미분)의 곱으로 나타낼 수 있음  \n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}}$  \n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\not{\\partial{t}}}\\frac{\\not{\\partial{t}}}{\\partial{x}}$  \n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{t}}=2t$,  $\\frac{\\partial{t}}{\\partial{x}}=1$  \n",
    "  \n",
    "$\\frac{\\partial{z}}{\\partial{x}}=\\frac{\\partial{z}}{\\partial{t}}\\frac{\\partial{t}}{\\partial{x}}=2t\\cdot 1=2(x+y)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb0af6",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\계산그래프_3.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495d01f",
   "metadata": {},
   "source": [
    "역전파의 계산 절차에서는 노드로 들어온 입력 신호에 그 노드의 국소적 미분(편미분)을 곱한 후 다음 노드로 전달  \n",
    "즉, 역전파가 하는 일은 연쇄법칙의 원리와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33ce1d",
   "metadata": {},
   "source": [
    "### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d7304",
   "metadata": {},
   "source": [
    "#### 덧셈 노드의 역전파  \n",
    "  \n",
    "덧셈 노드의 역전파는 입력 값을 그대로 흘려보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6d510",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\역전파_덧셈_1.png\" width=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda25b6",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\역전파_덧셈_2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fba677",
   "metadata": {},
   "source": [
    "#### 곱셈 노드의 역전파  \n",
    "  \n",
    "곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄  \n",
    "  \n",
    "덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않았지만,  \n",
    "곱셈의 역전파는 순방향 입력 신호의 값이 필요하므로  \n",
    "곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b652ca",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\역전파_곱셈_1.png\" width=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d07498",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\역전파_곱셈_2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bd278",
   "metadata": {},
   "source": [
    "### 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd92320",
   "metadata": {},
   "source": [
    "#### 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2a55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer: # 곱셈 계층\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y): # 순전파\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 역전파 / dout : 상류에서 넘어온 미분값\n",
    "        dx = dout * self.y # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae16b87",
   "metadata": {},
   "source": [
    "ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9688e2b",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\단순한 계층 구현.png\" width=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc3811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4c5c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6b9bd",
   "metadata": {},
   "source": [
    "#### 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c46928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self): # 덧셈 계층은 초기화가 필요 없음\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e42e00",
   "metadata": {},
   "source": [
    "ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55c877",
   "metadata": {},
   "source": [
    "<img src=\".\\img\\계산그래프_예시.png\" width=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a94641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax) # (4)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18070041",
   "metadata": {},
   "source": [
    "### 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79d711",
   "metadata": {},
   "source": [
    "### ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f43c17",
   "metadata": {},
   "source": [
    "$y =  \\Bigg\\{\n",
    " \\begin{matrix}\n",
    "  x  (x>0) \\\\\n",
    "  0  (x\\leq0)\n",
    " \\end{matrix}$  \n",
    "<br>\n",
    "$\\frac{\\partial{y}}{\\partial{x}} =  \\Bigg\\{\n",
    " \\begin{matrix}\n",
    "  1  (x>0) \\\\\n",
    "  0  (x\\leq0)\n",
    " \\end{matrix}$  \n",
    "<br>\n",
    "순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘림  \n",
    "반면, 순전파 때 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않음(0을 보냄)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7665430",
   "metadata": {},
   "source": [
    "<img src='.\\img\\relu.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaad27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec2e8d",
   "metadata": {},
   "source": [
    "mask는 True/False로 구성된 넘파이 배열로,  \n",
    "순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외(0보다 큰 원소)는 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8b808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0,-0.5],[-2.0,3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad50fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a179cf",
   "metadata": {},
   "source": [
    "순전파 때의 입력 값이 0 이하면 뎍전파 때의 값은 0이 되어야 함.  \n",
    "따라서 역전파 때는 순전파 때 만들어둔 mask를 써서 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a202a8",
   "metadata": {},
   "source": [
    "### Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7e924",
   "metadata": {},
   "source": [
    "$y=\\frac{1}{1+exp(-x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842abb2",
   "metadata": {},
   "source": [
    "<img src='.\\img\\sigmoid.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f2390",
   "metadata": {},
   "source": [
    "#### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab246352",
   "metadata": {},
   "source": [
    "<img src='.\\img\\sigmoid_backward.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148c642",
   "metadata": {},
   "source": [
    "##### 1단계  \n",
    "  \n",
    "'/'노드, 즉 $y=\\frac{1}{x}$를 미분하면,  \n",
    "<br>\n",
    "$\\frac{\\partial{y}}{\\partial{x}}=-\\frac{1}{x^2}=-y^2$\n",
    "<br>\n",
    "즉, 역전파 때는 상류에서 흘러온 값에 $-y^2$(순전파의 출력을 제곱한 후 마이너스를 붙인 값)을 곱해서 하류로 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd463f",
   "metadata": {},
   "source": [
    "##### 2단계   \n",
    "  \n",
    "'+'노드는 상류의 값을 여과 없이 하류로 보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968b65e",
   "metadata": {},
   "source": [
    "##### 3단계  \n",
    "  \n",
    "'exp'노드는 $y=exp(x)$연산을 수행하며,  \n",
    "<br>\n",
    "$\\frac{\\partial{y}}{\\partial{x}}=exp(x)$\n",
    "<br>\n",
    "이 예에서는 상류의 값에 순전파 때의 출력(exp(-x))를 곱해 하류로 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d3605",
   "metadata": {},
   "source": [
    "##### 4단계  \n",
    "  \n",
    "'$\\times$'노드는 순전파 때의 값을 '서로 바꿔' 곱함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4816db",
   "metadata": {},
   "source": [
    "##### 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c642882",
   "metadata": {},
   "source": [
    "<img src='.\\img\\sigmoid_정리_1.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1fa92",
   "metadata": {},
   "source": [
    "최종 값인 $\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)$를 정리하면,\n",
    "<br>\n",
    "$\\frac{\\partial{L}}{\\partial{y}}y^2exp(-x)=\\frac{\\partial{L}}{\\partial{y}}\\frac{1}{(1+exp(-x))^2}exp(-x)\n",
    "= \\frac{\\partial{L}}{\\partial{y}}\\frac{1}{1+exp(-x)}\\frac{exp(-x)}{1+exp(-x)}=\\frac{\\partial{L}}{\\partial{y}}y(1-y)$  \n",
    "<br>\n",
    "sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94483b16",
   "metadata": {},
   "source": [
    "<img src='.\\img\\sigmoid_정리_2.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea1c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    # 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값을 사용함\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbc485",
   "metadata": {},
   "source": [
    "### Affine/Softmax 계층 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8316f9",
   "metadata": {},
   "source": [
    "### Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a38f4",
   "metadata": {},
   "source": [
    "신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 '어파인 변환 affine transformation'이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c875d1",
   "metadata": {},
   "source": [
    "<img src='.\\img\\affine_1.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69b62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2) # 입력\n",
    "W = np.random.rand(2,3) # 가중치\n",
    "B = np.random.rand(3) # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c39db",
   "metadata": {},
   "source": [
    "##### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47334b",
   "metadata": {},
   "source": [
    "<img src='.\\img\\affine_2.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51902b14",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{L}}{\\partial{X}}=\\frac{\\partial{L}}{\\partial{Y}}\\cdot W^T$  \n",
    "<br>\n",
    "$\\frac{\\partial{L}}{\\partial{W}}=X^T \\cdot \\frac{\\partial{L}}{\\partial{Y}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1cb3e",
   "metadata": {},
   "source": [
    "여기서, $W^T$의 T는 전치행렬을 뜻함  \n",
    "<br>\n",
    "$W(2,3)=\n",
    " \\begin{pmatrix}\n",
    "  W_{11} & W_{12} & W_{13}  \\\\\n",
    "  W_{21} & W_{22} & W_{23}\n",
    " \\end{pmatrix}$  \n",
    "<br>\n",
    "$W^T(3,2)=\n",
    " \\begin{pmatrix}\n",
    "  W_{11} & W_{21}  \\\\\n",
    "  W_{12} & W_{22}  \\\\\n",
    "  W_{13} & W_{23}\n",
    " \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781db6f4",
   "metadata": {},
   "source": [
    "* $X$와 $\\frac{\\partial{L}}{\\partial{X}}$은 같은 형상  \n",
    "* $W$와 $\\frac{\\partial{L}}{\\partial{W}}$은 같은 형상  \n",
    "\n",
    "<br>\n",
    "$X=(x_0, x_1, \\cdots, x_n)$  \n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{X}}=(\\frac{\\partial{L}}{\\partial{x_0}},\\frac{\\partial{L}}{\\partial{x_1}}, \\cdots, \\frac{\\partial{L}}{\\partial{x_n}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e377a2",
   "metadata": {},
   "source": [
    "<img src='.\\img\\affine_3.png' width=500>  \n",
    "<br>\n",
    "행렬 곱('dot'노드)의 역전파는 행력의 대응하는 차원의 원소 수가 일치하도록 곱을 조립하여 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f485207",
   "metadata": {},
   "source": [
    "### 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fce1b5",
   "metadata": {},
   "source": [
    "데이터 N개를 묶어 순전파하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77319cb0",
   "metadata": {},
   "source": [
    "<img src='.\\img\\affine_4.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f3cf4",
   "metadata": {},
   "source": [
    "순전파의 편향 덧셈은 각각의 데이터(1번째 데이터, 2번째 데이터, ...)에 더해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97e9104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N=2(데이터가 2개)인 경우의 예시\n",
    "X_dot_W = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef374062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d23aea",
   "metadata": {},
   "source": [
    "따라서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ccf7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8913cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7f086",
   "metadata": {},
   "source": [
    "편향의 역전파는 데이터에 대한 미분을 데이터마다 더해서 구함  \n",
    "그렇기 때문에 np.sum()에서 데이터를 단위로 한 축에 대해(axis=0)의 총합을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f24d936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87246f7",
   "metadata": {},
   "source": [
    "### Softmax-with-Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17306b6e",
   "metadata": {},
   "source": [
    "Softmax-with-Loss : Softmax계층과 Cross Entropy Error계층으 ㅣ조합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9739c",
   "metadata": {},
   "source": [
    "소프트맥스 함수  \n",
    "- 출력층에서 사용  \n",
    "- 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력  \n",
    "- 추론과정에서는 일반적으로 사용하지 않고, 학습과정에서 사용  \n",
    "\n",
    "<br>\n",
    "*신경망에서 정규화하지 않은 출력 결과(Softmax앞의 Affine계층의 출력)를 점수 score이라고 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88830d42",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_1.png' width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86b424",
   "metadata": {},
   "source": [
    "간소화한 sotfmax-with-loss계층의 계산 그래프  \n",
    "<br>\n",
    "\n",
    "<img src='.\\img\\softmax_loss_2.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff648b",
   "metadata": {},
   "source": [
    "#### 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bbfbc",
   "metadata": {},
   "source": [
    "softmax계층  \n",
    "  \n",
    "$y_k=\\frac{exp(a_k)}{\\sum_{i=1}^nexp(a_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace80f2b",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_3.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9196735",
   "metadata": {},
   "source": [
    "Cross Entropy Error계층  \n",
    "<br>\n",
    "$L=-\\sum_kt^klogy_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bce6d",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_4.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7cbe98",
   "metadata": {},
   "source": [
    "#### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bef75",
   "metadata": {},
   "source": [
    "__Cross Entropy Error계층의 역전파__\n",
    "- 역전파의 초깃값, 즉 아래 그림의 가장 오른쪽 역전파의 값은 1임($\\frac{\\partial{L}}{\\partial{L}}=1이므로$)\n",
    "- '$\\times$'노드의 역전파는 순전파 시의 입력들의 값을 '서로 바꿔' 상류의 미분에 곱하고 하류로 흘림\n",
    "- '+'노드에서는 상류에서 전해지는 미분을 그대로 흘림\n",
    "- 'log'노드의 역전파는 다음 식을 따름    \n",
    "$y=logx$  \n",
    "<br>\n",
    "$\\frac{\\partial{y}}{\\partial{x}}=\\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8558fbf",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_5.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883b5ed",
   "metadata": {},
   "source": [
    "결과는 ($-\\frac{t_1}{y_1},-\\frac{t_2}{y_2},-\\frac{t_3}{y_4}$)이며, 이 값이 softmax 계층으로 역전파 입력이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87aed68",
   "metadata": {},
   "source": [
    "__Softmax계층의 역전파__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec364a3b",
   "metadata": {},
   "source": [
    "##### 1단계  \n",
    "  \n",
    "앞 계층(Cross Entropy Error계층)의 역전파 값이 흘러옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eac13",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_6.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92360e9",
   "metadata": {},
   "source": [
    "##### 2단계   \n",
    "  \n",
    "'$\\times$'노드에서는 순전파의 입력들을 '서로 바꿔' 곱함  \n",
    "<br>\n",
    "$-\\frac{t_1}{y_1}exp(a_1)=-t_1\\frac{S}{exp(a_1)}exp(a_1)=-t_1S$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d8867",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_7.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db080e90",
   "metadata": {},
   "source": [
    "##### 3단계   \n",
    "<br>\n",
    "순전파 때 여러 갈래로 나뉘어 흘렸다면 역전파 때는 그 반대로 흘러온 여러 값을 더함  \n",
    "\n",
    "따라서 여기선 3개의 갈라지 역전파의 값($-t_1S, -t_2S, -t_3S$)이 더해짐  \n",
    "\n",
    "더해진 값이 '/'노드의 역전파를 거쳐 $\\frac{1}{S}(t_1+t_2+t_3)$이 됨  \n",
    "<br>\n",
    "<span style='color:green'>'/'노드의 역전파는 '상류에서 흘러온 값'에 '순전파 때의 출력을 제곱한 후 마이너스를 붙인 값'을 곱해 하류로 전달  \n",
    "여기서 '상류에서 흘러온 값'은 $(-t_1S)+(-t_2S)+(-t_3S)=-S(t_1+t_2+t_3)$이고,  \n",
    "'순전파 때의 출력'은 $\\frac{1}{S}$이므로  \n",
    "역전파의 출력은 $-S(t_1+t_2+t_3) \\times -\\frac{1}{s^2}=\\frac{1}{S}(t_1+t_2+t_3)$이 됨\n",
    "\n",
    "\n",
    "여기서 $(t_1+t_2+t_3)$는 '원-핫 벡터'로 표현된 정답 레이블임  \n",
    "\n",
    "따라서 $t_1+t_2+t_3=1$이 됨  \n",
    "\n",
    "그러므로 $\\frac{1}{S}$가 됨  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc854ab",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_8.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21337aaa",
   "metadata": {},
   "source": [
    "##### 4단계   \n",
    "<br>\n",
    "'+'노드는 입력을 여과없이 보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daaf6c",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_9.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535f11a",
   "metadata": {},
   "source": [
    "##### 5단계   \n",
    "<br>\n",
    "'$\\times$'노드는 입력을 '서로 바꾼' 곱셈  \n",
    "\n",
    "여기에서는 $y_1=\\frac{exp(a_1)}{S}$를 이용하여 식을 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa8b96",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_10.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee85a2",
   "metadata": {},
   "source": [
    "##### 6단계  \n",
    "<br>\n",
    "'exp'노드에서는 다음 관계식이 성립함  \n",
    "\n",
    "$y = exp(x)$  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=exp(x)$  \n",
    "  \n",
    "두 갈래의 입력의 합에 $exp(a_1)$를 곱한 수치가 여기에서 구하는 역전파  \n",
    "\n",
    "$y_1=\\frac{exp(a_1)}{S}$이므로  \n",
    "역전파 값은 $y_1-t_1$이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44703484",
   "metadata": {},
   "source": [
    "<img src='.\\img\\softmax_loss_11.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce171598",
   "metadata": {},
   "source": [
    "softmax 계층의 출력은 $(y_1, y_2, y_3)$이고  \n",
    "정답레이블은 $(t_1, t_2, t_3)$  \n",
    "따라서, 역전파의 결과인 $y_1-t_1, y_2-t_2, y_3-t_3$은 softmax계층의 출력과 정답레이블의 차분  \n",
    "신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해짐  \n",
    "<br>\n",
    "<br>\n",
    "'소프트맥스 함수'의 손실함수를 '교차 엔트로피 오차'로 사용했을 때와 마찬가지로  \n",
    "회귀에서 출력층에서 사용하는 '항등 함수'의 손실함수를 '오차제곱합' 사용하였을 때  \n",
    "역전파의 결과가 $y_1-t_1, y_2-t_2, y_3-t_3$로 말끔이 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfbcab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # softmax의 출력\n",
    "        self.t = None # 정답레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3cc6a",
   "metadata": {},
   "source": [
    "역전파 때는 전파하는 값을 배치의 수(batch_size)로 나눠서 데이터를 1개당 오차를 앞 계층으로 전파함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adccfc7",
   "metadata": {},
   "source": [
    "### 신경망 학습의 전체 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6317ca5",
   "metadata": {},
   "source": [
    "__전제__  \n",
    "가중치와 편향을 훈련데이터에 적응하도록 조정하는 과정을 '학습'이라 함  \n",
    "신경망 학습은 다음과 같이 4단계로 수행  \n",
    "  \n",
    "__1단계-미니배치__  \n",
    "훈련 데이터 중 일부를 무작위로 가져옴  \n",
    "  \n",
    "__2단계-기울기 산출__  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함  \n",
    "  \n",
    "__3단계-매개변수 갱신__  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신  \n",
    "  \n",
    "__4단계-반복__\n",
    "1~3단계를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396cd529",
   "metadata": {},
   "source": [
    "### 오차역전파법을 적용한 신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a8828",
   "metadata": {},
   "source": [
    "__2층 신경망을 TwoLayerNet 클래스로 구현__  \n",
    "  \n",
    "__TwoLayerNet 클래스의 인스턴스 변수__\n",
    "- params : 딕셔너리 변수로, 신경망의 매개변수를 보관\n",
    "- layers : 순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관  \n",
    "        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지  \n",
    "- lastLayer : 신경망의 마지막 계층\n",
    "        이 예에서는 SoftmaxWithLoss 계층  \n",
    "  \n",
    "__TwoLayerNet 클래스의 메서드__  \n",
    "- \\_\\_init__(self, input_size, hidden_size, output_size, weight_init_std) : 초기화를 수행\n",
    "- predict(self, x) : 예측(추론)을 수행\n",
    "- loss(self, x, t) : 손실 함수의 값을 구함  \n",
    "- accuracy(self, x, t) : 정확도를 구함\n",
    "- numerical_gradient(self, x, t) : 가중치 매개변수의 기울기를 수치 미분 방식으로 구함\n",
    "- gradient(self, x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e877f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "        \n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b288c68",
   "metadata": {},
   "source": [
    "OrderedDict은 순서가 있는 딕셔너리  \n",
    "순전파 때는 추가한 순서대로 각 계층의 forward()메스드를 호출  \n",
    "역전파 때는 계층을 반대 순서로 호출  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b394522",
   "metadata": {},
   "source": [
    "### 오차역전파법으로 구한 기울기 검증하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483c292",
   "metadata": {},
   "source": [
    "- 수치미분은 느린 대신 구현하기 쉬움    \n",
    "- 오차역전파법은 빠르나 구현하기 복잡    \n",
    "\n",
    "두 방식으로 구한 기울기가 일치한지를 확인하는 작업을 기울기 확인(gradient check)이라고 함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e26a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.905533936075847e-10\n",
      "b1:2.135783551395973e-09\n",
      "W2:4.408335094940636e-09\n",
      "b2:1.400904222223498e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ff498",
   "metadata": {},
   "source": [
    "올바르게 구현했다면 0에 아주 가까운 작은 값이 됨  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4d448",
   "metadata": {},
   "source": [
    "### 오차역전파법을 사용한 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf01e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13235 0.1305\n",
      "0.9029333333333334 0.9051\n",
      "0.9216166666666666 0.9237\n",
      "0.9367833333333333 0.9367\n",
      "0.94425 0.9429\n",
      "0.95265 0.9479\n",
      "0.9561666666666667 0.9533\n",
      "0.9604 0.9574\n",
      "0.96325 0.9584\n",
      "0.9663666666666667 0.9624\n",
      "0.9686166666666667 0.9636\n",
      "0.9707333333333333 0.9638\n",
      "0.9737666666666667 0.9658\n",
      "0.97495 0.9663\n",
      "0.9764666666666667 0.9695\n",
      "0.9770833333333333 0.9681\n",
      "0.979 0.9697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(수치 미분 방식에 비해 훨씬 빠름)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
