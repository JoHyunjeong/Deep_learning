{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227e88a0",
   "metadata": {},
   "source": [
    "# <center>제4고지 신경망 만들기</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c556620",
   "metadata": {},
   "source": [
    "# 37단계 텐서를 다루다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88768f",
   "metadata": {},
   "source": [
    "지금까지는 변수로 주로 '스칼라'를 다뤘으나 머신러닝 데이터로는 벡터나 행렬 등의 '텐서'가 주로 쓰이므로  \n",
    "텐서를 사용할 때의 주의점을 알아보면서 DeZero의 확장을 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b0d52",
   "metadata": {},
   "source": [
    "전에 구현한 add, mul 등의 함수는 원소별 계산이 이뤄짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c006bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[ 0.84147098  0.90929743  0.14112001]\n",
      "          [-0.7568025  -0.95892427 -0.2794155 ]])\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.sin(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb12e9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[11 22 33]\n",
      "          [44 55 66]])\n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "c = Variable(np.array([[10,20,30],[40,50,60]]))\n",
    "y = x + c\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b53a5",
   "metadata": {},
   "source": [
    "텐서를 이용해 역전파를 계산해도 문제 없음!  \n",
    "이유는 다음과 같음\n",
    "- 지금까지 구현한 DeZero 함수에 '텐서'를 건네면 텐서의 원소마다 '스칼라'로 계산함\n",
    "- 텐서의 원소별 '스칼라' 계산이 이루어지면 '스칼라'를 가정해 구현한 역전파는 '텐서'의 원소별 계산에서도 성립함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98c02a",
   "metadata": {},
   "source": [
    "텐서의 미분을 머신러닝에서는 '기울기'라고 함  \n",
    "기울기의 형상과 데이터(순전파 때의 데이터)의 형상은 일치함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b394e91",
   "metadata": {},
   "source": [
    "__[보충] 텐서 사용 시의 역전파__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57ec9f",
   "metadata": {},
   "source": [
    "x와 y는 벡터이고, 두 벡터 모두 원소 수가 n개라고 가정하였을 때  \n",
    "$y=F(x)$ 함수의 y의 x에 대한 미분은 다음과 같다.  \n",
    "  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=\\begin{pmatrix}\n",
    "\\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\cdots & \\frac{\\partial{y_1}}{\\partial{x_n}}\\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\cdots & \\frac{\\partial{y_2}}{\\partial{x_n}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial{y_n}}{\\partial{x_1}} & \\frac{\\partial{y_n}}{\\partial{x_2}} & \\cdots & \\frac{\\partial{y_n}}{\\partial{x_n}}\n",
    "\\end{pmatrix}$  \n",
    "  \n",
    "x, y 모두 벡터이므로 그 미분은 위와 같이 '행렬'의 형태가 됨  \n",
    "이 행렬을 야코비 행렬(Jacobian matrix)라고 함  \n",
    "  \n",
    "만약 y가 스칼라면 y의 x에 대한 미분은 다음과 같음  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=\\begin{pmatrix}\n",
    "\\frac{\\partial{y}}{\\partial{x_1}} & \\frac{\\partial{y}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{y}}{\\partial{x_1}}\n",
    "\\end{pmatrix}$  \n",
    "  \n",
    "이는 1 $\\times$ n의 야코비 행렬이며, 행 벡터(가로로 나열된 벡터)로 간주할 수 있음  \n",
    "  \n",
    "합성 함수 $y=F(x)$가 $a=A(x), b=B(a), y=C(b)$라는 3개의 함수로 구성되어 있고, x, a, b는 벡터이며 원소수가 모두 n개라고 가정하였을 때  \n",
    "y의 x에 대한 미분은 연쇄 법칙에 의해 다음과 같음  \n",
    "  \n",
    "$\\frac{\\partial{y}}{\\partial{x}}=\\frac{\\partial{y}}{\\partial{b}}\\frac{\\partial{b}}{\\partial{a}}\\frac{\\partial{a}}{\\partial{x}}$  \n",
    "  \n",
    "여기서, $\\frac{\\partial{y}}{\\partial{b}}\\frac{\\partial{b}}{\\partial{a}}$는 야코비 행렬을 나타내며 '행렬의 곱'으로 계산함  \n",
    "  \n",
    "행렬곱을 계산하는 순서에는 두 가지 방법이 있음  \n",
    "1. 자동 미분의 forward 모드 : 입력 쪽에서 출력 쪽으로 괄호를 친다.  \n",
    "  \n",
    "<img src='./img/4/forward_mode.png' width=400>  \n",
    "  \n",
    "중간의 행렬 곱의 결과가 다시 행렬이 됨  \n",
    "ex) $\\frac{\\partial{b}}{\\partial{a}}\\frac{\\partial{a}}{\\partial{x}}$의 결과는 n $\\times$ n 행렬  \n",
    "  \n",
    "2. 자동 미분의 reverse 모드 : 출력 쪽에서 입력 쪽으로 괄호를 친다.  \n",
    "  \n",
    "<img src='./img/4/reverse_mode.png' width=400>  \n",
    "  \n",
    "y가 스칼라이므로 중간의 행렬 곱의 결과는 모두 백터(행 벡터)가 됨  \n",
    "따라서 벡터와 야코비 행렬의 곱으로 구성됨\n",
    "ex) $\\frac{\\partial{y}}{\\partial{b}}\\frac{\\partial{b}}{\\partial{a}}$의 결과는 n개의 원소로 구성된 벡터  \n",
    "  \n",
    "행렬과 행렬의 곱보다 벡터와 행렬의 곱의 계산량이 적으므로 reverse 모드, 즉 역전파 쪽의 계산 효율이 좋음  \n",
    "그렇기 때문에 역전파에서는 각 함수에 대해 벡터와 야코비 행렬의 곱을 계산함  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b73c7",
   "metadata": {},
   "source": [
    "결과만 필요한 상황이라면 야코비 행렬을 구하여 '행렬의 곱'을 계산할 필요가 없음  \n",
    "  \n",
    "$a=A(x)$가 원소별 연산을 수행한 경우를 가정(ex: a=sin(x))  \n",
    "이 함수의 야코비 행렬은 다음과 같음  \n",
    "  \n",
    "$\\begin{pmatrix}\n",
    "\\frac{\\partial{a_1}}{\\partial{x_1}} & 0 & \\cdots & 0\\\\\n",
    "0 & \\frac{\\partial{a_2}}{\\partial{x_2}} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{\\partial{a_n}}{\\partial{x_n}}\n",
    "\\end{pmatrix}$\n",
    "  \n",
    "위와 같이 원소별 연산의 야코비 행렬은 대각 행렬(대각 성분 외에는 모두 0인 행렬)이 됨  \n",
    "$x_i$는 $a_i$에만 영향을 주기 때문  \n",
    "  \n",
    "또한, 야코비 행렬이 대각 행렬이라면 벡터와 야코비 행렬의 곱은 다음과 같음  \n",
    "  \n",
    "$\\frac{\\partial{y}}{\\partial{a}}\\frac{\\partial{a}}{\\partial{x}}=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial{y}}{\\partial{a_1}} & \\frac{\\partial{y}}{\\partial{a_2}} & \\cdots & \\frac{\\partial{y}}{\\partial{a_n}}\n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial{a_1}}{\\partial{x_1}} & 0 & \\cdots & 0\\\\\n",
    "0 & \\frac{\\partial{a_2}}{\\partial{x_2}} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\frac{\\partial{a_n}}{\\partial{x_n}}\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "\\frac{\\partial{y}}{\\partial{a_1}}\\frac{\\partial{a_1}}{\\partial{x_1}} & \\frac{\\partial{y}}{\\partial{a_2}}\\frac{\\partial{a_2}}{\\partial{x_2}} & \\cdots & \\frac{\\partial{y}}{\\partial{a_n}}\\frac{\\partial{a_n}}{\\partial{x_n}}\n",
    "\\end{pmatrix}$  \n",
    "  \n",
    "위 식과 같이 최종 결과는 원소별 미분을 계산한 다음 그 결과값을 원소별로 곱하면 얻을 수 있음  \n",
    "야코비 행렬을 구하여 '행렬의 곱'을 계산할 필요 없이 단순히 원소별로 곱하여 구할 수 있으며,  \n",
    "따라서 더 효율적으로 계산(구현)할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e01ece",
   "metadata": {},
   "source": [
    "# 38단계 형상 변환 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312e070",
   "metadata": {},
   "source": [
    "원소별로 계산하지 않는 함수  \n",
    "\\* dezero/functions.py 에 추가\n",
    "  \n",
    "__reshape 함수 구현__  \n",
    "  \n",
    "텐서의 형상을 바꾸는 함수  \n",
    "텐서의 원소 수는 같고 형상만 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef61c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "y = np.reshape(x,(6,))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6734712",
   "metadata": {},
   "source": [
    "<img src='./img/4/reshape.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653bfb0",
   "metadata": {},
   "source": [
    "reshape 함수는 구체적인 계산 없이 단순히 형상만 변환함  \n",
    "따라서 역전파도 기울기에 계산 없이 흘려보내는데, 변수의 데이터와 기울기의 형상이 일치해야 함  \n",
    "ex) x가 Variable 인스턴스 일 때, x.data.shape == x.grad.shape이 만족할 수 있도록 역전파를 구현  \n",
    "즉, 입력 변수 쪽의 형상에 맞게 변환하는 것이 reshape 함수의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446b55c",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Reshape(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.reshape(self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        return reshape(gy, self.x_shape)\n",
    "    \n",
    "from dezero.core import as_variable\n",
    "\n",
    "def reshape(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return Reshape(shape)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c2c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.reshape(x, (6,))\n",
    "y.backward(retain_grad=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0d524",
   "metadata": {},
   "source": [
    "넘파이의 reshape은 다음과 같이 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca60ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1,2,3)\n",
    "\n",
    "y = x.reshape((2,3)) # 튜플로 받기\n",
    "y = x.reshape([2,3]) # 리스트로 받기\n",
    "y = x.reshape(2,3) # 인수를 풀어서 받기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e4f84",
   "metadata": {},
   "source": [
    "DeZero에서도 똑같은 용법을 제공하기 위해 Variable 클래스를 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bd305",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Variable:\n",
    "    ...\n",
    "    \n",
    "    def reshape(self, *shape): # 추가\n",
    "        if len(shape) == 1 and isinstance(shape[0], (tuple, list)):\n",
    "            shape = shape[0]\n",
    "        return dezero.functions.reshape(self, shape)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b221faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(np.random.randn(1,2,3))\n",
    "y = x.reshape((2,3))\n",
    "y = x.reshape(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260b430",
   "metadata": {},
   "source": [
    "__행렬의 전치 함수 구현__  \n",
    "  \n",
    "행렬을 전치하면 행렬의 형상이 다음과 같이 변함  \n",
    "  \n",
    "ex)  \n",
    "$x=\\begin{pmatrix}x_{11} & x_{12} & x_{13}\\\\\n",
    "x_{21} & x_{22} & x_{23}\\end{pmatrix}$  \n",
    "  \n",
    "$x^T=\\begin{pmatrix}x_{11} & x_{21}\\\\\n",
    "x_{12} & x_{22}\\\\\n",
    "x_{13} & x_{23}\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd2b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# 넘파이의 transpose gkatn\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = np.transpose(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519c819",
   "metadata": {},
   "source": [
    "텐서의 원소 자체는 그대로이고 형상만 바뀜  \n",
    "따라서 역전파에서도 출력 쪽에서 전해지는 기울기의 형상만 순전파 때의 반대로 변경함  \n",
    "\\* 지금은 입력 변수가 행렬(2차원 텐서)일 때로 한정하여 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b9e42",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Transpose(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.transpose(x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = transpose(gy)\n",
    "        return gx\n",
    "    \n",
    "def transpose(x):\n",
    "    return Transpose()(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d411d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.transpose(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4bbd0",
   "metadata": {},
   "source": [
    "Variable 인스턴스에서도 transpose 함수를 사용할 수 있도록 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfca83a",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Variable:\n",
    "    ...\n",
    "    \n",
    "    def transpose(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "    \n",
    "    @property # 인스턴스 변수로 사용할 수 있게 해주는 데코레이터\n",
    "    def T(self):\n",
    "        return dezero.functions.transpose(self)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639a84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(np.random.rand(2,3))\n",
    "y = x.transpose()\n",
    "y = x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35230d8f",
   "metadata": {},
   "source": [
    "# [보충] 실제 transpose 함수  \n",
    "  \n",
    "넘파이의 np.transpose 함수는 다음과 같이 축의 데이터 순서를 바꿀 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155d639",
   "metadata": {},
   "source": [
    "<img src='./img/4/transpose.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea45800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4)\n",
      "(2, 1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "A, B, C, D = 1, 2, 3, 4\n",
    "x = np.random.rand(A, B, C, D)\n",
    "print(x.shape)\n",
    "y = x.transpose(1, 0, 3, 2) # 인덱스\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157569ac",
   "metadata": {},
   "source": [
    "위 코드에서 np.transpose에 건네지는 인수는 변환 후의 축 순서(인덱스)임  \n",
    "인수를 None으로 주면 축이 역순으로 정렬됨(None이 기본값)  \n",
    "  \n",
    "Dezero에도 이를 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dc37c",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Transpose(Function):\n",
    "    def __init__(self, axes=None):\n",
    "        self.axes = axes\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.transpose(self.axes)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        if self.axes is None:\n",
    "            return transpose(gy)\n",
    "\n",
    "        axes_len = len(self.axes)\n",
    "        inv_axes = tuple(np.argsort([ax % axes_len for ax in self.axes])) ### ???\n",
    "        return transpose(gy, inv_axes)\n",
    "\n",
    "\n",
    "def transpose(x, axes=None):\n",
    "    return Transpose(axes)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba3ea6",
   "metadata": {},
   "source": [
    "# 39단계 합계 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b4f98",
   "metadata": {},
   "source": [
    "<img src='./img/4/add.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f0f6f",
   "metadata": {},
   "source": [
    "덧셈의 역전파는 출력 쪽에서 전해지는 기울기를 두 개로 '복사'하여 전달함  \n",
    "이는 원소가 2개인 벡터를 사용해도 동일하게 이뤄짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2f6a3",
   "metadata": {},
   "source": [
    "<img src='./img/4/sum_1.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5fffa",
   "metadata": {},
   "source": [
    "위 그림은 2개의 원소로 구성된 벡터이며,  \n",
    "이 벡터에 sum 함수를 적용하면 스칼라를 출력함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59964edc",
   "metadata": {},
   "source": [
    "<img src='./img/4/sum_2.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a123e95",
   "metadata": {},
   "source": [
    "원소가 2개 이상인 벡터에서도 위 그림과 같이  \n",
    "역전파에서 기울기를 입력 변수의 형상과 같아지도록 복사하여 벡터로 확장해 전파함  \n",
    "입력 변수가 2차원 이상의 배열일 때도 동일하게 적용됨  \n",
    "이것이 sum 함수의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da5d99",
   "metadata": {},
   "source": [
    "\\* sum 함수의 역전파에서 Variable 인스턴스를 사용하므로 복사 작업도 DeZero 함수로 해야하며,  \n",
    "넘파이의 브로드캐스트와 같은 기능인 지정한 형상에 맞게 원소를 복사하는 작업은  \n",
    "다음 단계에서 broadcast_to(x.shape)라는 함수로 구현할 예정  \n",
    "sum 함수 구현을 위해 미리 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fc583",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Sum(Function):\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum(x):\n",
    "    return Sum()(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb10c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(21)\n",
      "variable([1 1 1 1 1 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([1,2,3,4,5,6]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800e3383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(21)\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.sum(x)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d063e6",
   "metadata": {},
   "source": [
    "넘파이의 np.sum 함수는 합계를 구할 때 '축'을 지정하여 합계를 구할 방향을 정할 수 있음  \n",
    "기본값인 None일 경우 모든 원소를 다 더한 값 하나를 스칼라로 출력하며,  \n",
    "튜플로 지정하면 해당 튜플에서 지정한 축 모두에 대한 합계를 계산하고,  \n",
    "0과 1일 경우는 아래 그림과 같음\n",
    "  \n",
    "<img src='./img/4/sum_3.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90aa114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "(2, 3) -> (3,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = np.sum(x, axis=0)\n",
    "print(y)\n",
    "print(x.shape, '->', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a7d8f",
   "metadata": {},
   "source": [
    "또한 np.sum 함수는 keepdims라는 출력의 차원 수(축 수)를 동일하게 유지할지 정하는 인수도 받음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b46599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "[[5 7 9]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = np.sum(x, axis=0)\n",
    "print(y)\n",
    "y = np.sum(x, keepdims=True, axis=0)\n",
    "print(y)\n",
    "y = np.sum(x, keepdims=False, axis=0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7f221",
   "metadata": {},
   "source": [
    "위 두가지를 반영하여 Sum 클래스와 sum 함수를 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ca706",
   "metadata": {},
   "source": [
    "~~~python\n",
    "from dezero import utils\n",
    "\n",
    "class Sum(Function):\n",
    "    def __init__(self, axis, keepdims):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = x.sum(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gy = utils.reshape_sum_backward(gy, self.x_shape, self.axis, self.keepdims)\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return Sum(axis, keepdims)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51636b16",
   "metadata": {},
   "source": [
    "utils.reshape_sum_backward 함수는 gy의 형상을 미세하게 조정함  \n",
    "(axis와 keepdims를 지원하게 되면서 기울기의 형상을 변환하는 경우가 생기기 때문에 그에 대응하는 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707609f",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def reshape_sum_backward(gy, x_shape, axis, keepdims):\n",
    "    ndim = len(x_shape)\n",
    "    tupled_axis = axis\n",
    "    if axis is None:\n",
    "        tupled_axis = None\n",
    "    elif not isinstance(axis, tuple):\n",
    "        tupled_axis = (axis,)\n",
    "\n",
    "    if not (ndim == 0 or tupled_axis is None or keepdims):\n",
    "        actual_axis = [a if a >= 0 else a + ndim for a in tupled_axis]\n",
    "        shape = list(gy.shape)\n",
    "        for a in sorted(actual_axis):\n",
    "            shape.insert(a, 1)\n",
    "    else:\n",
    "        shape = gy.shape\n",
    "\n",
    "    gy = gy.reshape(shape)  # reshape\n",
    "    return gy\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907a44b",
   "metadata": {},
   "source": [
    "sum 함수를 Variable 메서드로 사용할 수 있도록 Variable 클래스에 다음 코드 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e18d78",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Variable:\n",
    "    ...\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return dezero.functions.sum(self, axis, keepdims)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893a5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([5 7 9])\n",
      "variable([[1 1 1]\n",
      "          [1 1 1]])\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.sum(x, axis=0)\n",
    "y.backward()\n",
    "print(y)\n",
    "print(x.grad)\n",
    "\n",
    "x = Variable(np.random.randn(2,3,4,5))\n",
    "y = x.sum(keepdims=True) # 차원 수 고정\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d6724",
   "metadata": {},
   "source": [
    "# 40단계 브로드캐스트 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507b271",
   "metadata": {},
   "source": [
    "__넘파이의 np.broadcast_to(x, shape)__  \n",
    "  \n",
    "이 함수는 ndarray 인스턴스인 x의 원소를 복제하여 shape 인수로 지정한 형상이 되도록 해줌  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca13bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.broadcast_to(x, (2,3))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42531467",
   "metadata": {},
   "source": [
    "<img src='./img/4/broadcast_1.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24b48",
   "metadata": {},
   "source": [
    "원소 복사가 일어날 경우 역전파에서는 기울기의 '합'을 구함  \n",
    "따라서 broadcast_to 함수의 역전파는 입력 x의 형상과 같아지도록 기울기의 합을 구함  \n",
    "  \n",
    "이를 위해 dezero/utils.py 에 sum_to 함수를 구현함  \n",
    "sum_to 함수는 shape 형상이 되도록 합을 계산함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d809bf7",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def sum_to(x, shape):\n",
    "    ndim = len(shape)\n",
    "    lead = x.ndim - ndim\n",
    "    lead_axis = tuple(range(lead))\n",
    "\n",
    "    axis = tuple([i + lead for i, sx in enumerate(shape) if sx == 1])\n",
    "    y = x.sum(lead_axis + axis, keepdims=True)\n",
    "    if lead > 0:\n",
    "        y = y.squeeze(lead_axis)\n",
    "    return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889fdb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 7 9]]\n",
      "[[ 6]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero.utils import sum_to\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = sum_to(x, (1, 3))\n",
    "print(y)\n",
    "\n",
    "y = sum_to(x, (2, 1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18c685",
   "metadata": {},
   "source": [
    "<img src='./img/4/sum_to.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4570d7f",
   "metadata": {},
   "source": [
    "sum_to 함수의 역전파는 위 그림과 같이 broadcast_to 함수를 사용하여 입력 x의 형상과 같아지도록 기울기의 원소를 복제함 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4be2c",
   "metadata": {},
   "source": [
    "DeZero의 BroadcastTo 클래스와 broadcast_to 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be788b70",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class BroadcastTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = np.broadcast_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = sum_to(gy, self.x_shape)\n",
    "        reutrn gx\n",
    "        \n",
    "def broadcast_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return BroadcastTo(shape)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda95420",
   "metadata": {},
   "source": [
    "DeZero의 SumTo 클래스와 sum_to 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fc52d",
   "metadata": {},
   "source": [
    "~~~python\n",
    "from dezero import utils\n",
    "\n",
    "class SumTo(Function):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        y = utils.sum_to(x, self.shape)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx = broadcast_to(gy, self.x_shape)\n",
    "        return gx\n",
    "    \n",
    "def sum_to(x, shape):\n",
    "    if x.shape == shape:\n",
    "        return as_variable(x)\n",
    "    return SumTo(shape)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968764a3",
   "metadata": {},
   "source": [
    "브로드캐스트란 형상이 다른 다차원 배열끼리의 연산을 가능하게 하는 넘파이의 기능  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1cefe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 12 13]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1,2,3])\n",
    "x1 = np.array([10])\n",
    "y = x0 + x1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2451bc5",
   "metadata": {},
   "source": [
    "위 예시에서 x0과 x1의 형상이 다르나, x1의 원소가 x0 형상에 맞게 복제되었음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9b0862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n"
     ]
    }
   ],
   "source": [
    "from dezero import Variable\n",
    "\n",
    "x0 = Variable(np.array([1,2,3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8232fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb0852",
   "metadata": {},
   "source": [
    "Dezero에서도 브로드캐스트의 순전파는 잘 진행되었지만,  \n",
    "역전파는 전혀 일어나지 않음  \n",
    "  \n",
    "이를 해결하기 위해 Add 클래스의 경우 다음의 수정을 반영  \n",
    "기울기 gx0은 x0의 형상이 되도록 합을 구하고,  \n",
    "기울기 gx1은 x1의 형상이 되도록 합을 구함  \n",
    "  \n",
    "Mul, Sub, Div 클래스 등 사칙연산 클래스에 모두 같은 수정 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932957c2",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Add(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        self.x0_shape, self.x1_shape = x0.shape, x1.shape\n",
    "        y = x0 + x1\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        gx0, gx1 = gy, gy\n",
    "        if self.x0_shape != self.x1_shape:\n",
    "            gx0 = dezero.functions.sum_to(gx0, self.x0_shape)\n",
    "            gx1 = dezero.functions.sum_to(gx1, self.x1_shape)\n",
    "        return gx0, gx1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e11d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([11 12 13])\n",
      "variable([1 1 1])\n",
      "variable([3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x0 = Variable(np.array([1,2,3]))\n",
    "x1 = Variable(np.array([10]))\n",
    "y = x0 + x1\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x0.grad)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe6ab7",
   "metadata": {},
   "source": [
    "# 41단계 행렬의 곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc76bf7",
   "metadata": {},
   "source": [
    "벡터 $a=(a_1,\\cdots,a_n)$, $b=(b_1,\\cdots,b_n)$이 있을 때, 두 벡터의 내적은 다음과 같음  \n",
    "  \n",
    "$ab=a_1b_1+a_2b_2+\\cdots +a_nb_n$  \n",
    "  \n",
    "위의 식과 같이 벡터의 내적은 두 벡터 사이의 대응 원소의 곱을 모두 합한 값임  \n",
    "  \n",
    "행렬의 곱은 아래 그림과 같이 계산함  \n",
    "  \n",
    "<img src='./img/4/matmul_1.png' width=300>  \n",
    "  \n",
    "왼쪽 행렬의 '가로 방향 벡터'와 오른 쪽 행렬의 '세로 방향 벡터' 사이의 내적을 계산하여  \n",
    "그 결과가 새로운 행렬의 원소가 되는 방식으로 계산함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48551d89",
   "metadata": {},
   "source": [
    "<img src='./img/4/matmul_2.png' width=300>  \n",
    "  \n",
    "행렬의 곱에서는 대응하는 차원(축)의 원소 수가 일치해야 함  \n",
    "행렬 a와 b의 행렬 곱의 결과로 만들어진 행렬 c의 형상은 행렬 a와 같은 수의 행, 행렬 b와 같은 수의 열을 갖게 됨  \n",
    "  \n",
    "벡터의 내적과 행렬의 곱 계산은 넘파에의 np.dot 함수로 모두 처리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42551690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 벡터의 내적\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.dot(a,b)\n",
    "print(c)\n",
    "\n",
    "# 행렬의 곱\n",
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[5,6],[7,8]])\n",
    "c = np.dot(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c29242",
   "metadata": {},
   "source": [
    "ex) $y = xW$ 계산  \n",
    "x, W, y의 형상은 1$\\times$D, D$\\times$H, 1$\\times$H 일 때,\n",
    "  \n",
    "<img src='./img/4/matmul_3.png' width=400>\n",
    "  \n",
    "x의 i번째 원소에 대한 미분은 다음과 같음  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}$  \n",
    "  \n",
    "여기서, $y_j=x_1W_{1j}+x_2W_{2j}+\\cdots+x_iW+{ij}+\\cdots+x_HW_{Hj}$이므로,  \n",
    "$\\frac{\\partial{y_j}}{\\partial{x_i}}=W_{ij}$가 성립하므로 위 식을 다음과 같이 정리할 수 있다.  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x_i}}=\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{x_i}}=\n",
    "\\sum_j\\frac{\\partial{L}}{\\partial{y_j}}W_{ij}$  \n",
    "  \n",
    "따라서 $\\frac{\\partial{L}}{\\partial{x}}$은 벡터 $\\frac{\\partial{L}}{\\partial{y}}$와 $W$의 i행 벡터의 내적으로 구해지는 것을 알 수 있음. 다시 정리하면 다음과 같음  \n",
    "  \n",
    "$\\frac{\\partial{L}}{\\partial{x}}=\\frac{\\partial{L}}{\\partial{y}}W^T$  \n",
    "  \n",
    "위 식의 형상체크는 다음과 같다.  \n",
    "  \n",
    "<img src='./img/4/matmul_4.png' width=300>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65b2e7",
   "metadata": {},
   "source": [
    "이번엔 $y=xW$라는 행렬 곱 계산에서 \n",
    "x, W, y의 형상이 N$\\times$D, D$\\times$H, N$\\times$H 라고 할 때,  \n",
    "역전파의 계산 그래프와 형상을 고려한 행렬 곱의 식은 아래 그림과 같음  \n",
    "  \n",
    "<img src='./img/4/matmul_5.png' width=500>  \n",
    "  \n",
    "<img src='./img/4/matmul_6.png' width=400>  \n",
    "  \n",
    "이를 고려하여 DeZero의 행렬 곱을 수행하는 MatMul 클래스와 matmul 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d42847",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class MatMul(Function):\n",
    "    def forward(self, x, W):\n",
    "        y = x.dot(W)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, W = self.inputs\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW\n",
    "    \n",
    "def matmul(x, W):\n",
    "    return MatMul()(x,W)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00adf451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.random.randn(2,3))\n",
    "W = Variable(np.random.randn(3,4))\n",
    "y = F.matmul(x, W)\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.shape) # x와 형상 동일\n",
    "print(W.grad.shape) # W와 형상 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec043f9",
   "metadata": {},
   "source": [
    "# 42단계 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9fe2f",
   "metadata": {},
   "source": [
    "x로부터 실숫값 y를 예측하는 것을 회귀(regression)이라고 하며,  \n",
    "회귀 모델 중 예측값이 선형(직선)을 이루는 것을 '선형 회귀(linear regression)이라고 함  \n",
    "  \n",
    "머신러닝의 가장 기본이 되는 선형 회귀를 구현하기 위해 실험용 데이터셋 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12831255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토이 데이터셋\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0) # 시드값 고정\n",
    "x = np.random.randn(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100,1) # y에 무작위 노이즈 추가\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d2da0",
   "metadata": {},
   "source": [
    "위 토이 데이터셋의 x와 y는 선형 관계이나 y에 노이즈가 추가되어 있음  \n",
    "  \n",
    "x값이 주어지면 y값을 예측하는 모델(수식)을 만들어야 함  \n",
    "  \n",
    "이 문제에서 x와 y가 선형 관계라고 가정하기 때문에 $y=Wx+b$로 표현할 수 있음  \n",
    "회귀에서는 데이터와 예측치의 차이인 잔차(residual)을 최소화하는 직선 $y=Wx+b$를 찾는 것이 목표이며,  \n",
    "예측치(모델)와 데이터의 오차를 나타내는 지표는 다음 식으로 정의함  \n",
    "  \n",
    "$L=\\frac{1}{N}\\sum^N_{i=1}(f(x_i)-y_i)^2$  \n",
    "  \n",
    "이는 평균 제곱 오차(mean squared error)로 총 N개의 점에 대해($x_i, y_i$)의 각 점에서 제곱 오차를 구한 다음 모두 더하고 $\\frac{1}{N}$을 곱하여 평균을 구한 값임  \n",
    "선형 회귀에서는 손실 함수로 평균 제곱 오차를 이용함  \n",
    "  \n",
    "따라서, 손실 함수의 출력을 최소화하는 W와 b를 찾는 함수 최적화 문제라고 할 수 있음  \n",
    "이를 경사하강법을 이용하여 구현하면 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f80745b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0.47225324]]) variable([1.12285597]) variable(35.639227142413375)\n",
      "variable([[0.83479198]]) variable([2.01549184]) variable(22.352525843294014)\n",
      "variable([[1.11273871]]) variable([2.72526399]) variable(14.03968744103063)\n",
      "variable([[1.32552745]]) variable([3.28975702]) variable(8.835506148369609)\n",
      "variable([[1.48818042]]) variable([3.73880616]) variable(5.575369864173143)\n",
      "variable([[1.61230024]]) variable([4.09609987]) variable(3.531709851601136)\n",
      "variable([[1.70684062]]) variable([4.38045017]) variable(2.2497326621513265)\n",
      "variable([[1.77870475]]) variable([4.60679956]) variable(1.4449848960136873)\n",
      "variable([[1.83320966]]) variable([4.78701945]) variable(0.9394443499151088)\n",
      "variable([[1.87444636]]) variable([4.93054341]) variable(0.6216269868304265)\n",
      "variable([[1.90555891]]) variable([5.04486931]) variable(0.4216716133545497)\n",
      "variable([[1.92896069]]) variable([5.13595788]) variable(0.2957702354273312)\n",
      "variable([[1.94650174]]) variable([5.20854881]) variable(0.21643290401114082)\n",
      "variable([[1.9595982]]) variable([5.26641174]) variable(0.1663969996947845)\n",
      "variable([[1.96933241]]) variable([5.31254542]) variable(0.13481425337581832)\n",
      "variable([[1.97653017]]) variable([5.34933594]) variable(0.11486216172148211)\n",
      "variable([[1.98182037]]) variable([5.37868225]) variable(0.10224669654520369)\n",
      "variable([[1.98568097]]) variable([5.40209602]) variable(0.09426307877493928)\n",
      "variable([[1.98847441]]) variable([5.42078086]) variable(0.08920619981134527)\n",
      "variable([[1.99047481]]) variable([5.43569531]) variable(0.08600025658335106)\n",
      "variable([[1.99188897]]) variable([5.44760295]) variable(0.08396591926215162)\n",
      "variable([[1.99287238]]) variable([5.45711215]) variable(0.08267384727308037)\n",
      "variable([[1.99354154]]) variable([5.46470774]) variable(0.08185245788461916)\n",
      "variable([[1.99398342]]) variable([5.47077621]) variable(0.0813298073518988)\n",
      "variable([[1.99426262]]) variable([5.4756257]) variable(0.08099693769542457)\n",
      "variable([[1.99442689]]) variable([5.47950195]) variable(0.08078474161913704)\n",
      "variable([[1.9945113]]) variable([5.48260099]) variable(0.08064934746462228)\n",
      "variable([[1.99454143]]) variable([5.48507921]) variable(0.08056287849824387)\n",
      "variable([[1.99453578]]) variable([5.48706142]) variable(0.0805076051143417)\n",
      "variable([[1.99450757]]) variable([5.48864726]) variable(0.08047224088352652)\n",
      "variable([[1.99446614]]) variable([5.48991627]) variable(0.0804495943871521)\n",
      "variable([[1.99441797]]) variable([5.49093197]) variable(0.08043507921860017)\n",
      "variable([[1.99436748]]) variable([5.49174511]) variable(0.08042576765978131)\n",
      "variable([[1.99431756]]) variable([5.49239623]) variable(0.08041978910313484)\n",
      "variable([[1.99427002]]) variable([5.49291772]) variable(0.08041594727609158)\n",
      "variable([[1.99422594]]) variable([5.49333548]) variable(0.08041347646173187)\n",
      "variable([[1.99418585]]) variable([5.49367021]) variable(0.08041188609915911)\n",
      "variable([[1.99414993]]) variable([5.49393848]) variable(0.08041086163126797)\n",
      "variable([[1.99411812]]) variable([5.49415352]) variable(0.08041020118300123)\n",
      "variable([[1.99409023]]) variable([5.49432594]) variable(0.08040977508524431)\n",
      "variable([[1.99406596]]) variable([5.4944642]) variable(0.08040949997856489)\n",
      "variable([[1.99404499]]) variable([5.49457511]) variable(0.080409322230174)\n",
      "variable([[1.99402696]]) variable([5.49466408]) variable(0.08040920730533824)\n",
      "variable([[1.99401155]]) variable([5.49473548]) variable(0.0804091329492323)\n",
      "variable([[1.99399842]]) variable([5.49479278]) variable(0.08040908480938995)\n",
      "variable([[1.99398729]]) variable([5.49483877]) variable(0.08040905362274552)\n",
      "variable([[1.99397787]]) variable([5.4948757]) variable(0.08040903340655912)\n",
      "variable([[1.99396993]]) variable([5.49490536]) variable(0.08040902029401203)\n",
      "variable([[1.99396326]]) variable([5.49492918]) variable(0.08040901178414428)\n",
      "variable([[1.99395766]]) variable([5.49494832]) variable(0.08040900625831802)\n",
      "variable([[1.99395298]]) variable([5.49496369]) variable(0.08040900266826266)\n",
      "variable([[1.99394906]]) variable([5.49497605]) variable(0.08040900033466752)\n",
      "variable([[1.9939458]]) variable([5.49498598]) variable(0.08040899881705313)\n",
      "variable([[1.99394308]]) variable([5.49499397]) variable(0.08040899782963704)\n",
      "variable([[1.99394082]]) variable([5.49500039]) variable(0.08040899718690023)\n",
      "variable([[1.99393895]]) variable([5.49500555]) variable(0.08040899676834572)\n",
      "variable([[1.99393739]]) variable([5.4950097]) variable(0.08040899649566864)\n",
      "variable([[1.9939361]]) variable([5.49501305]) variable(0.08040899631795732)\n",
      "variable([[1.99393504]]) variable([5.49501573]) variable(0.08040899620209466)\n",
      "variable([[1.99393416]]) variable([5.4950179]) variable(0.08040899612652863)\n",
      "variable([[1.99393343]]) variable([5.49501964]) variable(0.08040899607722748)\n",
      "variable([[1.99393283]]) variable([5.49502104]) variable(0.08040899604505183)\n",
      "variable([[1.99393234]]) variable([5.49502217]) variable(0.08040899602404633)\n",
      "variable([[1.99393193]]) variable([5.49502308]) variable(0.0804089960103292)\n",
      "variable([[1.9939316]]) variable([5.49502381]) variable(0.08040899600136905)\n",
      "variable([[1.99393132]]) variable([5.4950244]) variable(0.08040899599551464)\n",
      "variable([[1.9939311]]) variable([5.49502487]) variable(0.08040899599168853)\n",
      "variable([[1.99393091]]) variable([5.49502526]) variable(0.08040899598918738)\n",
      "variable([[1.99393076]]) variable([5.49502556]) variable(0.080408995987552)\n",
      "variable([[1.99393064]]) variable([5.49502581]) variable(0.08040899598648245)\n",
      "variable([[1.99393053]]) variable([5.49502601]) variable(0.08040899598578291)\n",
      "variable([[1.99393045]]) variable([5.49502617]) variable(0.08040899598532518)\n",
      "variable([[1.99393038]]) variable([5.4950263]) variable(0.08040899598502564)\n",
      "variable([[1.99393032]]) variable([5.49502641]) variable(0.08040899598482962)\n",
      "variable([[1.99393028]]) variable([5.49502649]) variable(0.0804089959847013)\n",
      "variable([[1.99393024]]) variable([5.49502656]) variable(0.08040899598461732)\n",
      "variable([[1.99393021]]) variable([5.49502662]) variable(0.08040899598456232)\n",
      "variable([[1.99393018]]) variable([5.49502666]) variable(0.08040899598452628)\n",
      "variable([[1.99393016]]) variable([5.4950267]) variable(0.08040899598450268)\n",
      "variable([[1.99393015]]) variable([5.49502673]) variable(0.08040899598448722)\n",
      "variable([[1.99393013]]) variable([5.49502675]) variable(0.08040899598447711)\n",
      "variable([[1.99393012]]) variable([5.49502677]) variable(0.08040899598447043)\n",
      "variable([[1.99393011]]) variable([5.49502678]) variable(0.08040899598446614)\n",
      "variable([[1.99393011]]) variable([5.4950268]) variable(0.08040899598446323)\n",
      "variable([[1.9939301]]) variable([5.49502681]) variable(0.08040899598446143)\n",
      "variable([[1.99393009]]) variable([5.49502681]) variable(0.08040899598446016)\n",
      "variable([[1.99393009]]) variable([5.49502682]) variable(0.08040899598445937)\n",
      "variable([[1.99393009]]) variable([5.49502683]) variable(0.08040899598445886)\n",
      "variable([[1.99393008]]) variable([5.49502683]) variable(0.08040899598445848)\n",
      "variable([[1.99393008]]) variable([5.49502683]) variable(0.08040899598445829)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.08040899598445814)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.08040899598445804)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.08040899598445796)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.08040899598445791)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.0804089959844579)\n",
      "variable([[1.99393008]]) variable([5.49502684]) variable(0.08040899598445787)\n",
      "variable([[1.99393007]]) variable([5.49502684]) variable(0.08040899598445787)\n",
      "variable([[1.99393007]]) variable([5.49502685]) variable(0.08040899598445785)\n",
      "variable([[1.99393007]]) variable([5.49502685]) variable(0.08040899598445787)\n",
      "variable([[1.99393007]]) variable([5.49502685]) variable(0.08040899598445787)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "# 토이 데이터셋\n",
    "np.random.seed(0) \n",
    "x = np.random.randn(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100,1)\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1,1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y\n",
    "\n",
    "def mean_squared_error(x0, x1):\n",
    "    diff = x0 - x1\n",
    "    return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward()\n",
    "    \n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "    print(W, b, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602eb64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9330d0b400>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randn(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100,1)\n",
    "y_n = x * W.data + b.data\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_n,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0320e5",
   "metadata": {},
   "source": [
    "위에서 사용한 predict 함수는 matmul 함수를 사용하여 행렬 곱을 계산하므로 여러 데이터를 모아서 한 번에 계산할 수 있음  \n",
    "\\* x.shape[1]과 W.shape[0]이 일치해야 행렬 곱이 제대로 계산됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec0985",
   "metadata": {},
   "source": [
    "# [보충] DeZero의 mean_squared_error 함수  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113d2ae",
   "metadata": {},
   "source": [
    "<img src='./img/4/mse.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7f0ad",
   "metadata": {},
   "source": [
    "위에서 구현한 평균 제곱 오차를 구하는 함수에서는 위 계산 그래프에서 중간에 나오는 이름 없는 변수 세 개가 계산 그래프가 존재하는 동안 메모리에 계속 살아있음  \n",
    "메모리를 덜 사용할 수 있도록 forward 메서드의 범위를 벗어나는 순간 메모리에 삭제되도록 수정하여 dezero/functions.py에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff848f57",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class MeanSquaredError(Function):\n",
    "    def forward(self, x0, x1):\n",
    "        diff = x0, x1\n",
    "        y = (diff ** 2).sum() / len(diff)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x0, x1 = self.inputs\n",
    "        diff = x0 - x1\n",
    "        gx0 = gy * diff * (2. / len(diff))\n",
    "        gx1 = -gx0\n",
    "        return gx0, gx1\n",
    "    \n",
    "def mean_squared_error(x0, x1):\n",
    "    return MeanSquaredError()(x0, x1)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172228b",
   "metadata": {},
   "source": [
    "# 43단계 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233cbdc",
   "metadata": {},
   "source": [
    "42단계에서 선형 회귀로 수행한 계산은 (손실 함수를 제외하면) 입력 x와 매개변수 W 사이에서 행렬 곱을 구하고, b를 더해줬음  \n",
    "이 변환을 선형 변환(linear transformation) 혹은 아핀 변환(affine transformation)이라고 함  \n",
    "(정확히는 선형 변환은 b는 포함되지 않으나 신경망 분야에서는 b 더하는 계산까지 포함하는 연산을 선형 변환이라고 부르는 것이 일반적)  \n",
    "선형 변환은 신경망에서는 완전연결계층(fully connected layer)에 해당하며,  \n",
    "매개변수 W는 가중치(Weight), 매개변수 b는 편향(bias)이라고 함  \n",
    "  \n",
    "위 계산을 수행하는 linear 함수 구현하는 방법은 두 가지가 있음  \n",
    "  \n",
    "__첫 번째 방법. linear_simple 함수__  \n",
    "  \n",
    "<img src='./img/4/linearregression_1.png' width=500>  \n",
    "  \n",
    "변수 t의 데이터는 역전파 시 필요하지 않음  \n",
    "따라서 $y=t+b$ 계산 후 t.data=None으로 설정하여 t의 데이터를 메모리에서 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffef03f",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def linear_simple(x, W, b=None):\n",
    "    t = matmul(x, W)\n",
    "    if b is None:\n",
    "        return t\n",
    "    \n",
    "    y = t + b\n",
    "    t.data = None # t의 데이터 삭제\n",
    "    return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccaa65",
   "metadata": {},
   "source": [
    "__두 번째 방법. linear_simple 함수__  \n",
    "  \n",
    "<img src='./img/4/linearregression_2.png' width=300>  \n",
    "  \n",
    "Function 클래스를 상속하여 Linear 클래스를 구현  \n",
    "중간 결과가 Variable 인스턴스로 보존되지 않기 때문에 순전파 시 사용하던 중간 데이터는 순전파가 끝나는 즉시 삭제됨   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3c950",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Linear(Function):\n",
    "    def forward(self, x, W, b):\n",
    "        y = x.dot(W)\n",
    "        if b is not None:\n",
    "            y += b\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, W, b = self.inputs\n",
    "        gb = None if b.data is None else sum_to(gy, b.shape)\n",
    "        gx = matmul(gy, W.T)\n",
    "        gW = matmul(x.T, gy)\n",
    "        return gx, gW, gb\n",
    "\n",
    "\n",
    "def linear(x, W, b=None):\n",
    "    return Linear()(x, W, b)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b03852",
   "metadata": {},
   "source": [
    "비선형 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f38dc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fca094f12b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100,1) # 데이터 생성에 sin 함수 이용\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430a25e",
   "metadata": {},
   "source": [
    "비선형 데이터셋은 선형 회귀로는 풀 수 없음  \n",
    "신경망에선 선형 변환의 출력에 비선형 변환을 텐서에 각 원소에 적용  \n",
    "비선형 변환을 활성화 함수(activation function)이라고 하며, 대표적으로 ReLU 함수, 시그모이드 함수 등이 있음  \n",
    "  \n",
    "이번 예시에서 활성화 함수로 시그모이드 함수를 사용!  \n",
    "시그모이드 함수의 식은 다음과 같음  \n",
    "$y = \\frac{1}{1+exp(-x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23a874",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def sigmoid_simple(x):\n",
    "    x = as_variable(x)\n",
    "    y = 1 / (1 + exp(-x))\n",
    "    return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95329f2",
   "metadata": {},
   "source": [
    "Function 클래스를 상속한 Sigmoid 클래스 구현은 다음과 같음  \n",
    "(이 방법으로 하면 클래스를 하나의 단위로 하여 기울기 계산 효율을 높일 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d75cd",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Sigmoid(Function):\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = gy * y * (1 - y)\n",
    "        return gx\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return Sigmoid()(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363fecd",
   "metadata": {},
   "source": [
    "__신경망 구현__  \n",
    "  \n",
    "일반적인 신경망은 '선형 변환$\\rightarrow$활성화 함수$\\rightarrow$선형 변환$\\rightarrow$활성화 함수$\\rightarrow$선형 변환$\\rightarrow$...' 형태로 '선형 변환'과 '활성화 함수'를 순서대로 적용하여 연속적으로 변환을 수행함. 이것이 신경망 추론(predict) 코드  \n",
    "  \n",
    "신경망 학습에서는 추론을 처리한 후 손실 함수를 추가하고,  \n",
    "손실 함수의 출력을 최소화하는 매개변수를 찾음  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98048fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8473695850105871)\n",
      "variable(0.2514286285183606)\n",
      "variable(0.2475948546674987)\n",
      "variable(0.23786120447054812)\n",
      "variable(0.21222231333102923)\n",
      "variable(0.1674218111783417)\n",
      "variable(0.09681932619992654)\n",
      "variable(0.07849528290602327)\n",
      "variable(0.07749729552991153)\n",
      "variable(0.07722132399559316)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 신경망 학습\n",
    "\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "# 데이터셋\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# 1. 가중치 초기화\n",
    "I, H, O = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I, H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H, O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "# 2. 신경망 추론\n",
    "def predict(x):\n",
    "    y = F.linear(x, W1, b1)\n",
    "    y = F.sigmoid(y)\n",
    "    y = F.linear(y, W2, b2)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "# 3. 신경망 학습\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    W1.cleargrad()\n",
    "    b1.cleargrad()\n",
    "    W2.cleargrad()\n",
    "    b2.cleargrad()\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data -= lr * W1.grad.data\n",
    "    b1.data -= lr * b1.grad.data\n",
    "    W2.data -= lr * W2.grad.data\n",
    "    b2.data -= lr * b2.grad.data\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)\n",
    "        \n",
    "# Plot\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "t = np.arange(0, 1, .01)[:, np.newaxis]\n",
    "y_pred = predict(t)\n",
    "plt.plot(t, y_pred.data, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedb270",
   "metadata": {},
   "source": [
    "# 44단계 매개변수를 모아두는 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfade55d",
   "metadata": {},
   "source": [
    "매개변수란 경사하강법 등의 최적화 기법에 의해 갱신되는 변수로, '가중치', '편향'이 해당됨  \n",
    "  \n",
    "매개변수를 담는 구조를 만들어 매개변수 관리를 자동화하기 위해 Parameter와 Layer라는 클래스를 구현  \n",
    "  \n",
    "__Pamameter 클래스 구현__  \n",
    "  \n",
    "~~~python\n",
    "class Parameter(Variable):\n",
    "    pass\n",
    "~~~\n",
    "  \n",
    "Parameter 클래스를 dezero/core.py에 추가하고  \n",
    "dezero/\\_\\_init__.py에 from dezero import Parameter 추가  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166c9f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, Parameter\n",
    "\n",
    "x = Variable(np.array(1.0))\n",
    "p = Parameter(np.array(2.0))\n",
    "y = x * p\n",
    "\n",
    "print(isinstance(p, Parameter))\n",
    "print(isinstance(x, Parameter))\n",
    "print(isinstance(y, Parameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5744f",
   "metadata": {},
   "source": [
    "__Layer 클래스 구현__  \n",
    "  \n",
    "Function 클래스와 마찬가지로 변수를 변환하는 클래스이나,  \n",
    "매개변수를 유지하고 매개변수를 사용하여 변환을 함  \n",
    "Layer 클래스를 기반 클래스로 두고 구체적인 변환은 자식 클래스에서 구현  \n",
    "dezero/layers.py 에 저장\n",
    "  \n",
    "~~~python\n",
    "from dezero.core import Parameter\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "        \n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Parameter):\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "~~~  \n",
    "  \n",
    "\\_params 인스턴스 변수는 매개변수를 보관함, 타입이 집합(set)으로 원소들에 순서가 없고, ID가 같은 객체를 중복 저장할 수 없음  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f633de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p1', 'p2'}\n",
      "-----------------\n",
      "p1 variable(1)\n",
      "p2 variable(2)\n"
     ]
    }
   ],
   "source": [
    "from dezero.layers import Layer\n",
    "from dezero import Variable\n",
    "from dezero import Parameter\n",
    "import numpy as np\n",
    "\n",
    "layer = Layer()\n",
    "\n",
    "layer.p1 = Parameter(np.array(1))\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Variable(np.array(3))\n",
    "layer.p4 = 'test'\n",
    "\n",
    "print(layer._params)\n",
    "print('-----------------')\n",
    "\n",
    "for name in layer._params:\n",
    "    print(name, layer.__dict__[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6be92f",
   "metadata": {},
   "source": [
    "Parameter 인스턴스를 보유하고 있는 인스턴스 변수 이름만 layer.\\_params에 추가되며, 인스턴스 변수 \\_\\_dict__에는 모든 모든 인스턴스 변수가 딕셔너리 타입으로 저장됨  \n",
    "  \n",
    "Layer 클래스에 다음 4개의 메서드 추가  \n",
    "  \n",
    "~~~python\n",
    "import weakref\n",
    "\n",
    "class Layer:\n",
    "    ...\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        outputs = self.forward(*inputs)\n",
    "        if not isinstance(outputs, tuple):\n",
    "            outputs = (outputs,)\n",
    "        self.inputs = [weakref.ref(x) for x in inputs]\n",
    "        self.outputs = [weakref.ref(y) for y in outputs]\n",
    "        return outputs if len(outputs) > 1 else outputs[0]\n",
    "    \n",
    "    def forward(self, inputs): # 자식 클래스에서 구현\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            yield self.__dict__[name]\n",
    "            \n",
    "    def cleargrads(self): # 기울기 재설정\n",
    "        for param in self.params():\n",
    "            param.cleargrad()\n",
    "~~~  \n",
    "  \n",
    "< 참고 >  \n",
    "  \n",
    "return은 처리를 종료하고 값을 반환하는 반면 yield는 처리를 일시 중지하고 값을 반환하므로 작업을 재개할 수 있음  \n",
    "따라서, yield를 for문과 함께 사용하여 매개변수를 순차적으로 꺼낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c489bc",
   "metadata": {},
   "source": [
    "__Linear 클래스 구현__  \n",
    "  \n",
    "~~~python\n",
    "import numpy as np\n",
    "import dezero functions as F\n",
    "from dezero.core import Parameter\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        I, O = in_size, out_size\n",
    "        W_data = np.random.randn(I, O).astype(dtype) * np.sqrt(1 / I)\n",
    "        self.W = Parameter(W_data, name ='W')\n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(0, dtype=dtype), name='b')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c008d42",
   "metadata": {},
   "source": [
    "가중치를 초기화 메서드가 아닌 forward 메서드에서 생성함으로써 Linear 클래스의 입력 크기를 사용자가 지정하지 않고 자동으로 결정되도록 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b0887",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "from dezero.core import Parameter\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, out_size, nobias=False, dtype=np.float32, in_size=None):\n",
    "        super().__init__()        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.W = Parameter(None, name='W')\n",
    "        if self.in_size is not None: # in_size가 지정되어 있지 않다면 나중으로 연기\n",
    "            self._init_W()\n",
    "            \n",
    "        if nobias:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = Parameter(np.zeros(out_size, dtype=dtype), name='b')\n",
    "            \n",
    "    def _init_W(self):\n",
    "        I, O = self.in_size, self.out_size\n",
    "        W_data = np.random.randn(I, O).astype(self.dtype) * np.sqrt(1 / I)\n",
    "        self.W.data = W_data\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 데이터를 흘려보내는 시점에 가중치 초기화\n",
    "        if self.W.data is None:\n",
    "            self.in_size = x.shape[1]\n",
    "            self._init_W()\n",
    "            \n",
    "        y = F.linear(x, self.W, self.b)\n",
    "        return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f8cf3",
   "metadata": {},
   "source": [
    "Layer를 이용한 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740bf849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350168)\n",
      "variable(0.1231190572064935)\n",
      "variable(0.07888166506355149)\n",
      "variable(0.07655073683421632)\n",
      "variable(0.07637803086238223)\n",
      "variable(0.0761876413118557)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L # L로 임포트\n",
    "\n",
    "# 데이터셋\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "l1 = L.Linear(10) # 출력 크기 지정\n",
    "l2 = L.Linear(1)\n",
    "\n",
    "def predict(x):\n",
    "    y = l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = l2(y)\n",
    "    return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    \n",
    "    l1.cleargrads()\n",
    "    l2.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    for l in [l1, l2]:\n",
    "        for p in l.params():\n",
    "            p.data -= lr * p.grad.data\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e674a",
   "metadata": {},
   "source": [
    "다음 단계에서는 여러 Layer를 개별적으로 다루지 않고 하나의 클래스로 묶어서 관리하도록 개선!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eef09",
   "metadata": {},
   "source": [
    "# 45단계 계층을 모아두는 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74488ab",
   "metadata": {},
   "source": [
    "__Layer 클래스에 다른 Layer도 담을 수 있도록 확장__  \n",
    "- Layer 인스턴스의 이름도 \\_params에 추가\n",
    "- params 메서드, \\_params에서 name을 꺼내 그 name에 해당하는 객체를 obj로 꺼내며,  \n",
    "    obj가 Layer라면 Layer에서 매개변수를 재귀적으로 꺼냄\n",
    "  \n",
    "< 참고 >  \n",
    "  \n",
    "yield를 사용한 함수를 제너레이터(generator)라고 하며, 제너레이터를 사요하여 또 다른 제너레이터를 만들 때는 yield from을 사용함  \n",
    "  \n",
    "~~~python\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self._params = set()\n",
    "        \n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, (Parameter, Layer)): # 1. Layer도 추가\n",
    "            self._params.add(name)\n",
    "        super().__setattr__(name, value)\n",
    "        \n",
    "    def params(self):\n",
    "        for name in self._params:\n",
    "            obj = self.__dict__[name]\n",
    "            \n",
    "            if isinstance(obj, Layer): # 2. Layer에서 매개변수 꺼내기\n",
    "                yield from obj.params()\n",
    "            else:\n",
    "                yield obj\n",
    "~~~  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564ce6a",
   "metadata": {},
   "source": [
    "__새로운 Layer 클래스를 사용하여 신경망 구현__  \n",
    "  \n",
    "model = Layer()에서 인스턴스를 생성하여 model의 인스턴스 변수로 Linear 인스턴스를 추가  \n",
    "따라서 model.params()로 model 내에 존재하는 모든 매개변수에 접근하거나  \n",
    "model.cleargrads()로 모든 매개변수의 기울기를 재설정하는 식으로  \n",
    "신경망에서 사용하는 매개변수를 한꺼번에 관리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350b7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([0. 0. 0. 0. 0.])\n",
      "variable(None)\n",
      "variable([0. 0. 0.])\n",
      "variable(None)\n"
     ]
    }
   ],
   "source": [
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "from dezero import Layer\n",
    "\n",
    "model = Layer()\n",
    "model.l1 = L.Linear(5) # 출력 크기만 지정\n",
    "model.l2 = L.Linear(3)\n",
    "\n",
    "# 추론을 수행하는 함수\n",
    "def predict(model, x):\n",
    "    y = model.l1(x)\n",
    "    y = F.sigmoid(y)\n",
    "    y = model.l2(y)\n",
    "    return y\n",
    "\n",
    "# 모든 매개변수에 접근\n",
    "for p in model.params():\n",
    "    print(p)\n",
    "    \n",
    "# 모든 매개변수의 기울기를 재설정\n",
    "model.cleargrads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb6f48",
   "metadata": {},
   "source": [
    "__Layer 클래스를 상속하여 모델 전체를 하나의 '클래스'로 정의하는 더 편리한 방식으로 구현__  \n",
    "  \n",
    "모델을 클래스 단위로 정의한 객체지향식 모델 정의 방법은 체이너가 최초로 제안했고, 그 후 파이토치와 텐서플로 등 많은 다른 프레임워크에서 보편적으로 사용하는 방식으로 굳어졌음!\n",
    "  \n",
    "~~~python\n",
    "class TwoLayerNet(Layer):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbd83d",
   "metadata": {},
   "source": [
    "__Model 클래스 구현__  \n",
    "  \n",
    "모델 : 복잡한 패턴이나 규칙이 숨어있는 현상을 수식을 사용하여 단순하게 표현한 것  \n",
    "  \n",
    "~~~python\n",
    "from dezero import Layer\n",
    "from dezero import utils\n",
    "\n",
    "class Model(Layer):\n",
    "    def plot(self, *inputs, to_file='model.png'):\n",
    "             y = self.forward(*inputs)\n",
    "             return utils.plot_dot_graph(y, verbose=True, to_file=to_file)\n",
    "~~~\n",
    "\n",
    "dezero/models.py 추가 및 dezero/\\_\\_init__.py에 from dezero.models import Model 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1635daae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, Model\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    \n",
    "x = Variable(np.random.randn(5, 10), name='x')\n",
    "model = TwoLayerNet(100, 10)\n",
    "model.plot(x) # 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8616417",
   "metadata": {},
   "source": [
    "__Model 클래스를 이용하여 sin 함수 데이터셋 회귀문제 풀기__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c841e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350168)\n",
      "variable(0.1231190572064935)\n",
      "variable(0.07888166506355149)\n",
      "variable(0.07655073683421632)\n",
      "variable(0.07637803086238223)\n",
      "variable(0.0761876413118557)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable, Model\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "\n",
    "# 데이터셋 생성\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "# 모델 정의\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "    \n",
    "model = TwoLayerNet(hidden_size, 1)\n",
    "\n",
    "# 학습 시작\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in model.params():\n",
    "        p.data -= lr * p.grad.data\n",
    "    if i % 1000 == 0 :\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82c392",
   "metadata": {},
   "source": [
    "MLP(Multi-Layer Perceptron, '다층 퍼셉트론') 클래스 구현  \n",
    "  \n",
    "앞에서는 2층 신경망을 구현했으나,  \n",
    "이번에는 더 범용적인 다층 완전연결계층 신경망 구현  \n",
    "\\* MLP는 완전연결계층 신경망의 별칭으로 흔히 쓰임  \n",
    "  \n",
    "dezero/models.py 에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783777a2",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "class MLP(Model):\n",
    "    def __init__(self, fc_output_sizes, activation=F.sigmoid): # fc_output_sizes의 fc는 fully connection(완전연결)의 약자\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        \n",
    "        for i, out_size in enumerate(fc_output_sizes):\n",
    "            layer = L.Linear(out_size)\n",
    "            setattr(self, 'l' + str(i), layer)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "        return self.layers[-1](x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35b9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "\n",
    "\n",
    "model = MLP((10, 1)) # 2층\n",
    "model = MLP((10, 20, 30, 40, 1)) # 5층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df396d",
   "metadata": {},
   "source": [
    "# 46단계 Optimizer로 수행하는 매개변수 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61dff02",
   "metadata": {},
   "source": [
    "__Optimizer 클래스 구현__  \n",
    "  \n",
    "매개변수 갱신 작업을 모듈화하고 쉽게 다른 모듈로 대체할 수 있는 구조를 만들기 위해  \n",
    "매개변수 갱신을 위한 기반 클래스인 Optimizer 클래스 구현  \n",
    "구체적인 최적화 기법은 Optimizer 클래스를 상속한 자식 클래스에서 구현  \n",
    "  \n",
    "- setup 메서드 : 매개변수를 갖는 클래스(Model 또는 Layer)를 인스턴스 변수인 target으로 설정  \n",
    "  \n",
    "- update_one 메서드 : 구체적인 매개변수 갱신 수행, 이 메서드는 Optimizer의 자식 클래스에서 재정의해야 함  \n",
    "  \n",
    "- add_hook 메서드 : 가중치 감소, 기울기 클리핑 등의 전처리를 수행할 수 있게 하는 함수  \n",
    "  \n",
    "dezero/optimizers.py 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e34fa",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "        \n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # None 이외의 매개변수를 리스트에 모아둠\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        \n",
    "        # 전처리(옵션)\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "            \n",
    "        # 매개변수 갱신\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "            \n",
    "    def update_one(self, param):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35633ab",
   "metadata": {},
   "source": [
    "__SGD(Stochastic Gradient Descent, 확률적경사하강법) 클래스 구현__  \n",
    "  \n",
    "경사하강법으로 매개변수 갱신하는 클래스 구현  \n",
    "  \n",
    "dezero/optimizers.py에 추가  \n",
    "  \n",
    "~~~python\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12f72b",
   "metadata": {},
   "source": [
    "SGD 클래스를 사용한 sin 함수 데이터셋 회귀문제 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e26517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.24990280802148895)\n",
      "variable(0.24609876581126014)\n",
      "variable(0.2372159081431807)\n",
      "variable(0.20793216413350168)\n",
      "variable(0.1231190572064935)\n",
      "variable(0.07888166506355149)\n",
      "variable(0.07655073683421632)\n",
      "variable(0.07637803086238223)\n",
      "variable(0.0761876413118557)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "max_iter = 10000\n",
    "hidden_size = 10\n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "# 또는 다음처럼 한 줄로 합칠 수 있음\n",
    "# optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "    \n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.update() # 매개변수 갱신\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a1af7",
   "metadata": {},
   "source": [
    "__최적화 기법 Momentum 클래스 구현__  \n",
    "  \n",
    "Momentum 수식  \n",
    "  \n",
    "$v \\leftarrow \\alpha v-\\eta\\frac{\\partial{L}}{\\partial{W}}$  \n",
    "  \n",
    "$W \\leftarrow W + v$  \n",
    "  \n",
    "- W : 갱신할 가중치 매개변수\n",
    "- $\\frac{\\partial{L}}{\\partial{W}}$ : W에 관한 손실함수 L의 기울기\n",
    "- $\\eta$ : 학습률\n",
    "- v : 속도  \n",
    "  \n",
    "위 식은 물체가 기울기 방향으로 힘을 받아 가속된다는 물리 법칙을 나타내며,  \n",
    "이 식에 의해 속도만큼 위치(매개변수)가 이동함  \n",
    "$\\alpha v$항은 물체가 아무런 힘을 받지 않을 때 서서히 감속시키는 역할을 하며, $\\alpha$의 값은 0.9 등으로 설정함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651cda9",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import numpy as np\n",
    "\n",
    "class MomentumSGD(Optimizers):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vs = {} # 속도\n",
    "        \n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "            \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr *param.grad.data\n",
    "        param.data += v\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec94ee",
   "metadata": {},
   "source": [
    "__AdaGrad 클래스 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacb2df",
   "metadata": {},
   "source": [
    "(밑바닥부터 시작하는 딥러닝 6장 참고)  \n",
    "신경망 학습에서는 학습률($\\eta$) 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않음  \n",
    "  \n",
    "학습률 감소(learning rate decay) : 학습을 진행하면서 학습률을 점차 줄여가는 방법,  \n",
    "처음에는 크게 학습하다가 조금씩 작게 학습  \n",
    "  \n",
    "AdaGrad는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행함  \n",
    "  \n",
    "$h \\leftarrow h+\\frac{\\partial{L}}{\\partial{W}}\t\\odot \\frac{\\partial{L}}{\\partial{W}}$  \n",
    "\n",
    "$W \\leftarrow W-\\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial{L}}{\\partial{W}}$  \n",
    "  \n",
    "- W : 갱신할 가중치 매개변수  \n",
    "- $\\frac{\\partial{L}}{\\partial{W}}$ : W에 대한 손실 함수의 기울기\n",
    "- $\\eta$ : 학습률\n",
    "- h : 기존 기울기 값을 제곱하여 계속 더해줌($\\odot$기호는 행렬의 원소별 곱셈을 의미),  \n",
    "    그리고 매개변수를 갱신할 때 $\\frac{1}{\\sqrt{h}}$을 곱해 학습률을 조정함  \n",
    "    매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻  \n",
    "    다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용됨을 뜻함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d763a02",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, lr=0.001, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.hs = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        h_key = id(param)\n",
    "        if h_key not in self.hs:\n",
    "            self.hs[h_key] = np.zeros_like(param.data)\n",
    "            \n",
    "        lr = self.lr\n",
    "        eps = self.eps\n",
    "        grad = param.grad.data\n",
    "        h = self.hs[h_key]\n",
    "\n",
    "        h += grad * grad\n",
    "        param.data -= lr * grad / (np.sqrt(h) + eps)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d76256",
   "metadata": {},
   "source": [
    "__AdaDelta 클래스 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d63a7",
   "metadata": {},
   "source": [
    "AdaGrad의 gradient vanishing 문제를 보완하기 위해 제안된 옵티마이저로,  \n",
    "스텝 크기를 학습률이 아닌 스텝 크기의 변화 값의 제곱하여 지수 평균값을 계산함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c03e49",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class AdaDelta(Optimizer):\n",
    "    def __init__(self, rho=0.95, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.msg = {}\n",
    "        self.msdx = {}\n",
    "\n",
    "    def update_one(self, param):\n",
    "        key = id(param)\n",
    "        if key not in self.msg:\n",
    "            self.msg[key] = np.zeros_like(param.data)\n",
    "            self.msdx[key] = np.zeros_like(param.data)\n",
    "\n",
    "        msg, msdx = self.msg[key], self.msdx[key]\n",
    "        rho = self.rho\n",
    "        eps = self.eps\n",
    "        grad = param.grad.data\n",
    "\n",
    "        msg *= rho\n",
    "        msg += (1 - rho) * grad * grad\n",
    "        dx = np.sqrt((msdx + eps) / (msg + eps)) * grad\n",
    "        msdx *= rho\n",
    "        msdx += (1 - rho) * dx * dx\n",
    "        param.data -= dx\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bf54a",
   "metadata": {},
   "source": [
    "__Adam 클래스 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b4d28",
   "metadata": {},
   "source": [
    "(밑바닥부터 시작하는 딥러닝 6장 참고)  \n",
    "모멘텀은 공이 그릇 바닥을 구르는 듯한 움직임을 보임\n",
    "AdaGrad는 매개변수의 원소마다 적응적으로 갱신 정도를 조정했음\n",
    "이 두 기법을 융합한 듯한 방법이 Adam\n",
    "또, 하이퍼파라미터의 '편향 보정'이 진행된다는 특징도 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683285a",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.t = 0\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.ms = {}\n",
    "        self.vs = {}\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        self.t += 1\n",
    "        super().update(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        fix1 = 1. - math.pow(self.beta1, self.t)\n",
    "        fix2 = 1. - math.pow(self.beta2, self.t)\n",
    "        return self.alpha * math.sqrt(fix2) / fix1\n",
    "\n",
    "    def update_one(self, param):\n",
    "\n",
    "        key = id(param)\n",
    "        if key not in self.ms:\n",
    "            self.ms[key] = np.zeros_like(param.data)\n",
    "            self.vs[key] = np.zeros_like(param.data)\n",
    "\n",
    "        m, v = self.ms[key], self.vs[key]\n",
    "        beta1, beta2, eps = self.beta1, self.beta2, self.eps\n",
    "        grad = param.grad.data\n",
    "\n",
    "        m += (1 - beta1) * (grad - m)\n",
    "        v += (1 - beta2) * (grad * grad - v)\n",
    "        param.data -= self.lr * m / (np.sqrt(v) + eps)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffebf2",
   "metadata": {},
   "source": [
    "# 47단계 소프트맥스 함수와 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0f7c5",
   "metadata": {},
   "source": [
    "지금까지 다룬 회귀 문제가 아닌 다중 클래스 분류(multi-class classification) 문제를 다루기 위한 사전 준비  \n",
    "  \n",
    "__get_item 함수__  \n",
    "  \n",
    "구현은 부록 B 참고(dezero/functions.py 에 추가함)  \n",
    "  \n",
    "get_item 함수는 Variable의 다차원 배열 중 일부를 슬라이스하여 뽑아줌  \n",
    "역전파는 원래의 다차원 배열에서 데이터가 추출된 위치에 해당 기울기를 설정하고, 그 외에는 0으로 설정함  \n",
    "  \n",
    "\\* 다차원 배열의 일부를 추출하는 작업을 '슬라이스'라고 함\n",
    "  \n",
    "<img src='./img/4/getitem.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36788958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac46203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[0. 0. 0.]\n",
      "          [1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward() # 역전파\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9b298",
   "metadata": {},
   "source": [
    "인덱스를 반복 지정하여 동일한 원소를 여러 번 빼낼 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7479b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[1 2 3]\n",
      "          [1 2 3]\n",
      "          [4 5 6]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "indices = np.array([0,0,1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d0aae",
   "metadata": {},
   "source": [
    "get_item 함수를 Variable의 메서드로도 사용할 수 있도록 특수 메서드로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e95574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([4 5 6])\n",
      "variable([3 6])\n"
     ]
    }
   ],
   "source": [
    "Variable.__getitem__ = F.get_item # Variabledml aptjemfh tjfwjd\n",
    "\n",
    "y = x[1]\n",
    "print(y)\n",
    "\n",
    "y = x[:,2]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9946e",
   "metadata": {},
   "source": [
    "__소프트맥스 함수__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aeb9df",
   "metadata": {},
   "source": [
    "이미 구현한 신경망을 그대로 다중 클래스 분류에서도 사용할 수 있음  \n",
    "  \n",
    "ex) 입력 데이터 차원 수 2, 3개의 클래스로 분류하는 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b990390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero.models import MLP\n",
    "import numpy as np\n",
    "\n",
    "model = MLP((10,3)) # 2층으로 이루어진 완전연결계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1dfede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.26106594 -0.59399635  0.34780657]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -.04]])\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740d520",
   "metadata": {},
   "source": [
    "출력된 3차원 벡터의 원소 각각이 하나의 클래스에 해당하며,  \n",
    "출력된 벡터에서 값이 가장 큰 원소의 인덱스가 이 모델이 분류한(예측한) 클래스가 됨  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0280c",
   "metadata": {},
   "source": [
    "여러 개의 입력 데이터를 한번에 처리할 수 있음 \n",
    "  \n",
    "ex) 입력 데이터 4개 한번에 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc26d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.22418196 -0.6658777   0.38897529]\n",
      "          [-0.33190307 -0.44997993  0.27934484]\n",
      "          [-0.01702671 -0.66961947  0.5498054 ]\n",
      "          [-0.62322019 -0.09944109  0.23457456]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.2, -0.4],[0.3, 0.5],[1.3,-3.2],[2.1,0.3]])\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146421eb",
   "metadata": {},
   "source": [
    "y의 형상은 (4,3)이 되며, i번째 입력 데이터는 x[i]가 되고, 그에 대응하는 출력은 y[i]가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc679ab",
   "metadata": {},
   "source": [
    "신경망의 출력은 단순히 수치이지만,  \n",
    "소프트맥스 함수를 통해 확률로 변환할 수 있음  \n",
    "소프트맥스 함수의 수식은 다음과 같음  \n",
    "  \n",
    "$p_k = \\frac{exp(y_k)}{\\sum_{i=1}^n exp(y_1}$  \n",
    "  \n",
    "여기서, $y_k$는 입력, n은 클래스 수이며, k번째 출력 $p_k$를 구함  \n",
    "  \n",
    "소프트맥스 함수는 $0\\leq p_i\\leq 1$이고, $p_1+p_2+\\cdots+p_n=1$이 성립하여  \n",
    "$(p_1, p_2, \\cdots, p_n)$의 원소 각각을 확률로 해석할 수 있음  \n",
    "  \n",
    "샘플 데이터가 하나인 경우에 한정한 소프트맥스 함수의 구현은 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23da49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp(x)\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af491f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([[-0.22418196 -0.6658777   0.38897529]])\n",
      "variable([[0.28659893 0.18426742 0.52913365]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a4727",
   "metadata": {},
   "source": [
    "소프트맥스 함수의 계산은 지수 함수로 이뤄져 결과값이 너무 커지거나 작아지기 쉬우므로 오버플로 문제에 잘 대처해야 함.  \n",
    "밑바닥부터 시작하는 딥러닝 3장 소프트맥스 함수 구현시 주의점 참고!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318b04b",
   "metadata": {},
   "source": [
    "__배치 데이터도 처리할 수 있는 소프트맥스 함수 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025db49",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def softmax_simple(x, axis=1):\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2234f",
   "metadata": {},
   "source": [
    "__Function 클래스를 상속하여 Softmax 클래스로 구현하고 파이썬 함수 softmax 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03efe18",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Softmax(Function):\n",
    "    def __init__(self, axis=1):\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x - x.max(axis=self.axis, keepdims=True) # 오버플로 대책\n",
    "        y = np.exp(y)\n",
    "        y /= y.sum(axis=self.axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        y = self.outputs[0]()\n",
    "        gx = y * gy\n",
    "        sumdx = gx.sum(axis=self.axis, keepdims=True)\n",
    "        gx -= y * sumdx\n",
    "        return gx\n",
    "\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    return Softmax(axis)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c79fc",
   "metadata": {},
   "source": [
    "__교차 엔트로피 오차__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4f4b4",
   "metadata": {},
   "source": [
    "선형 회귀에서는 손실 함수로 평균 제곱 오차를 이용하나,  \n",
    "다중 클래스 분류에서는 교차 엔트로피 오차(cross entropy error)를 사용함  \n",
    "  \n",
    "$L = -\\sum_{k}t_klogp_k$  \n",
    "  \n",
    "- $t_k$ : k 차원째 정답 데이터의 값. 원핫 벡터(one hot vector)로 표현되어 있음(해당되면 1, 아니면 0)  \n",
    "  \n",
    "- $p_k$ : 신경망에서 소프트맥스 함수를 적용한 후의 출력  \n",
    "  \n",
    "이는 정답 클래스에 해당하는 번호가 t로 주어지면 해당하는 번호의 확률 p를 추출함으로써 다음과 같이 계산할 수 있음  \n",
    "  \n",
    "$L = -logp[t]$  \n",
    "  \n",
    "- p[t] : 벡터 p에서 t번째 요소만을 추출한다는 뜻  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19258cf6",
   "metadata": {},
   "source": [
    "교차 엔트로피 오차의 구현은 다음과 같음  \n",
    "  \n",
    "- x : 신경망에서 소프트맥스 함수를 적용하기 전의 출력\n",
    "- t : 정답 레이블(원핫 벡터가 아닌 정답 클래스 번호(레이블))\n",
    "\n",
    "log_p[np.arange(N), t.data]코드는 log_p[0, t.data[0]], log_p[1, t.data[1]], ...와 정답 데이터(t.data)에 대응하는 모델의 출력을 구하고, 그 값을 1차원 배열에 담아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542534b",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0) # log(0)을 방지하기 위해 p의 값을 1e-15 이상으로 설정\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N), t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "class Max(Function):\n",
    "    def __init__(self, axis=None, keepdims=False):\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.max(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.inputs[0]\n",
    "        y = self.outputs[0]()  # weakref\n",
    "\n",
    "        shape = utils.max_backward_shape(x, self.axis)\n",
    "        gy = reshape(gy, shape)\n",
    "        y = reshape(y, shape)\n",
    "        cond = (x.data == y.data)\n",
    "        gy = broadcast_to(gy, cond.shape)\n",
    "        return gy * cond\n",
    "\n",
    "\n",
    "class Min(Max):\n",
    "    def forward(self, x):\n",
    "        y = x.min(axis=self.axis, keepdims=self.keepdims)\n",
    "        return y\n",
    "\n",
    "\n",
    "def max(x, axis=None, keepdims=False):\n",
    "    return Max(axis, keepdims)(x)\n",
    "\n",
    "\n",
    "def min(x, axis=None, keepdims=False):\n",
    "    return Min(axis, keepdims)(x)\n",
    "\n",
    "\n",
    "class Clip(Function):\n",
    "    def __init__(self, x_min, x_max):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = np.clip(x, self.x_min, self.x_max)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = (x.data >= self.x_min) * (x.data <= self.x_max)\n",
    "        gx = gy * mask\n",
    "        return gx\n",
    "\n",
    "\n",
    "def clip(x, x_min, x_max):\n",
    "    return Clip(x_min, x_max)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195436b6",
   "metadata": {},
   "source": [
    "clip 함수는 x의 원소가 x_min 이하이면 x_min으로 변환, x_max 이상이면 x_max로 변환해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72614802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(1.059585216699765)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "model = MLP((10,3)) \n",
    "x = np.array([[0.2, -0.4],[0.3, 0.5],[1.3,-3.2],[2.1,0.3]])\n",
    "t = np.array([2,0,1,0]) # 정답 레이블\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy_simple(y, t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03a94e",
   "metadata": {},
   "source": [
    "__Function 클래스를 상속하여 SoftmaxCrossEntropy 클래스로 구현__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5657f",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class SoftmaxCrossEntropy(Function):\n",
    "    def forward(self, x, t):\n",
    "        N = x.shape[0]\n",
    "        log_z = utils.logsumexp(x, axis=1)\n",
    "        log_p = x - log_z\n",
    "        log_p = log_p[np.arange(N), t.ravel()]\n",
    "        y = -log_p.sum() / np.float32(N)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, t = self.inputs\n",
    "        N, CLS_NUM = x.shape\n",
    "\n",
    "        gy *= 1/N\n",
    "        y = softmax(x)\n",
    "        # convert to one-hot\n",
    "        t_onehot = np.eye(CLS_NUM, dtype=t.dtype)[t.data]\n",
    "        y = (y - t_onehot) * gy\n",
    "        return y\n",
    "\n",
    "\n",
    "def softmax_cross_entropy(x, t):\n",
    "    return SoftmaxCrossEntropy()(x, t)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16c6f1",
   "metadata": {},
   "source": [
    "# 48단계 다중 클래스 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902e933",
   "metadata": {},
   "source": [
    "__스파이럴 데이터셋__  \n",
    "  \n",
    "dezero/datasets.py에 들어있는 스파이럴 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2b7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb72633",
   "metadata": {},
   "source": [
    "3 클래스 분류 문제이며, 스파이럴 데이터셋을 그래프로 표현하면 아래 그림과 같이 소용돌이 모양으로 분포함      \n",
    "\n",
    "<img src='./img/4/spiral_dataset.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e8490",
   "metadata": {},
   "source": [
    "__클래스 분류 학습 코드__  \n",
    "  \n",
    "데이터가 많을 때는 모든 데이터를 한꺼번에 처리하는 대신 조금씩 무작위로 모아서 처리함  \n",
    "이 때의 '데이터 뭉치'를 미니배치라고 함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcd8d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 2, loss 1.05\n",
      "epoch 3, loss 0.95\n",
      "epoch 4, loss 0.92\n",
      "epoch 5, loss 0.87\n",
      "epoch 6, loss 0.89\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.78\n",
      "epoch 9, loss 0.80\n",
      "epoch 10, loss 0.79\n",
      "epoch 11, loss 0.78\n",
      "epoch 12, loss 0.76\n",
      "epoch 13, loss 0.77\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.77\n",
      "epoch 17, loss 0.78\n",
      "epoch 18, loss 0.74\n",
      "epoch 19, loss 0.74\n",
      "epoch 20, loss 0.72\n",
      "epoch 21, loss 0.73\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.77\n",
      "epoch 24, loss 0.73\n",
      "epoch 25, loss 0.74\n",
      "epoch 26, loss 0.74\n",
      "epoch 27, loss 0.72\n",
      "epoch 28, loss 0.72\n",
      "epoch 29, loss 0.72\n",
      "epoch 30, loss 0.73\n",
      "epoch 31, loss 0.71\n",
      "epoch 32, loss 0.72\n",
      "epoch 33, loss 0.72\n",
      "epoch 34, loss 0.71\n",
      "epoch 35, loss 0.72\n",
      "epoch 36, loss 0.71\n",
      "epoch 37, loss 0.71\n",
      "epoch 38, loss 0.70\n",
      "epoch 39, loss 0.71\n",
      "epoch 40, loss 0.70\n",
      "epoch 41, loss 0.71\n",
      "epoch 42, loss 0.70\n",
      "epoch 43, loss 0.70\n",
      "epoch 44, loss 0.70\n",
      "epoch 45, loss 0.69\n",
      "epoch 46, loss 0.69\n",
      "epoch 47, loss 0.71\n",
      "epoch 48, loss 0.70\n",
      "epoch 49, loss 0.69\n",
      "epoch 50, loss 0.69\n",
      "epoch 51, loss 0.68\n",
      "epoch 52, loss 0.67\n",
      "epoch 53, loss 0.68\n",
      "epoch 54, loss 0.70\n",
      "epoch 55, loss 0.68\n",
      "epoch 56, loss 0.66\n",
      "epoch 57, loss 0.67\n",
      "epoch 58, loss 0.66\n",
      "epoch 59, loss 0.64\n",
      "epoch 60, loss 0.64\n",
      "epoch 61, loss 0.64\n",
      "epoch 62, loss 0.63\n",
      "epoch 63, loss 0.63\n",
      "epoch 64, loss 0.61\n",
      "epoch 65, loss 0.61\n",
      "epoch 66, loss 0.60\n",
      "epoch 67, loss 0.62\n",
      "epoch 68, loss 0.59\n",
      "epoch 69, loss 0.60\n",
      "epoch 70, loss 0.57\n",
      "epoch 71, loss 0.58\n",
      "epoch 72, loss 0.57\n",
      "epoch 73, loss 0.56\n",
      "epoch 74, loss 0.56\n",
      "epoch 75, loss 0.55\n",
      "epoch 76, loss 0.55\n",
      "epoch 77, loss 0.55\n",
      "epoch 78, loss 0.54\n",
      "epoch 79, loss 0.53\n",
      "epoch 80, loss 0.53\n",
      "epoch 81, loss 0.52\n",
      "epoch 82, loss 0.53\n",
      "epoch 83, loss 0.52\n",
      "epoch 84, loss 0.49\n",
      "epoch 85, loss 0.50\n",
      "epoch 86, loss 0.49\n",
      "epoch 87, loss 0.49\n",
      "epoch 88, loss 0.48\n",
      "epoch 89, loss 0.47\n",
      "epoch 90, loss 0.47\n",
      "epoch 91, loss 0.46\n",
      "epoch 92, loss 0.46\n",
      "epoch 93, loss 0.45\n",
      "epoch 94, loss 0.44\n",
      "epoch 95, loss 0.45\n",
      "epoch 96, loss 0.44\n",
      "epoch 97, loss 0.43\n",
      "epoch 98, loss 0.43\n",
      "epoch 99, loss 0.42\n",
      "epoch 100, loss 0.43\n",
      "epoch 101, loss 0.42\n",
      "epoch 102, loss 0.41\n",
      "epoch 103, loss 0.42\n",
      "epoch 104, loss 0.41\n",
      "epoch 105, loss 0.40\n",
      "epoch 106, loss 0.40\n",
      "epoch 107, loss 0.40\n",
      "epoch 108, loss 0.39\n",
      "epoch 109, loss 0.38\n",
      "epoch 110, loss 0.39\n",
      "epoch 111, loss 0.38\n",
      "epoch 112, loss 0.38\n",
      "epoch 113, loss 0.38\n",
      "epoch 114, loss 0.36\n",
      "epoch 115, loss 0.36\n",
      "epoch 116, loss 0.36\n",
      "epoch 117, loss 0.36\n",
      "epoch 118, loss 0.36\n",
      "epoch 119, loss 0.35\n",
      "epoch 120, loss 0.35\n",
      "epoch 121, loss 0.36\n",
      "epoch 122, loss 0.34\n",
      "epoch 123, loss 0.35\n",
      "epoch 124, loss 0.33\n",
      "epoch 125, loss 0.33\n",
      "epoch 126, loss 0.32\n",
      "epoch 127, loss 0.34\n",
      "epoch 128, loss 0.32\n",
      "epoch 129, loss 0.33\n",
      "epoch 130, loss 0.31\n",
      "epoch 131, loss 0.30\n",
      "epoch 132, loss 0.31\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.28\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.27\n",
      "epoch 145, loss 0.27\n",
      "epoch 146, loss 0.26\n",
      "epoch 147, loss 0.26\n",
      "epoch 148, loss 0.26\n",
      "epoch 149, loss 0.26\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.25\n",
      "epoch 153, loss 0.24\n",
      "epoch 154, loss 0.24\n",
      "epoch 155, loss 0.24\n",
      "epoch 156, loss 0.24\n",
      "epoch 157, loss 0.24\n",
      "epoch 158, loss 0.24\n",
      "epoch 159, loss 0.23\n",
      "epoch 160, loss 0.23\n",
      "epoch 161, loss 0.23\n",
      "epoch 162, loss 0.23\n",
      "epoch 163, loss 0.23\n",
      "epoch 164, loss 0.22\n",
      "epoch 165, loss 0.22\n",
      "epoch 166, loss 0.22\n",
      "epoch 167, loss 0.21\n",
      "epoch 168, loss 0.22\n",
      "epoch 169, loss 0.22\n",
      "epoch 170, loss 0.21\n",
      "epoch 171, loss 0.21\n",
      "epoch 172, loss 0.22\n",
      "epoch 173, loss 0.22\n",
      "epoch 174, loss 0.21\n",
      "epoch 175, loss 0.21\n",
      "epoch 176, loss 0.20\n",
      "epoch 177, loss 0.21\n",
      "epoch 178, loss 0.20\n",
      "epoch 179, loss 0.20\n",
      "epoch 180, loss 0.20\n",
      "epoch 181, loss 0.20\n",
      "epoch 182, loss 0.19\n",
      "epoch 183, loss 0.20\n",
      "epoch 184, loss 0.19\n",
      "epoch 185, loss 0.19\n",
      "epoch 186, loss 0.19\n",
      "epoch 187, loss 0.19\n",
      "epoch 188, loss 0.19\n",
      "epoch 189, loss 0.19\n",
      "epoch 190, loss 0.19\n",
      "epoch 191, loss 0.19\n",
      "epoch 192, loss 0.19\n",
      "epoch 193, loss 0.18\n",
      "epoch 194, loss 0.19\n",
      "epoch 195, loss 0.18\n",
      "epoch 196, loss 0.18\n",
      "epoch 197, loss 0.18\n",
      "epoch 198, loss 0.18\n",
      "epoch 199, loss 0.19\n",
      "epoch 200, loss 0.18\n",
      "epoch 201, loss 0.17\n",
      "epoch 202, loss 0.18\n",
      "epoch 203, loss 0.18\n",
      "epoch 204, loss 0.17\n",
      "epoch 205, loss 0.18\n",
      "epoch 206, loss 0.17\n",
      "epoch 207, loss 0.17\n",
      "epoch 208, loss 0.17\n",
      "epoch 209, loss 0.17\n",
      "epoch 210, loss 0.17\n",
      "epoch 211, loss 0.17\n",
      "epoch 212, loss 0.17\n",
      "epoch 213, loss 0.18\n",
      "epoch 214, loss 0.17\n",
      "epoch 215, loss 0.17\n",
      "epoch 216, loss 0.17\n",
      "epoch 217, loss 0.17\n",
      "epoch 218, loss 0.17\n",
      "epoch 219, loss 0.16\n",
      "epoch 220, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 222, loss 0.16\n",
      "epoch 223, loss 0.16\n",
      "epoch 224, loss 0.16\n",
      "epoch 225, loss 0.16\n",
      "epoch 226, loss 0.16\n",
      "epoch 227, loss 0.17\n",
      "epoch 228, loss 0.18\n",
      "epoch 229, loss 0.16\n",
      "epoch 230, loss 0.16\n",
      "epoch 231, loss 0.15\n",
      "epoch 232, loss 0.16\n",
      "epoch 233, loss 0.17\n",
      "epoch 234, loss 0.16\n",
      "epoch 235, loss 0.16\n",
      "epoch 236, loss 0.15\n",
      "epoch 237, loss 0.16\n",
      "epoch 238, loss 0.16\n",
      "epoch 239, loss 0.16\n",
      "epoch 240, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 242, loss 0.15\n",
      "epoch 243, loss 0.15\n",
      "epoch 244, loss 0.15\n",
      "epoch 245, loss 0.15\n",
      "epoch 246, loss 0.15\n",
      "epoch 247, loss 0.15\n",
      "epoch 248, loss 0.15\n",
      "epoch 249, loss 0.15\n",
      "epoch 250, loss 0.15\n",
      "epoch 251, loss 0.15\n",
      "epoch 252, loss 0.15\n",
      "epoch 253, loss 0.15\n",
      "epoch 254, loss 0.15\n",
      "epoch 255, loss 0.15\n",
      "epoch 256, loss 0.15\n",
      "epoch 257, loss 0.14\n",
      "epoch 258, loss 0.15\n",
      "epoch 259, loss 0.14\n",
      "epoch 260, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 262, loss 0.15\n",
      "epoch 263, loss 0.14\n",
      "epoch 264, loss 0.14\n",
      "epoch 265, loss 0.14\n",
      "epoch 266, loss 0.14\n",
      "epoch 267, loss 0.14\n",
      "epoch 268, loss 0.14\n",
      "epoch 269, loss 0.14\n",
      "epoch 270, loss 0.14\n",
      "epoch 271, loss 0.14\n",
      "epoch 272, loss 0.14\n",
      "epoch 273, loss 0.14\n",
      "epoch 274, loss 0.14\n",
      "epoch 275, loss 0.14\n",
      "epoch 276, loss 0.14\n",
      "epoch 277, loss 0.14\n",
      "epoch 278, loss 0.14\n",
      "epoch 279, loss 0.14\n",
      "epoch 280, loss 0.13\n",
      "epoch 281, loss 0.13\n",
      "epoch 282, loss 0.14\n",
      "epoch 283, loss 0.13\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.13\n",
      "epoch 286, loss 0.13\n",
      "epoch 287, loss 0.14\n",
      "epoch 288, loss 0.13\n",
      "epoch 289, loss 0.13\n",
      "epoch 290, loss 0.13\n",
      "epoch 291, loss 0.13\n",
      "epoch 292, loss 0.13\n",
      "epoch 293, loss 0.14\n",
      "epoch 294, loss 0.13\n",
      "epoch 295, loss 0.13\n",
      "epoch 296, loss 0.13\n",
      "epoch 297, loss 0.13\n",
      "epoch 298, loss 0.12\n",
      "epoch 299, loss 0.13\n",
      "epoch 300, loss 0.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "# 2. 데이터 읽기 / 모델, 옵티마이저 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size) # 소수점 반올림\n",
    "avg_loss_li = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # 3. 데이터셋의 인덱스 뒤섞기\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # 4. 미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "        \n",
    "        # 5. 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "        \n",
    "    # 6. 에포크마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    avg_loss_li.append(avg_loss)\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n",
    "    \n",
    "plt.plot(np.arange(max_epoch), avg_loss_li) # 에폭별 loss 시각화ㅁ\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c798f",
   "metadata": {},
   "source": [
    "# 49단계 Dataset 클래스와 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f0d6c",
   "metadata": {},
   "source": [
    "데이터셋의 크기가 커졌을 때 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올리게 됨  \n",
    "이러한 문제에 대응할 수 있도록 데이터셋 전용 클래스인 Dataset 클래스 구현  \n",
    "또한, Dataset 클래스를 이용함으로써 통일된 인터페이스로 데이터셋을 다룰 수 있음\n",
    "  \n",
    "__Dataset 클래스 구현__  \n",
    "  \n",
    "Dataset 클래스는 기반 클래스로서의 역할을 하고, 실제 사용하는 데이터셋은 이를 상속하여 구현  \n",
    "prepare 메서드는 자식클레스에서 데이터 준비 작업을 구현  \n",
    "\n",
    "  \n",
    "~~~python\n",
    "import numpy as np\n",
    "class Dataset:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index) # index는 정수(스칼라)만 지원\n",
    "        if self.label is None:\n",
    "            return self.data[index], None\n",
    "        else:\n",
    "            return self.data[index], self.label[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441cf6f",
   "metadata": {},
   "source": [
    "__스파이럴 데이터셋 구현__  \n",
    "  \n",
    "~~~python \n",
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data, self.label = get_spiral(self.train)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70b092ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354e54e",
   "metadata": {},
   "source": [
    "__데이터 이어 붙이기__  \n",
    "  \n",
    "신경망을 학습시킬 때는 데이터셋 중 일부를 미니배치로 꺼내 사용함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a54b471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dezero.datasets.Spiral()\n",
    "\n",
    "batch_index = [0,1,2] # 0에서 2번째까지의 데이터를 꺼냄\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "\n",
    "# batch = [(data_0, label_0), (data_1, label_1), (data_2, label_2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672d730",
   "metadata": {},
   "source": [
    "이 데이터를 DeZero의 신경망에 입력하려면 하나의 ndarray 인스턴스로 변환해야 함  \n",
    "아래 코드와 같이 하나의 ndarray 인스턴스로 이어붙일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce3f2279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d0ca9",
   "metadata": {},
   "source": [
    "__Spiral 클래스를 사용한 학습__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "438725bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.13\n",
      "epoch 2, loss 1.05\n",
      "epoch 3, loss 0.95\n",
      "epoch 4, loss 0.92\n",
      "epoch 5, loss 0.87\n",
      "epoch 6, loss 0.89\n",
      "epoch 7, loss 0.84\n",
      "epoch 8, loss 0.78\n",
      "epoch 9, loss 0.80\n",
      "epoch 10, loss 0.79\n",
      "epoch 11, loss 0.78\n",
      "epoch 12, loss 0.76\n",
      "epoch 13, loss 0.77\n",
      "epoch 14, loss 0.76\n",
      "epoch 15, loss 0.76\n",
      "epoch 16, loss 0.77\n",
      "epoch 17, loss 0.78\n",
      "epoch 18, loss 0.74\n",
      "epoch 19, loss 0.74\n",
      "epoch 20, loss 0.72\n",
      "epoch 21, loss 0.73\n",
      "epoch 22, loss 0.74\n",
      "epoch 23, loss 0.77\n",
      "epoch 24, loss 0.73\n",
      "epoch 25, loss 0.74\n",
      "epoch 26, loss 0.74\n",
      "epoch 27, loss 0.72\n",
      "epoch 28, loss 0.72\n",
      "epoch 29, loss 0.72\n",
      "epoch 30, loss 0.73\n",
      "epoch 31, loss 0.71\n",
      "epoch 32, loss 0.72\n",
      "epoch 33, loss 0.72\n",
      "epoch 34, loss 0.71\n",
      "epoch 35, loss 0.72\n",
      "epoch 36, loss 0.71\n",
      "epoch 37, loss 0.71\n",
      "epoch 38, loss 0.70\n",
      "epoch 39, loss 0.71\n",
      "epoch 40, loss 0.70\n",
      "epoch 41, loss 0.71\n",
      "epoch 42, loss 0.70\n",
      "epoch 43, loss 0.70\n",
      "epoch 44, loss 0.70\n",
      "epoch 45, loss 0.69\n",
      "epoch 46, loss 0.69\n",
      "epoch 47, loss 0.71\n",
      "epoch 48, loss 0.70\n",
      "epoch 49, loss 0.69\n",
      "epoch 50, loss 0.69\n",
      "epoch 51, loss 0.68\n",
      "epoch 52, loss 0.67\n",
      "epoch 53, loss 0.68\n",
      "epoch 54, loss 0.70\n",
      "epoch 55, loss 0.68\n",
      "epoch 56, loss 0.66\n",
      "epoch 57, loss 0.67\n",
      "epoch 58, loss 0.66\n",
      "epoch 59, loss 0.64\n",
      "epoch 60, loss 0.64\n",
      "epoch 61, loss 0.64\n",
      "epoch 62, loss 0.63\n",
      "epoch 63, loss 0.63\n",
      "epoch 64, loss 0.61\n",
      "epoch 65, loss 0.61\n",
      "epoch 66, loss 0.60\n",
      "epoch 67, loss 0.62\n",
      "epoch 68, loss 0.59\n",
      "epoch 69, loss 0.60\n",
      "epoch 70, loss 0.57\n",
      "epoch 71, loss 0.58\n",
      "epoch 72, loss 0.57\n",
      "epoch 73, loss 0.56\n",
      "epoch 74, loss 0.56\n",
      "epoch 75, loss 0.55\n",
      "epoch 76, loss 0.55\n",
      "epoch 77, loss 0.55\n",
      "epoch 78, loss 0.54\n",
      "epoch 79, loss 0.53\n",
      "epoch 80, loss 0.53\n",
      "epoch 81, loss 0.52\n",
      "epoch 82, loss 0.53\n",
      "epoch 83, loss 0.52\n",
      "epoch 84, loss 0.49\n",
      "epoch 85, loss 0.50\n",
      "epoch 86, loss 0.49\n",
      "epoch 87, loss 0.49\n",
      "epoch 88, loss 0.48\n",
      "epoch 89, loss 0.47\n",
      "epoch 90, loss 0.47\n",
      "epoch 91, loss 0.46\n",
      "epoch 92, loss 0.46\n",
      "epoch 93, loss 0.45\n",
      "epoch 94, loss 0.44\n",
      "epoch 95, loss 0.45\n",
      "epoch 96, loss 0.44\n",
      "epoch 97, loss 0.43\n",
      "epoch 98, loss 0.43\n",
      "epoch 99, loss 0.42\n",
      "epoch 100, loss 0.43\n",
      "epoch 101, loss 0.42\n",
      "epoch 102, loss 0.41\n",
      "epoch 103, loss 0.42\n",
      "epoch 104, loss 0.41\n",
      "epoch 105, loss 0.40\n",
      "epoch 106, loss 0.40\n",
      "epoch 107, loss 0.40\n",
      "epoch 108, loss 0.39\n",
      "epoch 109, loss 0.38\n",
      "epoch 110, loss 0.39\n",
      "epoch 111, loss 0.38\n",
      "epoch 112, loss 0.38\n",
      "epoch 113, loss 0.38\n",
      "epoch 114, loss 0.36\n",
      "epoch 115, loss 0.36\n",
      "epoch 116, loss 0.36\n",
      "epoch 117, loss 0.36\n",
      "epoch 118, loss 0.36\n",
      "epoch 119, loss 0.35\n",
      "epoch 120, loss 0.35\n",
      "epoch 121, loss 0.36\n",
      "epoch 122, loss 0.34\n",
      "epoch 123, loss 0.35\n",
      "epoch 124, loss 0.33\n",
      "epoch 125, loss 0.33\n",
      "epoch 126, loss 0.32\n",
      "epoch 127, loss 0.34\n",
      "epoch 128, loss 0.32\n",
      "epoch 129, loss 0.33\n",
      "epoch 130, loss 0.31\n",
      "epoch 131, loss 0.30\n",
      "epoch 132, loss 0.31\n",
      "epoch 133, loss 0.31\n",
      "epoch 134, loss 0.30\n",
      "epoch 135, loss 0.29\n",
      "epoch 136, loss 0.29\n",
      "epoch 137, loss 0.29\n",
      "epoch 138, loss 0.28\n",
      "epoch 139, loss 0.29\n",
      "epoch 140, loss 0.28\n",
      "epoch 141, loss 0.27\n",
      "epoch 142, loss 0.27\n",
      "epoch 143, loss 0.28\n",
      "epoch 144, loss 0.27\n",
      "epoch 145, loss 0.27\n",
      "epoch 146, loss 0.26\n",
      "epoch 147, loss 0.26\n",
      "epoch 148, loss 0.26\n",
      "epoch 149, loss 0.26\n",
      "epoch 150, loss 0.25\n",
      "epoch 151, loss 0.25\n",
      "epoch 152, loss 0.25\n",
      "epoch 153, loss 0.24\n",
      "epoch 154, loss 0.24\n",
      "epoch 155, loss 0.24\n",
      "epoch 156, loss 0.24\n",
      "epoch 157, loss 0.24\n",
      "epoch 158, loss 0.24\n",
      "epoch 159, loss 0.23\n",
      "epoch 160, loss 0.23\n",
      "epoch 161, loss 0.23\n",
      "epoch 162, loss 0.23\n",
      "epoch 163, loss 0.23\n",
      "epoch 164, loss 0.22\n",
      "epoch 165, loss 0.22\n",
      "epoch 166, loss 0.22\n",
      "epoch 167, loss 0.21\n",
      "epoch 168, loss 0.22\n",
      "epoch 169, loss 0.22\n",
      "epoch 170, loss 0.21\n",
      "epoch 171, loss 0.21\n",
      "epoch 172, loss 0.22\n",
      "epoch 173, loss 0.22\n",
      "epoch 174, loss 0.21\n",
      "epoch 175, loss 0.21\n",
      "epoch 176, loss 0.20\n",
      "epoch 177, loss 0.21\n",
      "epoch 178, loss 0.20\n",
      "epoch 179, loss 0.20\n",
      "epoch 180, loss 0.20\n",
      "epoch 181, loss 0.20\n",
      "epoch 182, loss 0.19\n",
      "epoch 183, loss 0.20\n",
      "epoch 184, loss 0.19\n",
      "epoch 185, loss 0.19\n",
      "epoch 186, loss 0.19\n",
      "epoch 187, loss 0.19\n",
      "epoch 188, loss 0.19\n",
      "epoch 189, loss 0.19\n",
      "epoch 190, loss 0.19\n",
      "epoch 191, loss 0.19\n",
      "epoch 192, loss 0.19\n",
      "epoch 193, loss 0.18\n",
      "epoch 194, loss 0.19\n",
      "epoch 195, loss 0.18\n",
      "epoch 196, loss 0.18\n",
      "epoch 197, loss 0.18\n",
      "epoch 198, loss 0.18\n",
      "epoch 199, loss 0.19\n",
      "epoch 200, loss 0.18\n",
      "epoch 201, loss 0.17\n",
      "epoch 202, loss 0.18\n",
      "epoch 203, loss 0.18\n",
      "epoch 204, loss 0.17\n",
      "epoch 205, loss 0.18\n",
      "epoch 206, loss 0.17\n",
      "epoch 207, loss 0.17\n",
      "epoch 208, loss 0.17\n",
      "epoch 209, loss 0.17\n",
      "epoch 210, loss 0.17\n",
      "epoch 211, loss 0.17\n",
      "epoch 212, loss 0.17\n",
      "epoch 213, loss 0.18\n",
      "epoch 214, loss 0.17\n",
      "epoch 215, loss 0.17\n",
      "epoch 216, loss 0.17\n",
      "epoch 217, loss 0.17\n",
      "epoch 218, loss 0.17\n",
      "epoch 219, loss 0.16\n",
      "epoch 220, loss 0.17\n",
      "epoch 221, loss 0.16\n",
      "epoch 222, loss 0.16\n",
      "epoch 223, loss 0.16\n",
      "epoch 224, loss 0.16\n",
      "epoch 225, loss 0.16\n",
      "epoch 226, loss 0.16\n",
      "epoch 227, loss 0.17\n",
      "epoch 228, loss 0.18\n",
      "epoch 229, loss 0.16\n",
      "epoch 230, loss 0.16\n",
      "epoch 231, loss 0.15\n",
      "epoch 232, loss 0.16\n",
      "epoch 233, loss 0.17\n",
      "epoch 234, loss 0.16\n",
      "epoch 235, loss 0.16\n",
      "epoch 236, loss 0.15\n",
      "epoch 237, loss 0.16\n",
      "epoch 238, loss 0.16\n",
      "epoch 239, loss 0.16\n",
      "epoch 240, loss 0.16\n",
      "epoch 241, loss 0.15\n",
      "epoch 242, loss 0.15\n",
      "epoch 243, loss 0.15\n",
      "epoch 244, loss 0.15\n",
      "epoch 245, loss 0.15\n",
      "epoch 246, loss 0.15\n",
      "epoch 247, loss 0.15\n",
      "epoch 248, loss 0.15\n",
      "epoch 249, loss 0.15\n",
      "epoch 250, loss 0.15\n",
      "epoch 251, loss 0.15\n",
      "epoch 252, loss 0.15\n",
      "epoch 253, loss 0.15\n",
      "epoch 254, loss 0.15\n",
      "epoch 255, loss 0.15\n",
      "epoch 256, loss 0.15\n",
      "epoch 257, loss 0.14\n",
      "epoch 258, loss 0.15\n",
      "epoch 259, loss 0.14\n",
      "epoch 260, loss 0.15\n",
      "epoch 261, loss 0.15\n",
      "epoch 262, loss 0.15\n",
      "epoch 263, loss 0.14\n",
      "epoch 264, loss 0.14\n",
      "epoch 265, loss 0.14\n",
      "epoch 266, loss 0.14\n",
      "epoch 267, loss 0.14\n",
      "epoch 268, loss 0.14\n",
      "epoch 269, loss 0.14\n",
      "epoch 270, loss 0.14\n",
      "epoch 271, loss 0.14\n",
      "epoch 272, loss 0.14\n",
      "epoch 273, loss 0.14\n",
      "epoch 274, loss 0.14\n",
      "epoch 275, loss 0.14\n",
      "epoch 276, loss 0.14\n",
      "epoch 277, loss 0.14\n",
      "epoch 278, loss 0.14\n",
      "epoch 279, loss 0.14\n",
      "epoch 280, loss 0.13\n",
      "epoch 281, loss 0.13\n",
      "epoch 282, loss 0.14\n",
      "epoch 283, loss 0.13\n",
      "epoch 284, loss 0.13\n",
      "epoch 285, loss 0.13\n",
      "epoch 286, loss 0.13\n",
      "epoch 287, loss 0.14\n",
      "epoch 288, loss 0.13\n",
      "epoch 289, loss 0.13\n",
      "epoch 290, loss 0.13\n",
      "epoch 291, loss 0.13\n",
      "epoch 292, loss 0.13\n",
      "epoch 293, loss 0.14\n",
      "epoch 294, loss 0.13\n",
      "epoch 295, loss 0.13\n",
      "epoch 296, loss 0.13\n",
      "epoch 297, loss 0.13\n",
      "epoch 298, loss 0.12\n",
      "epoch 299, loss 0.13\n",
      "epoch 300, loss 0.13\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral()\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 꺼내기(수정)\n",
    "        batch_index = index[i * batch_size : (i+1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "        \n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "        \n",
    "    # 에포크마다 손실 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch+1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01884db",
   "metadata": {},
   "source": [
    "__데이터 전처리(및 데이터 확장)에 대응하기 위해 Dataset 클래스에 전처리 기능 추가__  \n",
    "  \n",
    "transform과 target_transform이라는 새로운 인수를 받음  \n",
    "이 인수들은 호출 가능한 객체(ex. 파이썬 함수 등)를 받음  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d237fb",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform # 추가\n",
    "        self.target_transform = target_transform # 추가\n",
    "        if self.transform is None: # 추가\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None: # 추가\n",
    "            self.target_transform = lambda x: x\n",
    "            \n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index) \n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None # 수정\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\ # 수정\n",
    "                   self.target_transform(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c80e8",
   "metadata": {},
   "source": [
    "전처리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6691db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d819d7",
   "metadata": {},
   "source": [
    "__자주 사용되는 전처리 함수__  \n",
    "  \n",
    "dezero/transforms.py 에 저장되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48addaa",
   "metadata": {},
   "source": [
    "__정규화__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbab4c",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Normalize:\n",
    "    def __init__(self, mean=0, std=1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, array):\n",
    "        mean, std = self.mean, self.std\n",
    "\n",
    "        if not np.isscalar(mean):\n",
    "            mshape = [1] * array.ndim\n",
    "            mshape[0] = len(array) if len(self.mean) == 1 else len(self.mean)\n",
    "            mean = np.array(self.mean, dtype=array.dtype).reshape(*mshape)\n",
    "        if not np.isscalar(std):\n",
    "            rshape = [1] * array.ndim\n",
    "            rshape[0] = len(array) if len(self.std) == 1 else len(self.std)\n",
    "            std = np.array(self.std, dtype=array.dtype).reshape(*rshape)\n",
    "        return (array - mean) / std\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28e704bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0, std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce330e",
   "metadata": {},
   "source": [
    "아래 Compose 클래스를 이용하여 여러 변환 처리를 연달아 수행하는 것도 할 수 있음  \n",
    "transforms.Compose 클래스는 주어진 변환 목록을 앞에서부터 순서대로 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297292a",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class Compose:\n",
    "    def __init__(self, transforms=[]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if not self.transforms:\n",
    "            return img\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e895b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0),\n",
    "                       transforms.AsType(np.float64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837066c",
   "metadata": {},
   "source": [
    "# 50단계 미니배치를 뽑아주는 DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b85c1",
   "metadata": {},
   "source": [
    "반복자(iterator) : 원소를 반복하여 꺼내줌  \n",
    "파이썬의 반복자는 리스트나 튜플 등 여러 원소를 담고 있는 데이터 타입으로부터 데이터를 순차적으로 추출함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e111cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [1,2,3]\n",
    "x = iter(t) # 리스트를 반복자로 변환\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fe6b822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa46cb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf736036",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gs/xm5833ws6f7361xlj18d2j140000gn/T/ipykernel_1598/3485793935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(x) # 더 꺼낼 원소가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddfa1d0",
   "metadata": {},
   "source": [
    "반복자 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b380650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator:\n",
    "    def __init__(self, max_cnt):\n",
    "        self.max_cnt = max_cnt\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.cnt == self.max_cnt:\n",
    "            raise StopIteration()\n",
    "            \n",
    "        self.cnt += 1\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74d20ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "obj = MyIterator(5)\n",
    "for x in obj:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738523d4",
   "metadata": {},
   "source": [
    "반복자 구조를 이용하여 __미니배치를 뽑아주는 DataLoader 클래스 구현__  \n",
    "  \n",
    "dezero.dataloaders.py 추가 및 dezero.\\_\\_init__.py에 from dezero.dataloaders import DataLoader 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bac77",
   "metadata": {},
   "source": [
    "~~~python\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.iteration = 0 # 반복 횟수 초기화\n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset)) # 데이터 뒤섞기\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "            \n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        batch_index = self.index[i * batch_size:(i+1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        x = np.array([example[0] for example in batch])\n",
    "        t = np.array([example[1] for example in batch])\n",
    "        \n",
    "        self.iteration += 1\n",
    "        return x, t\n",
    "    \n",
    "    def next(self):\n",
    "        return self.__next__()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739c3a7",
   "metadata": {},
   "source": [
    "DataLoader 클래스를 사용하여 미니배치 꺼내오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909ce86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10,)\n",
      "(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train = True)\n",
    "test_set = Spiral(train = False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape) # x, t는 훈련 데이터\n",
    "        break\n",
    "        \n",
    "    # 에포크 끝에서 테스트 데이터를 꺼냄\n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape) # x, t는 테스트 데이터\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4e4db",
   "metadata": {},
   "source": [
    "__accuracy 함수 구현__  \n",
    "  \n",
    "\\* accuracy 함수는 ndarray 인스턴스를 사용해 수행하므로 미분은 할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b150ff",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "    \n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)\n",
    "    acc = result.mean()\n",
    "    return Variable(as_array(acc))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bb0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "\n",
    "y = np.array([[0.2, 0.8, 0], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])\n",
    "t = np.array([1, 2, 0])\n",
    "acc = F.accuracy(y, t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a37d95",
   "metadata": {},
   "source": [
    "__스파이럴 데이터셋 학습__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9551b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.0944, accuracy: 0.4033\n",
      "test loss: 1.0468, accuracy: 0.3267\n",
      "epoch: 2\n",
      "train loss: 0.9882, accuracy: 0.4933\n",
      "test loss: 0.9729, accuracy: 0.4333\n",
      "epoch: 3\n",
      "train loss: 0.9403, accuracy: 0.5133\n",
      "test loss: 0.8965, accuracy: 0.6233\n",
      "epoch: 4\n",
      "train loss: 0.8820, accuracy: 0.5700\n",
      "test loss: 0.8771, accuracy: 0.5967\n",
      "epoch: 5\n",
      "train loss: 0.8617, accuracy: 0.5600\n",
      "test loss: 0.8670, accuracy: 0.5867\n",
      "epoch: 6\n",
      "train loss: 0.8313, accuracy: 0.5300\n",
      "test loss: 0.8654, accuracy: 0.6000\n",
      "epoch: 7\n",
      "train loss: 0.8086, accuracy: 0.5833\n",
      "test loss: 0.7950, accuracy: 0.5600\n",
      "epoch: 8\n",
      "train loss: 0.7948, accuracy: 0.5733\n",
      "test loss: 0.7921, accuracy: 0.5900\n",
      "epoch: 9\n",
      "train loss: 0.7728, accuracy: 0.5500\n",
      "test loss: 0.7718, accuracy: 0.5300\n",
      "epoch: 10\n",
      "train loss: 0.7643, accuracy: 0.5633\n",
      "test loss: 0.7796, accuracy: 0.5800\n",
      "epoch: 11\n",
      "train loss: 0.7862, accuracy: 0.5600\n",
      "test loss: 0.7701, accuracy: 0.5633\n",
      "epoch: 12\n",
      "train loss: 0.7914, accuracy: 0.5500\n",
      "test loss: 0.8218, accuracy: 0.6067\n",
      "epoch: 13\n",
      "train loss: 0.7633, accuracy: 0.5567\n",
      "test loss: 0.7757, accuracy: 0.5800\n",
      "epoch: 14\n",
      "train loss: 0.7612, accuracy: 0.5833\n",
      "test loss: 0.7800, accuracy: 0.5667\n",
      "epoch: 15\n",
      "train loss: 0.7408, accuracy: 0.5667\n",
      "test loss: 0.7654, accuracy: 0.5867\n",
      "epoch: 16\n",
      "train loss: 0.7453, accuracy: 0.5733\n",
      "test loss: 0.7648, accuracy: 0.6033\n",
      "epoch: 17\n",
      "train loss: 0.7843, accuracy: 0.5667\n",
      "test loss: 0.7415, accuracy: 0.5267\n",
      "epoch: 18\n",
      "train loss: 0.7592, accuracy: 0.5633\n",
      "test loss: 0.7388, accuracy: 0.5267\n",
      "epoch: 19\n",
      "train loss: 0.7396, accuracy: 0.5567\n",
      "test loss: 0.7791, accuracy: 0.5667\n",
      "epoch: 20\n",
      "train loss: 0.7376, accuracy: 0.5533\n",
      "test loss: 0.7478, accuracy: 0.5200\n",
      "epoch: 21\n",
      "train loss: 0.7469, accuracy: 0.5600\n",
      "test loss: 0.7617, accuracy: 0.6100\n",
      "epoch: 22\n",
      "train loss: 0.7349, accuracy: 0.5667\n",
      "test loss: 0.7476, accuracy: 0.5967\n",
      "epoch: 23\n",
      "train loss: 0.7650, accuracy: 0.5600\n",
      "test loss: 0.7336, accuracy: 0.5433\n",
      "epoch: 24\n",
      "train loss: 0.7495, accuracy: 0.5667\n",
      "test loss: 0.7366, accuracy: 0.5500\n",
      "epoch: 25\n",
      "train loss: 0.7414, accuracy: 0.5700\n",
      "test loss: 0.7516, accuracy: 0.6133\n",
      "epoch: 26\n",
      "train loss: 0.7298, accuracy: 0.5900\n",
      "test loss: 0.7327, accuracy: 0.5533\n",
      "epoch: 27\n",
      "train loss: 0.7297, accuracy: 0.5700\n",
      "test loss: 0.7356, accuracy: 0.5933\n",
      "epoch: 28\n",
      "train loss: 0.7267, accuracy: 0.5900\n",
      "test loss: 0.7240, accuracy: 0.5233\n",
      "epoch: 29\n",
      "train loss: 0.7359, accuracy: 0.5800\n",
      "test loss: 0.7203, accuracy: 0.5533\n",
      "epoch: 30\n",
      "train loss: 0.7273, accuracy: 0.5600\n",
      "test loss: 0.7382, accuracy: 0.5433\n",
      "epoch: 31\n",
      "train loss: 0.7338, accuracy: 0.5867\n",
      "test loss: 0.7492, accuracy: 0.5933\n",
      "epoch: 32\n",
      "train loss: 0.7092, accuracy: 0.5900\n",
      "test loss: 0.7241, accuracy: 0.5500\n",
      "epoch: 33\n",
      "train loss: 0.6970, accuracy: 0.5767\n",
      "test loss: 0.7483, accuracy: 0.6167\n",
      "epoch: 34\n",
      "train loss: 0.7048, accuracy: 0.5967\n",
      "test loss: 0.7170, accuracy: 0.5433\n",
      "epoch: 35\n",
      "train loss: 0.7069, accuracy: 0.5967\n",
      "test loss: 0.7164, accuracy: 0.5467\n",
      "epoch: 36\n",
      "train loss: 0.7199, accuracy: 0.5800\n",
      "test loss: 0.7318, accuracy: 0.6133\n",
      "epoch: 37\n",
      "train loss: 0.7035, accuracy: 0.6167\n",
      "test loss: 0.7266, accuracy: 0.5900\n",
      "epoch: 38\n",
      "train loss: 0.6992, accuracy: 0.5767\n",
      "test loss: 0.7419, accuracy: 0.6000\n",
      "epoch: 39\n",
      "train loss: 0.7103, accuracy: 0.6000\n",
      "test loss: 0.7077, accuracy: 0.5333\n",
      "epoch: 40\n",
      "train loss: 0.7064, accuracy: 0.5733\n",
      "test loss: 0.7335, accuracy: 0.5900\n",
      "epoch: 41\n",
      "train loss: 0.7022, accuracy: 0.5767\n",
      "test loss: 0.7087, accuracy: 0.6100\n",
      "epoch: 42\n",
      "train loss: 0.7047, accuracy: 0.6000\n",
      "test loss: 0.6951, accuracy: 0.5800\n",
      "epoch: 43\n",
      "train loss: 0.7071, accuracy: 0.6000\n",
      "test loss: 0.7009, accuracy: 0.6133\n",
      "epoch: 44\n",
      "train loss: 0.7041, accuracy: 0.6033\n",
      "test loss: 0.6914, accuracy: 0.5833\n",
      "epoch: 45\n",
      "train loss: 0.6854, accuracy: 0.5767\n",
      "test loss: 0.7122, accuracy: 0.6100\n",
      "epoch: 46\n",
      "train loss: 0.6771, accuracy: 0.6067\n",
      "test loss: 0.6874, accuracy: 0.5600\n",
      "epoch: 47\n",
      "train loss: 0.6781, accuracy: 0.5900\n",
      "test loss: 0.6995, accuracy: 0.6467\n",
      "epoch: 48\n",
      "train loss: 0.6875, accuracy: 0.5933\n",
      "test loss: 0.7021, accuracy: 0.6300\n",
      "epoch: 49\n",
      "train loss: 0.6641, accuracy: 0.6000\n",
      "test loss: 0.6897, accuracy: 0.6000\n",
      "epoch: 50\n",
      "train loss: 0.6683, accuracy: 0.6200\n",
      "test loss: 0.6850, accuracy: 0.5867\n",
      "epoch: 51\n",
      "train loss: 0.6554, accuracy: 0.6033\n",
      "test loss: 0.6698, accuracy: 0.6367\n",
      "epoch: 52\n",
      "train loss: 0.6528, accuracy: 0.6200\n",
      "test loss: 0.6789, accuracy: 0.5800\n",
      "epoch: 53\n",
      "train loss: 0.6500, accuracy: 0.6467\n",
      "test loss: 0.6691, accuracy: 0.6467\n",
      "epoch: 54\n",
      "train loss: 0.6449, accuracy: 0.6267\n",
      "test loss: 0.6562, accuracy: 0.6633\n",
      "epoch: 55\n",
      "train loss: 0.6549, accuracy: 0.6500\n",
      "test loss: 0.6892, accuracy: 0.6467\n",
      "epoch: 56\n",
      "train loss: 0.6504, accuracy: 0.6400\n",
      "test loss: 0.6421, accuracy: 0.6167\n",
      "epoch: 57\n",
      "train loss: 0.6396, accuracy: 0.6667\n",
      "test loss: 0.6451, accuracy: 0.6167\n",
      "epoch: 58\n",
      "train loss: 0.6251, accuracy: 0.6733\n",
      "test loss: 0.6415, accuracy: 0.6200\n",
      "epoch: 59\n",
      "train loss: 0.6150, accuracy: 0.6433\n",
      "test loss: 0.6285, accuracy: 0.6267\n",
      "epoch: 60\n",
      "train loss: 0.6122, accuracy: 0.6633\n",
      "test loss: 0.6169, accuracy: 0.6400\n",
      "epoch: 61\n",
      "train loss: 0.5987, accuracy: 0.6767\n",
      "test loss: 0.6183, accuracy: 0.6167\n",
      "epoch: 62\n",
      "train loss: 0.5920, accuracy: 0.6667\n",
      "test loss: 0.6076, accuracy: 0.6967\n",
      "epoch: 63\n",
      "train loss: 0.5914, accuracy: 0.6700\n",
      "test loss: 0.6020, accuracy: 0.6300\n",
      "epoch: 64\n",
      "train loss: 0.5911, accuracy: 0.6867\n",
      "test loss: 0.5981, accuracy: 0.6533\n",
      "epoch: 65\n",
      "train loss: 0.5626, accuracy: 0.7000\n",
      "test loss: 0.6124, accuracy: 0.7100\n",
      "epoch: 66\n",
      "train loss: 0.5707, accuracy: 0.7067\n",
      "test loss: 0.5760, accuracy: 0.6567\n",
      "epoch: 67\n",
      "train loss: 0.5574, accuracy: 0.6900\n",
      "test loss: 0.5834, accuracy: 0.6967\n",
      "epoch: 68\n",
      "train loss: 0.5560, accuracy: 0.7133\n",
      "test loss: 0.5589, accuracy: 0.6933\n",
      "epoch: 69\n",
      "train loss: 0.5355, accuracy: 0.7267\n",
      "test loss: 0.5530, accuracy: 0.6967\n",
      "epoch: 70\n",
      "train loss: 0.5303, accuracy: 0.7300\n",
      "test loss: 0.5380, accuracy: 0.7267\n",
      "epoch: 71\n",
      "train loss: 0.5207, accuracy: 0.7167\n",
      "test loss: 0.5315, accuracy: 0.7367\n",
      "epoch: 72\n",
      "train loss: 0.5094, accuracy: 0.7533\n",
      "test loss: 0.5340, accuracy: 0.7367\n",
      "epoch: 73\n",
      "train loss: 0.5110, accuracy: 0.7400\n",
      "test loss: 0.5127, accuracy: 0.7433\n",
      "epoch: 74\n",
      "train loss: 0.4921, accuracy: 0.7500\n",
      "test loss: 0.5051, accuracy: 0.7400\n",
      "epoch: 75\n",
      "train loss: 0.4882, accuracy: 0.7700\n",
      "test loss: 0.5268, accuracy: 0.7467\n",
      "epoch: 76\n",
      "train loss: 0.4859, accuracy: 0.7533\n",
      "test loss: 0.4997, accuracy: 0.7400\n",
      "epoch: 77\n",
      "train loss: 0.4701, accuracy: 0.7767\n",
      "test loss: 0.4814, accuracy: 0.7800\n",
      "epoch: 78\n",
      "train loss: 0.4574, accuracy: 0.7733\n",
      "test loss: 0.4736, accuracy: 0.7700\n",
      "epoch: 79\n",
      "train loss: 0.4547, accuracy: 0.7533\n",
      "test loss: 0.4665, accuracy: 0.7900\n",
      "epoch: 80\n",
      "train loss: 0.4533, accuracy: 0.7667\n",
      "test loss: 0.4601, accuracy: 0.7900\n",
      "epoch: 81\n",
      "train loss: 0.4431, accuracy: 0.7967\n",
      "test loss: 0.4496, accuracy: 0.7900\n",
      "epoch: 82\n",
      "train loss: 0.4309, accuracy: 0.8167\n",
      "test loss: 0.4421, accuracy: 0.7867\n",
      "epoch: 83\n",
      "train loss: 0.4282, accuracy: 0.8167\n",
      "test loss: 0.4431, accuracy: 0.7833\n",
      "epoch: 84\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4353, accuracy: 0.8200\n",
      "epoch: 85\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4260, accuracy: 0.8367\n",
      "epoch: 86\n",
      "train loss: 0.4047, accuracy: 0.8167\n",
      "test loss: 0.4248, accuracy: 0.8400\n",
      "epoch: 87\n",
      "train loss: 0.3968, accuracy: 0.8367\n",
      "test loss: 0.4031, accuracy: 0.8333\n",
      "epoch: 88\n",
      "train loss: 0.3942, accuracy: 0.8267\n",
      "test loss: 0.4091, accuracy: 0.8333\n",
      "epoch: 89\n",
      "train loss: 0.3746, accuracy: 0.8333\n",
      "test loss: 0.3962, accuracy: 0.8100\n",
      "epoch: 90\n",
      "train loss: 0.3772, accuracy: 0.8333\n",
      "test loss: 0.3831, accuracy: 0.8300\n",
      "epoch: 91\n",
      "train loss: 0.3669, accuracy: 0.8467\n",
      "test loss: 0.3844, accuracy: 0.8433\n",
      "epoch: 92\n",
      "train loss: 0.3554, accuracy: 0.8600\n",
      "test loss: 0.3752, accuracy: 0.8567\n",
      "epoch: 93\n",
      "train loss: 0.3491, accuracy: 0.8700\n",
      "test loss: 0.3720, accuracy: 0.8400\n",
      "epoch: 94\n",
      "train loss: 0.3440, accuracy: 0.8533\n",
      "test loss: 0.3595, accuracy: 0.8600\n",
      "epoch: 95\n",
      "train loss: 0.3408, accuracy: 0.8633\n",
      "test loss: 0.3533, accuracy: 0.8700\n",
      "epoch: 96\n",
      "train loss: 0.3336, accuracy: 0.8600\n",
      "test loss: 0.3515, accuracy: 0.8800\n",
      "epoch: 97\n",
      "train loss: 0.3295, accuracy: 0.8700\n",
      "test loss: 0.3437, accuracy: 0.8933\n",
      "epoch: 98\n",
      "train loss: 0.3208, accuracy: 0.9133\n",
      "test loss: 0.3433, accuracy: 0.8533\n",
      "epoch: 99\n",
      "train loss: 0.3212, accuracy: 0.8667\n",
      "test loss: 0.3302, accuracy: 0.8633\n",
      "epoch: 100\n",
      "train loss: 0.3146, accuracy: 0.8700\n",
      "test loss: 0.3248, accuracy: 0.8600\n",
      "epoch: 101\n",
      "train loss: 0.3090, accuracy: 0.8633\n",
      "test loss: 0.3268, accuracy: 0.8833\n",
      "epoch: 102\n",
      "train loss: 0.3024, accuracy: 0.8933\n",
      "test loss: 0.3195, accuracy: 0.8533\n",
      "epoch: 103\n",
      "train loss: 0.3009, accuracy: 0.8933\n",
      "test loss: 0.3122, accuracy: 0.8900\n",
      "epoch: 104\n",
      "train loss: 0.2930, accuracy: 0.9067\n",
      "test loss: 0.3087, accuracy: 0.8633\n",
      "epoch: 105\n",
      "train loss: 0.2881, accuracy: 0.9067\n",
      "test loss: 0.3111, accuracy: 0.8567\n",
      "epoch: 106\n",
      "train loss: 0.2785, accuracy: 0.9167\n",
      "test loss: 0.3026, accuracy: 0.8867\n",
      "epoch: 107\n",
      "train loss: 0.2786, accuracy: 0.8933\n",
      "test loss: 0.2952, accuracy: 0.8833\n",
      "epoch: 108\n",
      "train loss: 0.2756, accuracy: 0.9100\n",
      "test loss: 0.2912, accuracy: 0.8800\n",
      "epoch: 109\n",
      "train loss: 0.2744, accuracy: 0.9200\n",
      "test loss: 0.2965, accuracy: 0.8667\n",
      "epoch: 110\n",
      "train loss: 0.2705, accuracy: 0.9267\n",
      "test loss: 0.2856, accuracy: 0.8933\n",
      "epoch: 111\n",
      "train loss: 0.2645, accuracy: 0.9067\n",
      "test loss: 0.2814, accuracy: 0.8867\n",
      "epoch: 112\n",
      "train loss: 0.2624, accuracy: 0.9100\n",
      "test loss: 0.2835, accuracy: 0.8700\n",
      "epoch: 113\n",
      "train loss: 0.2608, accuracy: 0.9067\n",
      "test loss: 0.2752, accuracy: 0.8933\n",
      "epoch: 114\n",
      "train loss: 0.2524, accuracy: 0.9167\n",
      "test loss: 0.2740, accuracy: 0.9033\n",
      "epoch: 115\n",
      "train loss: 0.2484, accuracy: 0.9167\n",
      "test loss: 0.2688, accuracy: 0.8967\n",
      "epoch: 116\n",
      "train loss: 0.2480, accuracy: 0.9200\n",
      "test loss: 0.2768, accuracy: 0.8600\n",
      "epoch: 117\n",
      "train loss: 0.2445, accuracy: 0.9133\n",
      "test loss: 0.2635, accuracy: 0.9067\n",
      "epoch: 118\n",
      "train loss: 0.2444, accuracy: 0.9267\n",
      "test loss: 0.2593, accuracy: 0.8967\n",
      "epoch: 119\n",
      "train loss: 0.2379, accuracy: 0.9267\n",
      "test loss: 0.2576, accuracy: 0.8867\n",
      "epoch: 120\n",
      "train loss: 0.2361, accuracy: 0.9200\n",
      "test loss: 0.2573, accuracy: 0.9200\n",
      "epoch: 121\n",
      "train loss: 0.2372, accuracy: 0.9333\n",
      "test loss: 0.2534, accuracy: 0.9067\n",
      "epoch: 122\n",
      "train loss: 0.2305, accuracy: 0.9333\n",
      "test loss: 0.2511, accuracy: 0.8867\n",
      "epoch: 123\n",
      "train loss: 0.2277, accuracy: 0.9367\n",
      "test loss: 0.2531, accuracy: 0.9000\n",
      "epoch: 124\n",
      "train loss: 0.2318, accuracy: 0.9233\n",
      "test loss: 0.2456, accuracy: 0.9067\n",
      "epoch: 125\n",
      "train loss: 0.2248, accuracy: 0.9400\n",
      "test loss: 0.2454, accuracy: 0.9133\n",
      "epoch: 126\n",
      "train loss: 0.2229, accuracy: 0.9433\n",
      "test loss: 0.2427, accuracy: 0.9033\n",
      "epoch: 127\n",
      "train loss: 0.2264, accuracy: 0.9167\n",
      "test loss: 0.2433, accuracy: 0.9000\n",
      "epoch: 128\n",
      "train loss: 0.2152, accuracy: 0.9333\n",
      "test loss: 0.2373, accuracy: 0.9133\n",
      "epoch: 129\n",
      "train loss: 0.2176, accuracy: 0.9367\n",
      "test loss: 0.2402, accuracy: 0.8967\n",
      "epoch: 130\n",
      "train loss: 0.2149, accuracy: 0.9200\n",
      "test loss: 0.2332, accuracy: 0.9033\n",
      "epoch: 131\n",
      "train loss: 0.2136, accuracy: 0.9333\n",
      "test loss: 0.2377, accuracy: 0.8933\n",
      "epoch: 132\n",
      "train loss: 0.2143, accuracy: 0.9200\n",
      "test loss: 0.2304, accuracy: 0.9167\n",
      "epoch: 133\n",
      "train loss: 0.2141, accuracy: 0.9267\n",
      "test loss: 0.2449, accuracy: 0.8833\n",
      "epoch: 134\n",
      "train loss: 0.2088, accuracy: 0.9133\n",
      "test loss: 0.2362, accuracy: 0.8900\n",
      "epoch: 135\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2278, accuracy: 0.9133\n",
      "epoch: 136\n",
      "train loss: 0.2021, accuracy: 0.9333\n",
      "test loss: 0.2275, accuracy: 0.9200\n",
      "epoch: 137\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2236, accuracy: 0.9167\n",
      "epoch: 138\n",
      "train loss: 0.1991, accuracy: 0.9267\n",
      "test loss: 0.2207, accuracy: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 139\n",
      "train loss: 0.1960, accuracy: 0.9433\n",
      "test loss: 0.2216, accuracy: 0.9267\n",
      "epoch: 140\n",
      "train loss: 0.1981, accuracy: 0.9333\n",
      "test loss: 0.2178, accuracy: 0.9233\n",
      "epoch: 141\n",
      "train loss: 0.1945, accuracy: 0.9367\n",
      "test loss: 0.2194, accuracy: 0.9200\n",
      "epoch: 142\n",
      "train loss: 0.1969, accuracy: 0.9233\n",
      "test loss: 0.2158, accuracy: 0.9233\n",
      "epoch: 143\n",
      "train loss: 0.1942, accuracy: 0.9333\n",
      "test loss: 0.2127, accuracy: 0.9167\n",
      "epoch: 144\n",
      "train loss: 0.1886, accuracy: 0.9433\n",
      "test loss: 0.2288, accuracy: 0.8933\n",
      "epoch: 145\n",
      "train loss: 0.1966, accuracy: 0.9133\n",
      "test loss: 0.2103, accuracy: 0.9167\n",
      "epoch: 146\n",
      "train loss: 0.1899, accuracy: 0.9300\n",
      "test loss: 0.2124, accuracy: 0.9100\n",
      "epoch: 147\n",
      "train loss: 0.1890, accuracy: 0.9400\n",
      "test loss: 0.2104, accuracy: 0.9200\n",
      "epoch: 148\n",
      "train loss: 0.1865, accuracy: 0.9333\n",
      "test loss: 0.2080, accuracy: 0.9267\n",
      "epoch: 149\n",
      "train loss: 0.1825, accuracy: 0.9367\n",
      "test loss: 0.2285, accuracy: 0.8967\n",
      "epoch: 150\n",
      "train loss: 0.1890, accuracy: 0.9233\n",
      "test loss: 0.2152, accuracy: 0.9067\n",
      "epoch: 151\n",
      "train loss: 0.1828, accuracy: 0.9400\n",
      "test loss: 0.2042, accuracy: 0.9300\n",
      "epoch: 152\n",
      "train loss: 0.1807, accuracy: 0.9467\n",
      "test loss: 0.2136, accuracy: 0.9067\n",
      "epoch: 153\n",
      "train loss: 0.1795, accuracy: 0.9333\n",
      "test loss: 0.2076, accuracy: 0.9033\n",
      "epoch: 154\n",
      "train loss: 0.1797, accuracy: 0.9300\n",
      "test loss: 0.2052, accuracy: 0.9200\n",
      "epoch: 155\n",
      "train loss: 0.1816, accuracy: 0.9400\n",
      "test loss: 0.2015, accuracy: 0.9233\n",
      "epoch: 156\n",
      "train loss: 0.1791, accuracy: 0.9300\n",
      "test loss: 0.2012, accuracy: 0.9333\n",
      "epoch: 157\n",
      "train loss: 0.1791, accuracy: 0.9367\n",
      "test loss: 0.1978, accuracy: 0.9267\n",
      "epoch: 158\n",
      "train loss: 0.1751, accuracy: 0.9400\n",
      "test loss: 0.2046, accuracy: 0.9033\n",
      "epoch: 159\n",
      "train loss: 0.1719, accuracy: 0.9433\n",
      "test loss: 0.2039, accuracy: 0.9067\n",
      "epoch: 160\n",
      "train loss: 0.1730, accuracy: 0.9433\n",
      "test loss: 0.1963, accuracy: 0.9300\n",
      "epoch: 161\n",
      "train loss: 0.1690, accuracy: 0.9467\n",
      "test loss: 0.1973, accuracy: 0.9267\n",
      "epoch: 162\n",
      "train loss: 0.1676, accuracy: 0.9567\n",
      "test loss: 0.1951, accuracy: 0.9400\n",
      "epoch: 163\n",
      "train loss: 0.1734, accuracy: 0.9433\n",
      "test loss: 0.1941, accuracy: 0.9300\n",
      "epoch: 164\n",
      "train loss: 0.1725, accuracy: 0.9233\n",
      "test loss: 0.1922, accuracy: 0.9367\n",
      "epoch: 165\n",
      "train loss: 0.1665, accuracy: 0.9433\n",
      "test loss: 0.1951, accuracy: 0.9233\n",
      "epoch: 166\n",
      "train loss: 0.1698, accuracy: 0.9433\n",
      "test loss: 0.1907, accuracy: 0.9333\n",
      "epoch: 167\n",
      "train loss: 0.1737, accuracy: 0.9300\n",
      "test loss: 0.1923, accuracy: 0.9300\n",
      "epoch: 168\n",
      "train loss: 0.1661, accuracy: 0.9367\n",
      "test loss: 0.1944, accuracy: 0.9300\n",
      "epoch: 169\n",
      "train loss: 0.1635, accuracy: 0.9567\n",
      "test loss: 0.1894, accuracy: 0.9233\n",
      "epoch: 170\n",
      "train loss: 0.1688, accuracy: 0.9400\n",
      "test loss: 0.1878, accuracy: 0.9300\n",
      "epoch: 171\n",
      "train loss: 0.1675, accuracy: 0.9300\n",
      "test loss: 0.1899, accuracy: 0.9367\n",
      "epoch: 172\n",
      "train loss: 0.1602, accuracy: 0.9500\n",
      "test loss: 0.1859, accuracy: 0.9400\n",
      "epoch: 173\n",
      "train loss: 0.1644, accuracy: 0.9433\n",
      "test loss: 0.1901, accuracy: 0.9267\n",
      "epoch: 174\n",
      "train loss: 0.1597, accuracy: 0.9500\n",
      "test loss: 0.1878, accuracy: 0.9267\n",
      "epoch: 175\n",
      "train loss: 0.1599, accuracy: 0.9400\n",
      "test loss: 0.1840, accuracy: 0.9367\n",
      "epoch: 176\n",
      "train loss: 0.1612, accuracy: 0.9533\n",
      "test loss: 0.1909, accuracy: 0.9200\n",
      "epoch: 177\n",
      "train loss: 0.1582, accuracy: 0.9500\n",
      "test loss: 0.1829, accuracy: 0.9433\n",
      "epoch: 178\n",
      "train loss: 0.1599, accuracy: 0.9433\n",
      "test loss: 0.1847, accuracy: 0.9333\n",
      "epoch: 179\n",
      "train loss: 0.1541, accuracy: 0.9400\n",
      "test loss: 0.1892, accuracy: 0.9267\n",
      "epoch: 180\n",
      "train loss: 0.1539, accuracy: 0.9500\n",
      "test loss: 0.1814, accuracy: 0.9367\n",
      "epoch: 181\n",
      "train loss: 0.1572, accuracy: 0.9400\n",
      "test loss: 0.1960, accuracy: 0.9333\n",
      "epoch: 182\n",
      "train loss: 0.1612, accuracy: 0.9400\n",
      "test loss: 0.1824, accuracy: 0.9367\n",
      "epoch: 183\n",
      "train loss: 0.1533, accuracy: 0.9533\n",
      "test loss: 0.1836, accuracy: 0.9233\n",
      "epoch: 184\n",
      "train loss: 0.1526, accuracy: 0.9533\n",
      "test loss: 0.1787, accuracy: 0.9433\n",
      "epoch: 185\n",
      "train loss: 0.1530, accuracy: 0.9533\n",
      "test loss: 0.1892, accuracy: 0.9100\n",
      "epoch: 186\n",
      "train loss: 0.1558, accuracy: 0.9433\n",
      "test loss: 0.1840, accuracy: 0.9267\n",
      "epoch: 187\n",
      "train loss: 0.1559, accuracy: 0.9400\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 188\n",
      "train loss: 0.1513, accuracy: 0.9500\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 189\n",
      "train loss: 0.1491, accuracy: 0.9533\n",
      "test loss: 0.1850, accuracy: 0.9267\n",
      "epoch: 190\n",
      "train loss: 0.1488, accuracy: 0.9500\n",
      "test loss: 0.1790, accuracy: 0.9333\n",
      "epoch: 191\n",
      "train loss: 0.1499, accuracy: 0.9467\n",
      "test loss: 0.1755, accuracy: 0.9400\n",
      "epoch: 192\n",
      "train loss: 0.1469, accuracy: 0.9433\n",
      "test loss: 0.1788, accuracy: 0.9267\n",
      "epoch: 193\n",
      "train loss: 0.1528, accuracy: 0.9433\n",
      "test loss: 0.1769, accuracy: 0.9367\n",
      "epoch: 194\n",
      "train loss: 0.1482, accuracy: 0.9433\n",
      "test loss: 0.1737, accuracy: 0.9467\n",
      "epoch: 195\n",
      "train loss: 0.1425, accuracy: 0.9533\n",
      "test loss: 0.1770, accuracy: 0.9333\n",
      "epoch: 196\n",
      "train loss: 0.1493, accuracy: 0.9433\n",
      "test loss: 0.1757, accuracy: 0.9300\n",
      "epoch: 197\n",
      "train loss: 0.1436, accuracy: 0.9467\n",
      "test loss: 0.1752, accuracy: 0.9300\n",
      "epoch: 198\n",
      "train loss: 0.1449, accuracy: 0.9433\n",
      "test loss: 0.1721, accuracy: 0.9433\n",
      "epoch: 199\n",
      "train loss: 0.1456, accuracy: 0.9467\n",
      "test loss: 0.1734, accuracy: 0.9300\n",
      "epoch: 200\n",
      "train loss: 0.1447, accuracy: 0.9500\n",
      "test loss: 0.1710, accuracy: 0.9433\n",
      "epoch: 201\n",
      "train loss: 0.1426, accuracy: 0.9500\n",
      "test loss: 0.1707, accuracy: 0.9433\n",
      "epoch: 202\n",
      "train loss: 0.1406, accuracy: 0.9600\n",
      "test loss: 0.1733, accuracy: 0.9400\n",
      "epoch: 203\n",
      "train loss: 0.1388, accuracy: 0.9667\n",
      "test loss: 0.1760, accuracy: 0.9333\n",
      "epoch: 204\n",
      "train loss: 0.1427, accuracy: 0.9433\n",
      "test loss: 0.1696, accuracy: 0.9467\n",
      "epoch: 205\n",
      "train loss: 0.1403, accuracy: 0.9567\n",
      "test loss: 0.1754, accuracy: 0.9433\n",
      "epoch: 206\n",
      "train loss: 0.1436, accuracy: 0.9400\n",
      "test loss: 0.1698, accuracy: 0.9433\n",
      "epoch: 207\n",
      "train loss: 0.1410, accuracy: 0.9567\n",
      "test loss: 0.1723, accuracy: 0.9333\n",
      "epoch: 208\n",
      "train loss: 0.1418, accuracy: 0.9567\n",
      "test loss: 0.1685, accuracy: 0.9400\n",
      "epoch: 209\n",
      "train loss: 0.1408, accuracy: 0.9467\n",
      "test loss: 0.1695, accuracy: 0.9367\n",
      "epoch: 210\n",
      "train loss: 0.1382, accuracy: 0.9533\n",
      "test loss: 0.1676, accuracy: 0.9467\n",
      "epoch: 211\n",
      "train loss: 0.1359, accuracy: 0.9633\n",
      "test loss: 0.1815, accuracy: 0.9367\n",
      "epoch: 212\n",
      "train loss: 0.1377, accuracy: 0.9600\n",
      "test loss: 0.1677, accuracy: 0.9433\n",
      "epoch: 213\n",
      "train loss: 0.1350, accuracy: 0.9467\n",
      "test loss: 0.1669, accuracy: 0.9467\n",
      "epoch: 214\n",
      "train loss: 0.1340, accuracy: 0.9600\n",
      "test loss: 0.1727, accuracy: 0.9367\n",
      "epoch: 215\n",
      "train loss: 0.1425, accuracy: 0.9567\n",
      "test loss: 0.1654, accuracy: 0.9467\n",
      "epoch: 216\n",
      "train loss: 0.1404, accuracy: 0.9533\n",
      "test loss: 0.1685, accuracy: 0.9333\n",
      "epoch: 217\n",
      "train loss: 0.1349, accuracy: 0.9567\n",
      "test loss: 0.1688, accuracy: 0.9367\n",
      "epoch: 218\n",
      "train loss: 0.1410, accuracy: 0.9533\n",
      "test loss: 0.1678, accuracy: 0.9300\n",
      "epoch: 219\n",
      "train loss: 0.1342, accuracy: 0.9533\n",
      "test loss: 0.1663, accuracy: 0.9400\n",
      "epoch: 220\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1649, accuracy: 0.9433\n",
      "epoch: 221\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1632, accuracy: 0.9533\n",
      "epoch: 222\n",
      "train loss: 0.1318, accuracy: 0.9633\n",
      "test loss: 0.1661, accuracy: 0.9433\n",
      "epoch: 223\n",
      "train loss: 0.1345, accuracy: 0.9567\n",
      "test loss: 0.1629, accuracy: 0.9467\n",
      "epoch: 224\n",
      "train loss: 0.1319, accuracy: 0.9600\n",
      "test loss: 0.1629, accuracy: 0.9533\n",
      "epoch: 225\n",
      "train loss: 0.1334, accuracy: 0.9533\n",
      "test loss: 0.1620, accuracy: 0.9500\n",
      "epoch: 226\n",
      "train loss: 0.1339, accuracy: 0.9633\n",
      "test loss: 0.1675, accuracy: 0.9367\n",
      "epoch: 227\n",
      "train loss: 0.1364, accuracy: 0.9467\n",
      "test loss: 0.1613, accuracy: 0.9467\n",
      "epoch: 228\n",
      "train loss: 0.1319, accuracy: 0.9533\n",
      "test loss: 0.1616, accuracy: 0.9467\n",
      "epoch: 229\n",
      "train loss: 0.1301, accuracy: 0.9600\n",
      "test loss: 0.1640, accuracy: 0.9300\n",
      "epoch: 230\n",
      "train loss: 0.1298, accuracy: 0.9467\n",
      "test loss: 0.1656, accuracy: 0.9333\n",
      "epoch: 231\n",
      "train loss: 0.1289, accuracy: 0.9567\n",
      "test loss: 0.1633, accuracy: 0.9500\n",
      "epoch: 232\n",
      "train loss: 0.1276, accuracy: 0.9600\n",
      "test loss: 0.1632, accuracy: 0.9433\n",
      "epoch: 233\n",
      "train loss: 0.1269, accuracy: 0.9567\n",
      "test loss: 0.1675, accuracy: 0.9333\n",
      "epoch: 234\n",
      "train loss: 0.1311, accuracy: 0.9600\n",
      "test loss: 0.1630, accuracy: 0.9433\n",
      "epoch: 235\n",
      "train loss: 0.1335, accuracy: 0.9500\n",
      "test loss: 0.1602, accuracy: 0.9467\n",
      "epoch: 236\n",
      "train loss: 0.1275, accuracy: 0.9633\n",
      "test loss: 0.1596, accuracy: 0.9500\n",
      "epoch: 237\n",
      "train loss: 0.1227, accuracy: 0.9633\n",
      "test loss: 0.1619, accuracy: 0.9467\n",
      "epoch: 238\n",
      "train loss: 0.1275, accuracy: 0.9600\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 239\n",
      "train loss: 0.1270, accuracy: 0.9533\n",
      "test loss: 0.1657, accuracy: 0.9367\n",
      "epoch: 240\n",
      "train loss: 0.1251, accuracy: 0.9500\n",
      "test loss: 0.1583, accuracy: 0.9433\n",
      "epoch: 241\n",
      "train loss: 0.1273, accuracy: 0.9533\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 242\n",
      "train loss: 0.1268, accuracy: 0.9567\n",
      "test loss: 0.1584, accuracy: 0.9500\n",
      "epoch: 243\n",
      "train loss: 0.1242, accuracy: 0.9633\n",
      "test loss: 0.1580, accuracy: 0.9433\n",
      "epoch: 244\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1642, accuracy: 0.9433\n",
      "epoch: 245\n",
      "train loss: 0.1247, accuracy: 0.9600\n",
      "test loss: 0.1562, accuracy: 0.9500\n",
      "epoch: 246\n",
      "train loss: 0.1273, accuracy: 0.9567\n",
      "test loss: 0.1555, accuracy: 0.9500\n",
      "epoch: 247\n",
      "train loss: 0.1305, accuracy: 0.9500\n",
      "test loss: 0.1552, accuracy: 0.9533\n",
      "epoch: 248\n",
      "train loss: 0.1274, accuracy: 0.9500\n",
      "test loss: 0.1579, accuracy: 0.9467\n",
      "epoch: 249\n",
      "train loss: 0.1282, accuracy: 0.9500\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 250\n",
      "train loss: 0.1286, accuracy: 0.9600\n",
      "test loss: 0.1548, accuracy: 0.9533\n",
      "epoch: 251\n",
      "train loss: 0.1251, accuracy: 0.9667\n",
      "test loss: 0.1566, accuracy: 0.9400\n",
      "epoch: 252\n",
      "train loss: 0.1213, accuracy: 0.9533\n",
      "test loss: 0.1615, accuracy: 0.9167\n",
      "epoch: 253\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 254\n",
      "train loss: 0.1238, accuracy: 0.9567\n",
      "test loss: 0.1564, accuracy: 0.9400\n",
      "epoch: 255\n",
      "train loss: 0.1212, accuracy: 0.9633\n",
      "test loss: 0.1571, accuracy: 0.9533\n",
      "epoch: 256\n",
      "train loss: 0.1261, accuracy: 0.9600\n",
      "test loss: 0.1546, accuracy: 0.9433\n",
      "epoch: 257\n",
      "train loss: 0.1238, accuracy: 0.9600\n",
      "test loss: 0.1575, accuracy: 0.9533\n",
      "epoch: 258\n",
      "train loss: 0.1224, accuracy: 0.9633\n",
      "test loss: 0.1583, accuracy: 0.9333\n",
      "epoch: 259\n",
      "train loss: 0.1239, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 260\n",
      "train loss: 0.1288, accuracy: 0.9567\n",
      "test loss: 0.1522, accuracy: 0.9500\n",
      "epoch: 261\n",
      "train loss: 0.1245, accuracy: 0.9567\n",
      "test loss: 0.1530, accuracy: 0.9500\n",
      "epoch: 262\n",
      "train loss: 0.1225, accuracy: 0.9600\n",
      "test loss: 0.1549, accuracy: 0.9433\n",
      "epoch: 263\n",
      "train loss: 0.1193, accuracy: 0.9533\n",
      "test loss: 0.1523, accuracy: 0.9533\n",
      "epoch: 264\n",
      "train loss: 0.1177, accuracy: 0.9633\n",
      "test loss: 0.1553, accuracy: 0.9433\n",
      "epoch: 265\n",
      "train loss: 0.1140, accuracy: 0.9633\n",
      "test loss: 0.1566, accuracy: 0.9500\n",
      "epoch: 266\n",
      "train loss: 0.1210, accuracy: 0.9500\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 267\n",
      "train loss: 0.1224, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 268\n",
      "train loss: 0.1234, accuracy: 0.9500\n",
      "test loss: 0.1504, accuracy: 0.9533\n",
      "epoch: 269\n",
      "train loss: 0.1163, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 270\n",
      "train loss: 0.1191, accuracy: 0.9567\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 271\n",
      "train loss: 0.1187, accuracy: 0.9567\n",
      "test loss: 0.1543, accuracy: 0.9367\n",
      "epoch: 272\n",
      "train loss: 0.1182, accuracy: 0.9633\n",
      "test loss: 0.1567, accuracy: 0.9267\n",
      "epoch: 273\n",
      "train loss: 0.1137, accuracy: 0.9567\n",
      "test loss: 0.1490, accuracy: 0.9533\n",
      "epoch: 274\n",
      "train loss: 0.1205, accuracy: 0.9467\n",
      "test loss: 0.1541, accuracy: 0.9400\n",
      "epoch: 275\n",
      "train loss: 0.1159, accuracy: 0.9633\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 276\n",
      "train loss: 0.1151, accuracy: 0.9633\n",
      "test loss: 0.1518, accuracy: 0.9533\n",
      "epoch: 277\n",
      "train loss: 0.1166, accuracy: 0.9633\n",
      "test loss: 0.1578, accuracy: 0.9233\n",
      "epoch: 278\n",
      "train loss: 0.1172, accuracy: 0.9667\n",
      "test loss: 0.1548, accuracy: 0.9400\n",
      "epoch: 279\n",
      "train loss: 0.1134, accuracy: 0.9633\n",
      "test loss: 0.1499, accuracy: 0.9533\n",
      "epoch: 280\n",
      "train loss: 0.1162, accuracy: 0.9533\n",
      "test loss: 0.1506, accuracy: 0.9500\n",
      "epoch: 281\n",
      "train loss: 0.1189, accuracy: 0.9700\n",
      "test loss: 0.1478, accuracy: 0.9567\n",
      "epoch: 282\n",
      "train loss: 0.1188, accuracy: 0.9600\n",
      "test loss: 0.1484, accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 283\n",
      "train loss: 0.1142, accuracy: 0.9667\n",
      "test loss: 0.1524, accuracy: 0.9433\n",
      "epoch: 284\n",
      "train loss: 0.1123, accuracy: 0.9700\n",
      "test loss: 0.1500, accuracy: 0.9433\n",
      "epoch: 285\n",
      "train loss: 0.1192, accuracy: 0.9600\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 286\n",
      "train loss: 0.1114, accuracy: 0.9600\n",
      "test loss: 0.1471, accuracy: 0.9533\n",
      "epoch: 287\n",
      "train loss: 0.1109, accuracy: 0.9633\n",
      "test loss: 0.1481, accuracy: 0.9567\n",
      "epoch: 288\n",
      "train loss: 0.1143, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 289\n",
      "train loss: 0.1126, accuracy: 0.9600\n",
      "test loss: 0.1486, accuracy: 0.9500\n",
      "epoch: 290\n",
      "train loss: 0.1125, accuracy: 0.9733\n",
      "test loss: 0.1485, accuracy: 0.9533\n",
      "epoch: 291\n",
      "train loss: 0.1136, accuracy: 0.9700\n",
      "test loss: 0.1472, accuracy: 0.9500\n",
      "epoch: 292\n",
      "train loss: 0.1130, accuracy: 0.9600\n",
      "test loss: 0.1495, accuracy: 0.9433\n",
      "epoch: 293\n",
      "train loss: 0.1118, accuracy: 0.9700\n",
      "test loss: 0.1456, accuracy: 0.9533\n",
      "epoch: 294\n",
      "train loss: 0.1111, accuracy: 0.9600\n",
      "test loss: 0.1591, accuracy: 0.9367\n",
      "epoch: 295\n",
      "train loss: 0.1170, accuracy: 0.9533\n",
      "test loss: 0.1451, accuracy: 0.9533\n",
      "epoch: 296\n",
      "train loss: 0.1115, accuracy: 0.9567\n",
      "test loss: 0.1459, accuracy: 0.9567\n",
      "epoch: 297\n",
      "train loss: 0.1096, accuracy: 0.9667\n",
      "test loss: 0.1464, accuracy: 0.9533\n",
      "epoch: 298\n",
      "train loss: 0.1128, accuracy: 0.9500\n",
      "test loss: 0.1476, accuracy: 0.9533\n",
      "epoch: 299\n",
      "train loss: 0.1090, accuracy: 0.9633\n",
      "test loss: 0.1471, accuracy: 0.9467\n",
      "epoch: 300\n",
      "train loss: 0.1082, accuracy: 0.9667\n",
      "test loss: 0.1458, accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import Variable\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "test_set = dezero.datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    \n",
    "    for x, t in train_loader: # 1. 훈련용 미니배치 데이터\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t) # 2. 훈련 데이터의 인식 정확도\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "        \n",
    "        \n",
    "    print('epoch: {}'.format(epoch + 1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad(): # 3. 기울기 불필요 모드\n",
    "        for x, t in test_loader: # 4. 테스트용 미니배치 데이터\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t) # 5. 테스트 데이터의 인식 정확도\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "            \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72042dff",
   "metadata": {},
   "source": [
    "에포크가 진행됨에 따라 손실이 낮아지고 인식 정확도는 상승함  \n",
    "훈련과 테스트의 차이가 작으므로 모델이 과적합을 일으키지 않은 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44992b01",
   "metadata": {},
   "source": [
    "# 51단계 MNIST 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54acc8",
   "metadata": {},
   "source": [
    "MNIST 데이터셋은 0\\~9까지 숫자를 나타내는 손글씨 이미지 데이터로, \n",
    "입력 데이터 형상은 (1, 28, 28), 즉 1채널(그레이스케일)의 28$\\times$28 픽셀 이미지 데이터이며,\n",
    "레이블에는 정답 숫자의 인덱스(0\\~9)가 들어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3e7a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: train-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: train-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: t10k-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: t10k-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=None)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423627aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 28, 28)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d5943",
   "metadata": {},
   "source": [
    "입력 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafdebcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 예시\n",
    "x, t = train_set[0] # 0번째(data, label) 추출\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('label:',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5142108",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad4e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x.flatten() # 입력 데이터를 1열로 나열(평탄화) -> (784,) 형상\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0 # 0.0 ~ 1.0 사이 부동소수점\n",
    "    return x\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=f)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adfe69",
   "metadata": {},
   "source": [
    "MNIST 클래스는 위 전처리가 기본으로 설정되어 있어 호출하면 자동으로 수행됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a89c2b",
   "metadata": {},
   "source": [
    "MNIST 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefba0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.9288, accuracy: 0.5452\n",
      "test loss: 1.5528, accuracy: 0.7374\n",
      "epoch: 2\n",
      "train loss: 1.2940, accuracy: 0.7671\n",
      "test loss: 1.0518, accuracy: 0.8122\n",
      "epoch: 3\n",
      "train loss: 0.9306, accuracy: 0.8143\n",
      "test loss: 0.7999, accuracy: 0.8352\n",
      "epoch: 4\n",
      "train loss: 0.7443, accuracy: 0.8384\n",
      "test loss: 0.6604, accuracy: 0.8527\n",
      "epoch: 5\n",
      "train loss: 0.6383, accuracy: 0.8521\n",
      "test loss: 0.5795, accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import Variable\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "\n",
    "\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    \n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "        \n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "            \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c7b14",
   "metadata": {},
   "source": [
    "모델 개선  \n",
    "  \n",
    "1. 활성화 함수 : 시그모이드 함수 $\\rightarrow$ ReLU 함수  \n",
    "  \n",
    "    여기서, ReLU(rectified linear unit) 함수는 입력이 0보다 크면 그대로 출력하고, 0 이하면 0을 출력하는 함수이며,  역전파에서는 순전파 시 신호를 통과시킨 원소라면 해당 역전파 시 기울기를 그대로 통과시키고, 순전파 시 신호가 막힌 원소(0 이하)는 기울기도 0이 됨  \n",
    "    $h(x)= \\Bigg\\{ \\begin{matrix} x (x>0)\\\\\n",
    "    0 (x\\geq0) \\end{matrix}$  \n",
    "  \n",
    "~~~python\n",
    "class ReLU(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.maximum(x, 0.0) \n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = x.data > 0\n",
    "        gx = gy * mask\n",
    "        return gx\n",
    "    \n",
    "def relu(x):\n",
    "    return ReLU()(x)\n",
    "~~~\n",
    "  \n",
    "2. 2층 신경망 $\\rightarrow$ 3층 신경망\n",
    "3. 최적화 기법 : SGD $\\rightarrow$ Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad89184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 0.1916, accuracy: 0.9426\n",
      "test loss: 0.1017, accuracy: 0.9686\n",
      "epoch: 2\n",
      "train loss: 0.0744, accuracy: 0.9766\n",
      "test loss: 0.0825, accuracy: 0.9730\n",
      "epoch: 3\n",
      "train loss: 0.0477, accuracy: 0.9843\n",
      "test loss: 0.0813, accuracy: 0.9755\n",
      "epoch: 4\n",
      "train loss: 0.0354, accuracy: 0.9886\n",
      "test loss: 0.0659, accuracy: 0.9813\n",
      "epoch: 5\n",
      "train loss: 0.0275, accuracy: 0.9912\n",
      "test loss: 0.0740, accuracy: 0.9806\n",
      "epoch: 6\n",
      "train loss: 0.0259, accuracy: 0.9911\n",
      "test loss: 0.0772, accuracy: 0.9808\n",
      "epoch: 7\n",
      "train loss: 0.0217, accuracy: 0.9930\n",
      "test loss: 0.0741, accuracy: 0.9813\n",
      "epoch: 8\n",
      "train loss: 0.0166, accuracy: 0.9948\n",
      "test loss: 0.0737, accuracy: 0.9807\n",
      "epoch: 9\n",
      "train loss: 0.0168, accuracy: 0.9945\n",
      "test loss: 0.0928, accuracy: 0.9792\n",
      "epoch: 10\n",
      "train loss: 0.0146, accuracy: 0.9953\n",
      "test loss: 0.0805, accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import Variable\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "\n",
    "\n",
    "max_epoch = 10\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "optimizer = optimizers.Adam().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    \n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "        \n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "            \n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db377b3",
   "metadata": {},
   "source": [
    "# APPENDIX B get_item 함수 구현(47단계 보충)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a83cb8",
   "metadata": {},
   "source": [
    "GetItem 클래스와 get_item 함수 코드  \n",
    "  \n",
    "forward(x) 메서드에서 단순히 원소를 추출함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef95df8",
   "metadata": {},
   "source": [
    "~~~python \n",
    "class GetItem(Function):\n",
    "    def __init__(self, slices):\n",
    "        self.slices = slices\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x[self.slices]\n",
    "        return y\n",
    "    \n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        f = GetItemGrad(self.slices, x.shape)\n",
    "        return f(gy)\n",
    "    \n",
    "def get_item(x, slices):\n",
    "    return GetItem(slices)(x)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94173f78",
   "metadata": {},
   "source": [
    "역전파 계산을 위해 GetItemGrad 클래스 구현  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e40432",
   "metadata": {},
   "source": [
    "~~~python\n",
    "class GetItemGrad(Function):\n",
    "    def __init__(self, slices, in_shape):\n",
    "        self.slices = slices\n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "    def forward(self, gy):\n",
    "        gx = np.zeros(self.in_shape)\n",
    "        np.add.at(gx, self.slices, gy)\n",
    "        return gx\n",
    "    \n",
    "    def backward(self, ggx):\n",
    "        return get_item(ggx, self.slices)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80c829",
   "metadata": {},
   "source": [
    "다차원 배열을 슬라이스하여 여러 원소가 한 번에 추출된 경우 역전파 시 대응하는 기울기를 더해줘야 함  \n",
    "np.add.at 함수로 기울기를 더해줌  \n",
    "np.add.at 사용 예시는 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96d69ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06775504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.ones(3,)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824ef438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slices = 1\n",
    "np.add.at(a, slices, b)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
