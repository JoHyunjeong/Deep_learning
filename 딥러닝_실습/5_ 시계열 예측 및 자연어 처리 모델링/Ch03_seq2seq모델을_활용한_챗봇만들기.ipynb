{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "[문제]_seq2seq모델을_활용한_챗봇_만들기_원본+수정본.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoHyunjeong/Deep_learning/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%A4%EC%8A%B5/5_%20%EC%8B%9C%EA%B3%84%EC%97%B4%20%EC%98%88%EC%B8%A1%20%EB%B0%8F%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%20%EB%AA%A8%EB%8D%B8%EB%A7%81/Ch03_seq2seq%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%E1%84%8B%E1%85%B3%E1%86%AF_%E1%84%92%E1%85%AA%E1%86%AF%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB_%E1%84%8E%E1%85%A2%E1%86%BA%E1%84%87%E1%85%A9%E1%86%BA%E1%84%86%E1%85%A1%E1%86%AB%E1%84%83%E1%85%B3%E1%86%AF%E1%84%80%E1%85%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ggNKq4cdxDr"
      },
      "source": [
        "# Seq2Seq 모델을 활용한 챗봇 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iay72EBedxDs"
      },
      "source": [
        "## STEP 1. Seq2Seq 모델의 개요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5OoHUgCgw2F"
      },
      "source": [
        "### 문제 01. seq2seq 모델의 구조에 대한 이해\n",
        "\n",
        "- encoder & decoder 구조\n",
        "- 데이터 셋의 구성\n",
        "- context vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTMGVydFdxDs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "64370bb3-d3e9-4e5c-831d-ba7d28bfd96d"
      },
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBwHP5uOdxDs"
      },
      "source": [
        "### 문제 02. 데이터셋에 필요한 라이브러리를 다운로드 받습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft3-FND8dxDt"
      },
      "source": [
        "`Korpora`는 한글 자연어처리 데이터셋입니다.\n",
        "\n",
        "- [깃헙 주소 링크](https://github.com/ko-nlp/Korpora)\n",
        "- [공식 도큐먼트](https://pypi.org/project/Korpora/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4abEgfIdxDt"
      },
      "source": [
        "설치 명령어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2riYkfBdxDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080d5668-4050-40dd-d0b7-2d25865ebea2"
      },
      "source": [
        "!pip install Korpora"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (1.21.5)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMa5kXSrdxDt"
      },
      "source": [
        "- 이 중 챗봇용 데이터셋인 `KoreanChatbotKorpus`를 다운로드 받습니다.\n",
        "- `KoreanChatbotKorpus` 데이터셋을 활용하여 챗봇 모델을 학습합니다.\n",
        "- text, pair로 구성되어 있습니다.\n",
        "- 질의는 **text**, 답변은 **pair**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPdBMDJlwHOS"
      },
      "source": [
        "### 문제 03. Korpora의 챗봇 데이터를 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ1yROOodxDu"
      },
      "source": [
        "# from Korpora import KoreanChatbotKorpus\n",
        "# corpus = KoreanChatbotKorpus() # HTTPError"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5opBY8yrdxDu"
      },
      "source": [
        "예시 텍스트를 보면 구어체로 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3XvQkDCNK-H"
      },
      "source": [
        "# corpus.get_all_pairs() # 답변"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVPNlrCbMt6J"
      },
      "source": [
        "# corpus.get_all_texts() # 질문"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F4CO4GnOLS0"
      },
      "source": [
        "데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koQmrUxOPOmf"
      },
      "source": [
        "# corpus.get_all_texts()[:5] # 질문"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwscPmwgdxDu"
      },
      "source": [
        "# corpus.get_all_pairs()[:5] # 답변"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n",
        "corpus.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wA1SeH7Uwna-",
        "outputId": "5ebe1b03-a770-49a8-afd9-9ce7087b074f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8240d112-540a-4932-a39c-cff9a3ef8fdc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8240d112-540a-4932-a39c-cff9a3ef8fdc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8240d112-540a-4932-a39c-cff9a3ef8fdc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8240d112-540a-4932-a39c-cff9a3ef8fdc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.Q[:5] # 질문"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kne9gDIKwolV",
        "outputId": "06f20114-8dc4-4d5d-bb49-5a63c870a9ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             12시 땡!\n",
              "1        1지망 학교 떨어졌어\n",
              "2       3박4일 놀러가고 싶다\n",
              "3    3박4일 정도 놀러가고 싶다\n",
              "4            PPL 심하네\n",
              "Name: Q, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.A[:5] # 답변"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkwiQQrmwsXz",
        "outputId": "39d8a775-1856-4745-a0df-c244f31e9c5c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     하루가 또 가네요.\n",
              "1      위로해 드립니다.\n",
              "2    여행은 언제나 좋죠.\n",
              "3    여행은 언제나 좋죠.\n",
              "4     눈살이 찌푸려지죠.\n",
              "Name: A, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qkxF9uEdxDv"
      },
      "source": [
        "## STEP 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ri7fDUwO8c"
      },
      "source": [
        "### 문제 04. quenstion 데이터셋과 answer 데이터셋 분리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKWVS48VdxDv"
      },
      "source": [
        "**question**과 **answer**를 분리합니다.\n",
        "\n",
        "**question**은 질의로 활용될 데이터셋, **answer**는 답변으로 활용될 데이터 셋입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yne75jOgdxDv"
      },
      "source": [
        "# texts = []\n",
        "# pairs = []\n",
        "\n",
        "# for text, pair in zip(corpus.get_all_texts(), corpus.get_all_pairs()):\n",
        "#   texts.append(text)\n",
        "#   pairs.append(pairs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "pairs = []\n",
        "for i, (text, pair) in enumerate(zip(corpus['Q'], corpus['A'])):\n",
        "    texts.append(text)\n",
        "    pairs.append(pair)"
      ],
      "metadata": {
        "id": "rhwJQfJfwxrH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAKrBxD1dxDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d51fb8a-4711-4c5f-a6a3-4e7b8362ec55"
      },
      "source": [
        "list(zip(texts, pairs))[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('12시 땡!', '하루가 또 가네요.'),\n",
              " ('1지망 학교 떨어졌어', '위로해 드립니다.'),\n",
              " ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('PPL 심하네', '눈살이 찌푸려지죠.')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMqdaHg4dxDv"
      },
      "source": [
        "### 문제 05. 특수문자는 제거합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXxfmSf_dxDw"
      },
      "source": [
        "**한글과 숫자를 제외한 특수문자를 제거**하도록 합니다. (선택사항)\n",
        "\n",
        "\n",
        "[참고] 튜토리얼에서는 특수문자와 영문자를 제거하나, 실제 프로젝트에 적용해보기 위해서는 신중히 결정해야합니다\n",
        "\n",
        "*챗봇 대화에서 영어도 많이 사용되고, 특수문자도 굉장히 많이 사용됩니다. 따라서, 선택적으로 제거할 특수기호나 영문자를 정의한 후에 전처리를 진행하야합니다.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQGtKQL3dxDw"
      },
      "source": [
        "# re 모듈은 regex expression을 적용하기 위하여 활용합니다.\n",
        "import re"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ociO_uPCdxDw"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
        "    # 코드를 입력하세요\n",
        "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n",
        "    return sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viW_yTWYdxDw"
      },
      "source": [
        "**적용한 예시**\n",
        "\n",
        "한글, 숫자 이외의 모든 문자를 전부 제거됨을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8addNjEkdxDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6cb51599-4c7a-42f5-ee9d-d19580816718"
      },
      "source": [
        "clean_sentence('12시 땡^^!??')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12시 땡'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luGFOsVMdxDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "125a820c-3746-4acd-da32-4f4055432f02"
      },
      "source": [
        "clean_sentence('abcef가나다^^$%@12시 땡^^!??')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'가나다12시 땡'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TYCMUC5dxDx"
      },
      "source": [
        "### 문제 06. 한글 형태소 분석기 (Konlpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5jA0mZEdxDx"
      },
      "source": [
        "형태소 분석기를 활용하여 문장을 분리합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSMeXN3dxDx"
      },
      "source": [
        "```가방에 들어가신다 -> 가방/NNG + 에/JKM + 들어가/VV + 시/EPH + ㄴ다/EFN```\n",
        "\n",
        "- **형태소 분석** 이란 형태소를 비롯하여, 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 것입니다.\n",
        "- **konlpy 형태소 분석기를 활용**하여 한글 문장에 대한 토큰화처리를 보다 효율적으로 처리합니다.\n",
        "\n",
        "\n",
        "\n",
        "[공식 도큐먼트](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az7bZjTmdxDx"
      },
      "source": [
        "**설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TvyZSXdxDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f9642d-3ec7-4bd5-bc99-106a0b5a5c3e"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64h11Yq2dxDx"
      },
      "source": [
        "konlpy 내부에는 Kkma, Okt, Twitter 등등의 형태소 분석기가 존재하지만, 이번 튜토리얼에서는 Okt를 활용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otm0rw1HdxDy"
      },
      "source": [
        "from konlpy.tag import Okt"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YQyXtGNdxDy"
      },
      "source": [
        "# Okt 토크나이저 객체를 생성합니다.\n",
        "# 코드를 입력하세요\n",
        "okt = Okt()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iab8xa-PdxDy"
      },
      "source": [
        "# 형태소 변환에 활용하는 함수\n",
        "# morphs 함수 안에 변환한 한글 문장을 입력 합니다.\n",
        "def process_morph(sentence):\n",
        "    return ' '.join(okt.morphs(sentence))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_morph('한글은 홀소리와 닿소리 모두 소리틀을 본떠 만든 음소문자[1]로 한글 맞춤법에서는 닿소리 14개와 홀소리 10개, 모두 24개를 표준으로 삼는다. \"나랏말이 중국과 달라\" 문제를 느낀 조선의 세종대왕이 한국어는 물론 이웃나라 말까지 나타내도록 1443년 창제하여 1446년 반포하였다. ')"
      ],
      "metadata": {
        "id": "K18f9Lb3UgzO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "38241449-f12a-4d9b-8f0c-88f480bf5e25"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'한글 은 홀소리 와 닿소리 모두 소 리틀 을 본떠 만든 음소문자 [ 1 ] 로 한글 맞춤법 에서는 닿소리 14 개 와 홀소리 10 개 , 모두 24 개 를 표준 으로 삼는다 . \" 나랏말 이 중국 과 달라 \" 문제 를 느낀 조선 의 세종대왕 이 한국어 는 물론 이웃 나라 말 까지 나타내도록 1443년 창제 하여 1446년 반포 하였다 .'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJiMmY55w7VK"
      },
      "source": [
        "## STEP 3. 데이터셋 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TSofap7xBuz"
      },
      "source": [
        "### 문제 07. Seq2Seq 학습을 위한 데이터셋 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJ9TiNldxDy"
      },
      "source": [
        "**Seq2Seq** 모델이 학습하기 위한 데이터셋을 구성할 때, 다음과 같이 **3가지 데이터셋**을 구성합니다.\n",
        "\n",
        "- `question`: encoder input 데이터셋 (질의 전체)\n",
        "- `answer_input`: decoder input 데이터셋 (답변의 시작). START 토큰을 문장 처음에 추가 합니다.\n",
        "- `answer_output`: decoder output 데이터셋 (답변의 끝). END 토큰을 문장 마지막에 추가 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFV7yQggdxDy"
      },
      "source": [
        "def clean_and_morph(sentence, is_question=True):\n",
        "    # 한글 문장 전처리\n",
        "    sentence = clean_sentence(sentence)\n",
        "    \n",
        "    # 형태소 변환\n",
        "    sentence = process_morph(sentence)\n",
        "    \n",
        "    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n",
        "    if is_question:\n",
        "        return sentence\n",
        "    else:\n",
        "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
        "        return ('<START> ' + sentence, sentence + ' <END>')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S_SjPQ5xOy6"
      },
      "source": [
        "### 문제 08. preprocess 함수에서는 `text`와 `pair`에 대한 데이터 셋 구성을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgnFV1AdxDz"
      },
      "source": [
        "def preprocess(texts, pairs):\n",
        "    questions = [] # encoder 입력\n",
        "    answer_in = [] # decoder 입력\n",
        "    answer_out = [] # decoder 출력(답)\n",
        "\n",
        "    # 질의에 대한 전처리\n",
        "    for text in texts:\n",
        "        # 전처리와 morph 수행\n",
        "        question = clean_and_morph(text, is_question = True)\n",
        "        questions.append(question)\n",
        "\n",
        "    # 답변에 대한 전처리\n",
        "    for pair in pairs:\n",
        "        # 전처리와 morph 수행\n",
        "        # 코드를 입력하세요\n",
        "        in_, out_ = clean_and_morph(pair, is_question = False)\n",
        "        answer_in.append(in_)\n",
        "        answer_out.append(out_)\n",
        "    \n",
        "    return questions, answer_in, answer_out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPQzKJnqdxDz"
      },
      "source": [
        "# questions, answer_in, answer_out = preprocess(texts, pairs)\n",
        "\n",
        "# 15번에서 RAM 초과 에러 시 -> data 2000개 씩 사용으로 수정\n",
        "texts = texts[:2000]\n",
        "pairs = pairs[:2000]\n",
        "questions, answer_in, answer_out = preprocess(texts, pairs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1v4qThdxDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f7ae48-1c85-4f43-8acc-10cf6d3109c1"
      },
      "source": [
        "questions[:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', '심하네']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P-wlhGPdxDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81e118c-b77e-4611-85fa-5a6e059cd4f6"
      },
      "source": [
        "answer_in[:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> 하루 가 또 가네요',\n",
              " '<START> 위로 해 드립니다',\n",
              " '<START> 여행 은 언제나 좋죠',\n",
              " '<START> 여행 은 언제나 좋죠',\n",
              " '<START> 눈살 이 찌푸려지죠']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAiUkA9OdxDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978a3aff-8a2e-4548-cf45-b04db9ad49c3"
      },
      "source": [
        "answer_out[:5]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하루 가 또 가네요 <END>',\n",
              " '위로 해 드립니다 <END>',\n",
              " '여행 은 언제나 좋죠 <END>',\n",
              " '여행 은 언제나 좋죠 <END>',\n",
              " '눈살 이 찌푸려지죠 <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx6G3-32xc6w"
      },
      "source": [
        "### 문제 09. `all_sentences` 변수에 모든 데이터셋 문장을 합칩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc3aTpP_xja-"
      },
      "source": [
        "합치는 이유는 모든 문장을 합쳐서 토큰화를 진행하기 위함입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM0Z-lEAdxDz"
      },
      "source": [
        "all_sentences = questions + answer_in + answer_out"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6RirVQPdxDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba111335-961d-4148-ea6a-42cc74007db3"
      },
      "source": [
        "a = (' '.join(questions) + ' '.join(answer_in) + ' '.join(answer_out)).split()\n",
        "len(set(a)) # 고유 단어만 남김"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3605"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9VrosgvdxD0"
      },
      "source": [
        "## STEP 4. 토큰화 (Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxgX7BzSdxD0"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# WARNING 무시\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imZynPXTx7KX"
      },
      "source": [
        "### 문제 10. 토근의 option을 정의 합니다.\n",
        "\n",
        "- filter는 ''로 지정합니다.(빈 문자열) -> 빈 문자열 기준으로 토큰화  \n",
        "- lower는 False로 지정합니다.\n",
        "- oov_token(out of vocab token)은 '\\<OOV>'로 지정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpHQZgXNdxD0"
      },
      "source": [
        "**토큰의 정의**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T64bShSydxD0"
      },
      "source": [
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXX-jHRdyOP5"
      },
      "source": [
        "### 문제 11. 단어 사전을 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SglFhzjdxD0"
      },
      "source": [
        "**Tokenizer**로 문장에 대한 Word-Index Vocabulary(단어 사전)을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jm3j8YadxD0"
      },
      "source": [
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ5t4QKydxD0"
      },
      "source": [
        "**단어 사전 10개 출력**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9e3ZjBudxD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26df6a3-1445-4fbd-bdc1-14b11ab03274"
      },
      "source": [
        "for word, idx in tokenizer.word_index.items():\n",
        "    print(f'{word}\\t\\t => \\t{idx}')\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<OOV>\t\t => \t1\n",
            "<START>\t\t => \t2\n",
            "<END>\t\t => \t3\n",
            "이\t\t => \t4\n",
            "을\t\t => \t5\n",
            "거\t\t => \t6\n",
            "가\t\t => \t7\n",
            "예요\t\t => \t8\n",
            "도\t\t => \t9\n",
            "해보세요\t\t => \t10\n",
            "요\t\t => \t11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKRMzy-y0GA8"
      },
      "source": [
        "### 문제 12. 토큰의 갯수 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auMa_FfzdxD1"
      },
      "source": [
        "**토큰의 갯수 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7Rfl3AdxD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d90eb43-4050-4a94-88b6-cc22e5b0b93c"
      },
      "source": [
        "print(len(tokenizer.word_index))\n",
        "# VOCAB_SIZE 변수에 토큰의 단어 사전의 갯수를 입력합니다.\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1 # padding을 추가할 예정이기 때문에 1 더해줌"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybduw-HedxD1"
      },
      "source": [
        "### 문제 13. 치환: 텍스트를 시퀀스로 인코딩 (`texts_to_sequences`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV0p7p7VdxD1"
      },
      "source": [
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[0]"
      ],
      "metadata": {
        "id": "Ko8Cl_Zec4-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "19a62345-ff90-4696-c33c-bba7659aa8ac"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12시 땡'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_sequence[0]"
      ],
      "metadata": {
        "id": "OYik1fWec9v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c35049-b380-481a-8b77-dfa2b7f21a7f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1757, 2494]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in[0]"
      ],
      "metadata": {
        "id": "ym8KEKPzdANa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e502c834-fc20-4d70-b26e-32a64a4ad2b1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<START> 하루 가 또 가네요'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_sequence[0]"
      ],
      "metadata": {
        "id": "MMvAEt5gc-Gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70b20f9-1853-47a3-8eed-4008421785c7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 366, 7, 128, 1057]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index['하루']"
      ],
      "metadata": {
        "id": "mMxiLK0IdHhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ea114e-55b4-450d-d107-c3088bff871a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y1HNPGndxD1"
      },
      "source": [
        "### 문제 14. 문장의 길이 맞추기 (`pad_sequences`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp3eupZgyliG"
      },
      "source": [
        "`pad_sequences`를 활용하여 문장의 길이를 맞춰 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdhUgUc_dxD1"
      },
      "source": [
        "# 최대 문자의 토큰 갯수는 30개로 지정합니다.\n",
        "MAX_LENGTH = 30 # 한 문장에 들어가는 최대 단어 개수\n",
        "TRUNCATING = 'post' # 문장의 뒷 부분을 잘라냄 'post' / 앞 부분은 'pre'\n",
        "PADDING = 'post' # 문장의 뒷 부분을 채워줌 'post' / 앞 부분은 'pre'"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1VwlFM3dxD2"
      },
      "source": [
        "question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_in_padded = pad_sequences(answer_in_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_out_padded = pad_sequences(answer_out_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL4SGOghy3gD"
      },
      "source": [
        "변환된 데이터 셋의 shape를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F01y9olHdxD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981fe5af-73ae-4c30-d2a4-4a38c3f0519b"
      },
      "source": [
        "question_padded.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0siuIXzdxD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3fec1f-5548-4566-c4cb-6c0782b95194"
      },
      "source": [
        "answer_in_padded.shape, answer_out_padded.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 30), (2000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_padded[0]"
      ],
      "metadata": {
        "id": "akvA7ScNeyus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296e26a1-2b24-4c69-898b-8ab157e94bac"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1757, 2494,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_padded[0]"
      ],
      "metadata": {
        "id": "3rGXZS5Ce0Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811bb11a-0455-498a-de93-0dfbdce5d77f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,  366,    7,  128, 1057,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_out_padded[0]"
      ],
      "metadata": {
        "id": "XaEcrs0Ve2uD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c7ca98-6479-490a-f600-8d0e900a56b6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 366,    7,  128, 1057,    3,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMY1xxFa0hUg"
      },
      "source": [
        "## STEP 5. 데이터셋 변환 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2MLohkdxD4"
      },
      "source": [
        "### 문제 15. 단어별 원핫인코딩 적용\n",
        "\n",
        "단어별 원핫인코딩을 적용하는 이유는 decoder의 output(출력)을 원핫인코딩 vector로 변환하기 위함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe6My-e6dxD4"
      },
      "source": [
        "def convert_to_one_hot(padded):\n",
        "    # 원핫인코딩 초기화\n",
        "    one_hot_vector = np.zeros((len(padded), MAX_LENGTH, VOCAB_SIZE)) # (문장 수, 한 문장의 단어 수(=30), 단어사전의 단어 수)\n",
        "\n",
        "    # 디코더 목표를 원핫인코딩으로 변환\n",
        "    # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
        "    for i, sequence in enumerate(padded):\n",
        "      for j, index in enumerate(sequence):\n",
        "        one_hot_vector[i, j, index] = 1\n",
        "        \n",
        "    return one_hot_vector"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWU_9by8dxD4"
      },
      "source": [
        "# RAM 용량이 초과하는 경우 세션이 다운될 수 있습니다.\n",
        "# 그럴때는 학습 세트의 전체 사이즈를 줄여주세요.\n",
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)"
      ],
      "metadata": {
        "id": "i6VnVEpEymkN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FozfIPBTdxD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ee197d-e7ea-418f-b45b-26bfc9e7a5d6"
      },
      "source": [
        "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 3605), (30, 3605))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FpX53_MdxD5"
      },
      "source": [
        "### 문제 16. 변환된 index를 다시 단어로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Djs6RsadxD5"
      },
      "source": [
        "def convert_index_to_text(indexs, end_token): \n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == end_token:\n",
        "            # 끝 단어이므로 예측 중지\n",
        "            break;\n",
        "        # 사전에 존재하는 단어의 경우 단어 추가\n",
        "        if index > 0 and tokenizer.index_word[index] is not None:\n",
        "            sentence += tokenizer.index_word[index]\n",
        "        else:\n",
        "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
        "            sentence += ''\n",
        "            \n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "    return sentence"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf_g3T1RdxD2"
      },
      "source": [
        "## STEP 6. 모델 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kY98vN5zCkl"
      },
      "source": [
        "모델 생성을 위한 필요한 모듈을 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A7JPC6OdxD2"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3XQHILadxD2"
      },
      "source": [
        "### 문제 17. 학습용 인코더 (Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_UTWGaodxD2"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2) # 과적합 방지 용도\n",
        "        self.lstm = LSTM(units, return_state=True) # return_state -> context 벡터 만들 때 사용 / state는 hidden state, cell state 두 가지\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "\n",
        "        return [hidden_state, cell_state] # context vector 리턴"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4hFzCh-dxD3"
      },
      "source": [
        "### 문제 18. 학습용 디코더 (Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WW_fADJdxD3"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True,\n",
        "                         )\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, initial_state):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state) # initial_state는 encoder에서 만든 context vector\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x, hidden_state, cell_state"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmXc7Jw8dxD3"
      },
      "source": [
        "### 문제 19. Seq2Seq 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcGmmcY_zUx3"
      },
      "source": [
        "Seq2Seq 모델은 사전 단계에서 만들어놓은 encoder와 decoder를 활용하여 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUx38fcsdxD3"
      },
      "source": [
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps # 한 문장의 단어 길이\n",
        "        \n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        \n",
        "    def call(self, inputs, training=True): # training -> True : 학습 / False : 예측\n",
        "        if training: # 학습\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            context_vector = self.encoder(encoder_inputs)\n",
        "            decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector)\n",
        "            return decoder_outputs\n",
        "\n",
        "        else: # 예측\n",
        "            context_vector = self.encoder(inputs)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32) # 한 단어씩 예측 / decoder 입력 첫 단어는 start_token\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps) # 빈 그릇\n",
        "            \n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32) # index로 변환 / onehot -> sparse\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output) # 결과 추가\n",
        "                \n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "                    \n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "                \n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFULTBxcdxD5"
      },
      "source": [
        "## STEP 7. 학습 (Training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTFXrO2T06f4"
      },
      "source": [
        "### 문제 20. 학습을 위한 parameter를 정의하고 체크포인트를 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FzbsGvdxD6"
      },
      "source": [
        "**하이퍼 파라미터 정의**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noiy8uJ-dxD6"
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16 # 하드웨어 여건에 따라 조절\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "\n",
        "# 코드를 입력하세요\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "\n",
        "UNITS = 128 # lstm에 들어가는 unit 수\n",
        "\n",
        "# 코드를 입력하세요\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "DATA_LENGTH = len(questions) # 데이터 개수\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6jJEq-mdxD6"
      },
      "source": [
        "**체크포인트 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUMtSpCldxD6"
      },
      "source": [
        "checkpoint_path = './sample-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=True,\n",
        "                             monitor='loss', \n",
        "                             verbose=1\n",
        "                            )"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQRNjsiCdxD6"
      },
      "source": [
        "### 문제 21. 모델 생성 & compile을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccl3jF-w1eQe"
      },
      "source": [
        "optimizer는 `adam`, loss는 `categorical_crossentropy`로 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DSNFivkdxD7"
      },
      "source": [
        "# 코드를 입력하세요\n",
        "seq2seq = Seq2Seq(units=UNITS, vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, time_steps=TIME_STEPS, start_token=START_TOKEN, end_token=END_TOKEN)\n",
        "seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc']) # onehot -> categorical crossentropy 사용"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KepR0JJOdxD7"
      },
      "source": [
        "# (필요시) 연속하여 학습시 체크포인트를 로드하여 이어서 학습합니다.\n",
        "# seq2seq.load_weights(checkpoint_path)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GpyZ2SH1q_Q"
      },
      "source": [
        "### 문제 22. make_prediction 함수를 정의 합니다. 예측된 결과를 역 출력 해주기 위한 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YuyKYV3dxD7"
      },
      "source": [
        "def make_prediction(model, question_inputs):\n",
        "    results = model(inputs=question_inputs, training=False)\n",
        "    # 변환된 인덱스를 문장으로 변환\n",
        "    results = np.asarray(results).reshape(-1)\n",
        "    return results"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    # model.fit(): 학습\n",
        "    # 코드를 입력하세요\n",
        "    seq2seq.fit([question_padded, answer_in_padded], # inputs\n",
        "                answer_out_one_hot, # outputs\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "        \n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "        \n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJFZ8PAy3HNk",
        "outputId": "ca4d6421-87c1-4502-8b43-a03b41087122"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "122/125 [============================>.] - ETA: 0s - loss: 2.4456 - acc: 0.7982\n",
            "Epoch 1: loss improved from inf to 2.41877, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 7s 14ms/step - loss: 2.4188 - acc: 0.7983\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 1.2115 - acc: 0.8163\n",
            "Epoch 2: loss improved from 2.41877 to 1.20989, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 1.2099 - acc: 0.8166\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 1.1092 - acc: 0.8370\n",
            "Epoch 3: loss improved from 1.20989 to 1.11064, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 1.1106 - acc: 0.8368\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 1.0577 - acc: 0.8400\n",
            "Epoch 4: loss improved from 1.11064 to 1.05878, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 1.0588 - acc: 0.8398\n",
            "Epoch 5/10\n",
            "122/125 [============================>.] - ETA: 0s - loss: 1.0222 - acc: 0.8413\n",
            "Epoch 5: loss improved from 1.05878 to 1.02328, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 1.0233 - acc: 0.8411\n",
            "Epoch 6/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.8428\n",
            "Epoch 6: loss improved from 1.02328 to 0.99575, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.9957 - acc: 0.8430\n",
            "Epoch 7/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.8458\n",
            "Epoch 7: loss improved from 0.99575 to 0.97110, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.9711 - acc: 0.8459\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.9489 - acc: 0.8472\n",
            "Epoch 8: loss improved from 0.97110 to 0.94904, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.9490 - acc: 0.8472\n",
            "Epoch 9/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.8482\n",
            "Epoch 9: loss improved from 0.94904 to 0.92958, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.9296 - acc: 0.8482\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.9126 - acc: 0.8490\n",
            "Epoch 10: loss improved from 0.92958 to 0.91258, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.9126 - acc: 0.8490\n",
            "Q: 단발 어울릴까\n",
            "A: 저 을 더 있어요 \n",
            "\n",
            "\n",
            "Q: 덜 싸우는 법\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 가출 할까\n",
            "A: 저 을 더 더 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8958 - acc: 0.8506\n",
            "Epoch 1: loss improved from 0.91258 to 0.89548, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8955 - acc: 0.8505\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8784 - acc: 0.8521\n",
            "Epoch 2: loss improved from 0.89548 to 0.87922, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8792 - acc: 0.8521\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8592 - acc: 0.8544\n",
            "Epoch 3: loss improved from 0.87922 to 0.86243, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8624 - acc: 0.8539\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8461 - acc: 0.8552\n",
            "Epoch 4: loss improved from 0.86243 to 0.84735, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.8473 - acc: 0.8550\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8283 - acc: 0.8573\n",
            "Epoch 5: loss improved from 0.84735 to 0.82829, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8283 - acc: 0.8573\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.8103 - acc: 0.8596\n",
            "Epoch 6: loss improved from 0.82829 to 0.80930, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.8093 - acc: 0.8599\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.7917 - acc: 0.8615\n",
            "Epoch 7: loss improved from 0.80930 to 0.79054, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7905 - acc: 0.8620\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.7696 - acc: 0.8644\n",
            "Epoch 8: loss improved from 0.79054 to 0.76987, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7699 - acc: 0.8645\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.8680\n",
            "Epoch 9: loss improved from 0.76987 to 0.74901, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7490 - acc: 0.8675\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.7290 - acc: 0.8715\n",
            "Epoch 10: loss improved from 0.74901 to 0.72895, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7290 - acc: 0.8715\n",
            "Q: 나 실수 했나\n",
            "A: 저 도 인 사람 이 있어요 \n",
            "\n",
            "\n",
            "Q: 날씨 가 북극 같아\n",
            "A: 저 도 위로 싶어요 \n",
            "\n",
            "\n",
            "Q: 내 지인 한테 내 험담 했대\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.7072 - acc: 0.8747\n",
            "Epoch 1: loss improved from 0.72895 to 0.70885, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.7088 - acc: 0.8744\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.8759\n",
            "Epoch 2: loss improved from 0.70885 to 0.68943, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6894 - acc: 0.8765\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.8793\n",
            "Epoch 3: loss improved from 0.68943 to 0.67011, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6701 - acc: 0.8792\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.6519 - acc: 0.8815\n",
            "Epoch 4: loss improved from 0.67011 to 0.65133, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.6513 - acc: 0.8816\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.8837\n",
            "Epoch 5: loss improved from 0.65133 to 0.63215, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6321 - acc: 0.8840\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.6130 - acc: 0.8870\n",
            "Epoch 6: loss improved from 0.63215 to 0.61446, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.6145 - acc: 0.8866\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8894\n",
            "Epoch 7: loss improved from 0.61446 to 0.59686, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5969 - acc: 0.8895\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.5799 - acc: 0.8919\n",
            "Epoch 8: loss improved from 0.59686 to 0.57989, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5799 - acc: 0.8919\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.5635 - acc: 0.8947\n",
            "Epoch 9: loss improved from 0.57989 to 0.56347, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.5635 - acc: 0.8947\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8977\n",
            "Epoch 10: loss improved from 0.56347 to 0.54724, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5472 - acc: 0.8977\n",
            "Q: 라면 잘 끓이는 법\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 다른 학과 간다\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 내 가 나빴네\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.5318 - acc: 0.9006\n",
            "Epoch 1: loss improved from 0.54724 to 0.53178, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5318 - acc: 0.9006\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.9030\n",
            "Epoch 2: loss improved from 0.53178 to 0.51636, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5164 - acc: 0.9032\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.9064\n",
            "Epoch 3: loss improved from 0.51636 to 0.50223, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.5022 - acc: 0.9062\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.9092\n",
            "Epoch 4: loss improved from 0.50223 to 0.48825, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4883 - acc: 0.9094\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.9120\n",
            "Epoch 5: loss improved from 0.48825 to 0.47462, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4746 - acc: 0.9120\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4621 - acc: 0.9151\n",
            "Epoch 6: loss improved from 0.47462 to 0.46184, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4618 - acc: 0.9152\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.9177\n",
            "Epoch 7: loss improved from 0.46184 to 0.45021, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4502 - acc: 0.9175\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.9205\n",
            "Epoch 8: loss improved from 0.45021 to 0.43864, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4386 - acc: 0.9201\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.9227\n",
            "Epoch 9: loss improved from 0.43864 to 0.42742, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4274 - acc: 0.9223\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.4167 - acc: 0.9249\n",
            "Epoch 10: loss improved from 0.42742 to 0.41672, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.4167 - acc: 0.9249\n",
            "Q: 결혼 하면 행복해\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 물 차가워\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 정신차리게 말 해줘\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.9273\n",
            "Epoch 1: loss improved from 0.41672 to 0.40669, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4067 - acc: 0.9270\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.9290\n",
            "Epoch 2: loss improved from 0.40669 to 0.39651, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3965 - acc: 0.9288\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.9307\n",
            "Epoch 3: loss improved from 0.39651 to 0.38771, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.3877 - acc: 0.9308\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.9327\n",
            "Epoch 4: loss improved from 0.38771 to 0.37870, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3787 - acc: 0.9327\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.9341\n",
            "Epoch 5: loss improved from 0.37870 to 0.37051, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3705 - acc: 0.9338\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.9360\n",
            "Epoch 6: loss improved from 0.37051 to 0.36269, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3627 - acc: 0.9358\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.9370\n",
            "Epoch 7: loss improved from 0.36269 to 0.35474, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3547 - acc: 0.9371\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.9383\n",
            "Epoch 8: loss improved from 0.35474 to 0.34779, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3478 - acc: 0.9382\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.9398\n",
            "Epoch 9: loss improved from 0.34779 to 0.34118, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3412 - acc: 0.9398\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.9404\n",
            "Epoch 10: loss improved from 0.34118 to 0.33579, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3358 - acc: 0.9402\n",
            "Q: 과일 챙겨 먹어야지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 담배 너무 비쌈\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 뭐 입고 가지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.3297 - acc: 0.9410\n",
            "Epoch 1: loss improved from 0.33579 to 0.32969, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.3297 - acc: 0.9410\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9424\n",
            "Epoch 2: loss improved from 0.32969 to 0.32368, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3237 - acc: 0.9424\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.9438\n",
            "Epoch 3: loss improved from 0.32368 to 0.31793, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3179 - acc: 0.9436\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9439\n",
            "Epoch 4: loss improved from 0.31793 to 0.31324, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3132 - acc: 0.9439\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9445\n",
            "Epoch 5: loss improved from 0.31324 to 0.30853, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3085 - acc: 0.9444\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9458\n",
            "Epoch 6: loss improved from 0.30853 to 0.30389, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3039 - acc: 0.9456\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9461\n",
            "Epoch 7: loss improved from 0.30389 to 0.29926, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2993 - acc: 0.9460\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9472\n",
            "Epoch 8: loss improved from 0.29926 to 0.29540, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2954 - acc: 0.9474\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9475\n",
            "Epoch 9: loss improved from 0.29540 to 0.29112, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2911 - acc: 0.9475\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9481\n",
            "Epoch 10: loss improved from 0.29112 to 0.28799, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2880 - acc: 0.9483\n",
            "Q: 물이 안 나와\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 과거 는 잊고 앞 으로 나아 가야 지\n",
            "A: 축하 합니다 \n",
            "\n",
            "\n",
            "Q: 남자친구 가 너무 잘생겼어\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9488\n",
            "Epoch 1: loss improved from 0.28799 to 0.28471, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2847 - acc: 0.9488\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2810 - acc: 0.9489\n",
            "Epoch 2: loss improved from 0.28471 to 0.28100, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2810 - acc: 0.9489\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9497\n",
            "Epoch 3: loss improved from 0.28100 to 0.27725, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2772 - acc: 0.9497\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2745 - acc: 0.9499\n",
            "Epoch 4: loss improved from 0.27725 to 0.27446, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2745 - acc: 0.9499\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9507\n",
            "Epoch 5: loss improved from 0.27446 to 0.27285, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2729 - acc: 0.9505\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9505\n",
            "Epoch 6: loss improved from 0.27285 to 0.27060, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2706 - acc: 0.9506\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9509\n",
            "Epoch 7: loss improved from 0.27060 to 0.26682, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2668 - acc: 0.9510\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9521\n",
            "Epoch 8: loss improved from 0.26682 to 0.26433, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2643 - acc: 0.9520\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9515\n",
            "Epoch 9: loss improved from 0.26433 to 0.26255, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2625 - acc: 0.9513\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9522\n",
            "Epoch 10: loss improved from 0.26255 to 0.25975, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2598 - acc: 0.9521\n",
            "Q: 나도 중국 진출 해볼까\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 냄새 나면 어쩌지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 무기 력 한 것 일 뿐\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9527\n",
            "Epoch 1: loss improved from 0.25975 to 0.25658, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2566 - acc: 0.9529\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9527\n",
            "Epoch 2: loss improved from 0.25658 to 0.25514, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2551 - acc: 0.9527\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9531\n",
            "Epoch 3: loss improved from 0.25514 to 0.25256, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2526 - acc: 0.9531\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9531\n",
            "Epoch 4: loss improved from 0.25256 to 0.25121, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2512 - acc: 0.9531\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9537\n",
            "Epoch 5: loss improved from 0.25121 to 0.24843, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2484 - acc: 0.9536\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9538\n",
            "Epoch 6: loss improved from 0.24843 to 0.24729, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2473 - acc: 0.9537\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9540\n",
            "Epoch 7: loss improved from 0.24729 to 0.24543, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2454 - acc: 0.9540\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9537\n",
            "Epoch 8: loss did not improve from 0.24543\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2462 - acc: 0.9537\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9542\n",
            "Epoch 9: loss improved from 0.24543 to 0.24341, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2434 - acc: 0.9542\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9547\n",
            "Epoch 10: loss improved from 0.24341 to 0.24070, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2407 - acc: 0.9547\n",
            "Q: 마음 이 싱숭생숭해서 자꾸 여기 다 쓰게되넹\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 머리 좀 다듬을까\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 물 끓여서 차 마셔야지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9550\n",
            "Epoch 1: loss improved from 0.24070 to 0.23844, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2384 - acc: 0.9549\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9551\n",
            "Epoch 2: loss improved from 0.23844 to 0.23669, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2367 - acc: 0.9550\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2356 - acc: 0.9556\n",
            "Epoch 3: loss improved from 0.23669 to 0.23561, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2356 - acc: 0.9556\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9558\n",
            "Epoch 4: loss improved from 0.23561 to 0.23411, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2341 - acc: 0.9559\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9557\n",
            "Epoch 5: loss improved from 0.23411 to 0.23250, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2325 - acc: 0.9557\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9557\n",
            "Epoch 6: loss improved from 0.23250 to 0.23144, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2314 - acc: 0.9558\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9561\n",
            "Epoch 7: loss improved from 0.23144 to 0.23074, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2307 - acc: 0.9560\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9558\n",
            "Epoch 8: loss improved from 0.23074 to 0.23038, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2304 - acc: 0.9558\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9560\n",
            "Epoch 9: loss improved from 0.23038 to 0.22852, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2285 - acc: 0.9560\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2271 - acc: 0.9564\n",
            "Epoch 10: loss improved from 0.22852 to 0.22711, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2271 - acc: 0.9564\n",
            "Q: 눈 이 안 떠져\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 내 가 왜 해야하는지 모르겠어\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 금값 어때\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9567\n",
            "Epoch 1: loss improved from 0.22711 to 0.22589, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2259 - acc: 0.9567\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2246 - acc: 0.9567\n",
            "Epoch 2: loss improved from 0.22589 to 0.22463, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2246 - acc: 0.9567\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9571\n",
            "Epoch 3: loss improved from 0.22463 to 0.22329, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2233 - acc: 0.9571\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2224 - acc: 0.9570\n",
            "Epoch 4: loss improved from 0.22329 to 0.22243, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2224 - acc: 0.9570\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2229 - acc: 0.9571\n",
            "Epoch 5: loss did not improve from 0.22243\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2229 - acc: 0.9571\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9574\n",
            "Epoch 6: loss improved from 0.22243 to 0.22211, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2221 - acc: 0.9573\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9574\n",
            "Epoch 7: loss improved from 0.22211 to 0.21989, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2199 - acc: 0.9574\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9578\n",
            "Epoch 8: loss improved from 0.21989 to 0.21754, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2175 - acc: 0.9578\n",
            "Epoch 9/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9581\n",
            "Epoch 9: loss improved from 0.21754 to 0.21700, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2170 - acc: 0.9581\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2153 - acc: 0.9581\n",
            "Epoch 10: loss improved from 0.21700 to 0.21535, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2153 - acc: 0.9581\n",
            "Q: 밥 이 없어\n",
            "A: 저 도 부러워요 \n",
            "\n",
            "\n",
            "Q: 괜히 건 들 지 말 라고\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 노래방 가면 어색할까\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9584\n",
            "Epoch 1: loss improved from 0.21535 to 0.21452, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.2145 - acc: 0.9584\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9587\n",
            "Epoch 2: loss improved from 0.21452 to 0.21344, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2134 - acc: 0.9586\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2124 - acc: 0.9584\n",
            "Epoch 3: loss improved from 0.21344 to 0.21243, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2124 - acc: 0.9584\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2118 - acc: 0.9582\n",
            "Epoch 4: loss improved from 0.21243 to 0.21175, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2118 - acc: 0.9582\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2105 - acc: 0.9587\n",
            "Epoch 5: loss improved from 0.21175 to 0.21053, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2105 - acc: 0.9587\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9587\n",
            "Epoch 6: loss improved from 0.21053 to 0.21022, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2102 - acc: 0.9588\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2086 - acc: 0.9584\n",
            "Epoch 7: loss improved from 0.21022 to 0.20857, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2086 - acc: 0.9584\n",
            "Epoch 8/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.2073 - acc: 0.9589\n",
            "Epoch 8: loss improved from 0.20857 to 0.20724, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2072 - acc: 0.9589\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2065 - acc: 0.9588\n",
            "Epoch 9: loss improved from 0.20724 to 0.20650, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2065 - acc: 0.9588\n",
            "Epoch 10/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9588\n",
            "Epoch 10: loss improved from 0.20650 to 0.20484, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.2048 - acc: 0.9588\n",
            "Q: 나 미팅 한다\n",
            "A: 저 는 마음 을 이어주는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "Q: 그렇게 오래 살았는데도 이해 를 못 하겠어\n",
            "A: 저 는 마음 을 이어주는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "Q: 나 괜찮지 않니\n",
            "A: 저 는 마음 을 이어주는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9593\n",
            "Epoch 1: loss improved from 0.20484 to 0.20353, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2035 - acc: 0.9592\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9595\n",
            "Epoch 2: loss improved from 0.20353 to 0.20186, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2019 - acc: 0.9594\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.2010 - acc: 0.9598\n",
            "Epoch 3: loss improved from 0.20186 to 0.20098, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.2010 - acc: 0.9598\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9591\n",
            "Epoch 4: loss did not improve from 0.20098\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2028 - acc: 0.9591\n",
            "Epoch 5/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9595\n",
            "Epoch 5: loss improved from 0.20098 to 0.19787, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1979 - acc: 0.9595\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9597\n",
            "Epoch 6: loss improved from 0.19787 to 0.19562, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1956 - acc: 0.9597\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9601\n",
            "Epoch 7: loss improved from 0.19562 to 0.19381, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1938 - acc: 0.9602\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1913 - acc: 0.9602\n",
            "Epoch 8: loss improved from 0.19381 to 0.19135, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1913 - acc: 0.9602\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1898 - acc: 0.9600\n",
            "Epoch 9: loss improved from 0.19135 to 0.18980, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1898 - acc: 0.9600\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9607\n",
            "Epoch 10: loss improved from 0.18980 to 0.18843, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1884 - acc: 0.9607\n",
            "Q: 발표 무사히 끝내고 싶어\n",
            "A: 지금 도 그러고 있어요 \n",
            "\n",
            "\n",
            "Q: 가상 화폐 쫄딱 망함\n",
            "A: 지금 도 그러고 있어요 \n",
            "\n",
            "\n",
            "Q: 건너 건너 아는 사람 인데 연락 해도 될까\n",
            "A: 저 랑 한 잔 해 요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1870 - acc: 0.9606\n",
            "Epoch 1: loss improved from 0.18843 to 0.18703, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.1870 - acc: 0.9606\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9609\n",
            "Epoch 2: loss improved from 0.18703 to 0.18464, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1846 - acc: 0.9609\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9606\n",
            "Epoch 3: loss improved from 0.18464 to 0.18377, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1838 - acc: 0.9607\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9605\n",
            "Epoch 4: loss improved from 0.18377 to 0.18283, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1828 - acc: 0.9605\n",
            "Epoch 5/10\n",
            "122/125 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9610\n",
            "Epoch 5: loss improved from 0.18283 to 0.18108, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.1811 - acc: 0.9609\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9611\n",
            "Epoch 6: loss improved from 0.18108 to 0.18011, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.1801 - acc: 0.9609\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9610\n",
            "Epoch 7: loss improved from 0.18011 to 0.17971, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1797 - acc: 0.9608\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9597\n",
            "Epoch 8: loss did not improve from 0.17971\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1857 - acc: 0.9598\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9611\n",
            "Epoch 9: loss improved from 0.17971 to 0.17822, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1782 - acc: 0.9611\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9612\n",
            "Epoch 10: loss improved from 0.17822 to 0.17477, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1748 - acc: 0.9612\n",
            "Q: 너무 뻔 뻔하게 구 는 데\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "Q: 결혼 하면 좋아\n",
            "A: 지금 그러고 있어요 \n",
            "\n",
            "\n",
            "Q: 나 친구 들 한테 인정받고 싶어\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9614\n",
            "Epoch 1: loss improved from 0.17477 to 0.17313, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1731 - acc: 0.9614\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9620\n",
            "Epoch 2: loss improved from 0.17313 to 0.17086, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1709 - acc: 0.9620\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1685 - acc: 0.9624\n",
            "Epoch 3: loss improved from 0.17086 to 0.16847, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1685 - acc: 0.9624\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9621\n",
            "Epoch 4: loss improved from 0.16847 to 0.16752, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1675 - acc: 0.9622\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1659 - acc: 0.9625\n",
            "Epoch 5: loss improved from 0.16752 to 0.16586, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1659 - acc: 0.9625\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1642 - acc: 0.9630\n",
            "Epoch 6: loss improved from 0.16586 to 0.16424, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1642 - acc: 0.9630\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1633 - acc: 0.9626\n",
            "Epoch 7: loss improved from 0.16424 to 0.16335, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1633 - acc: 0.9626\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1633 - acc: 0.9625\n",
            "Epoch 8: loss improved from 0.16335 to 0.16331, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1633 - acc: 0.9625\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9631\n",
            "Epoch 9: loss improved from 0.16331 to 0.16025, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1602 - acc: 0.9630\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9635\n",
            "Epoch 10: loss improved from 0.16025 to 0.15827, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1583 - acc: 0.9635\n",
            "Q: 내 편이 없는 거 같아\n",
            "A: 저 도 좀 알려주세요 \n",
            "\n",
            "\n",
            "Q: 감정 컨트롤 이 안 돼\n",
            "A: 같은 하늘 아래 어딘가 에 \n",
            "\n",
            "\n",
            "Q: 나 잘 살 수 있겠지\n",
            "A: 잘 할 수 있을 거 예요 조금 만 더 힘내세요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1569 - acc: 0.9630\n",
            "Epoch 1: loss improved from 0.15827 to 0.15685, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1569 - acc: 0.9630\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9637\n",
            "Epoch 2: loss improved from 0.15685 to 0.15516, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1552 - acc: 0.9637\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9638\n",
            "Epoch 3: loss improved from 0.15516 to 0.15348, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1535 - acc: 0.9638\n",
            "Epoch 4/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9641\n",
            "Epoch 4: loss improved from 0.15348 to 0.15266, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1527 - acc: 0.9641\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9645\n",
            "Epoch 5: loss improved from 0.15266 to 0.15092, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1509 - acc: 0.9644\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1491 - acc: 0.9641\n",
            "Epoch 6: loss improved from 0.15092 to 0.14905, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.1491 - acc: 0.9641\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1479 - acc: 0.9646\n",
            "Epoch 7: loss improved from 0.14905 to 0.14794, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1479 - acc: 0.9646\n",
            "Epoch 8/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9651\n",
            "Epoch 8: loss improved from 0.14794 to 0.14622, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1462 - acc: 0.9651\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9649\n",
            "Epoch 9: loss improved from 0.14622 to 0.14572, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1457 - acc: 0.9649\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9654\n",
            "Epoch 10: loss improved from 0.14572 to 0.14365, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1437 - acc: 0.9653\n",
            "Q: 나 는 좋아하는 게 뭘 까\n",
            "A: 더 많이 연습 하고 준비 해보세요 \n",
            "\n",
            "\n",
            "Q: 고구마 다이어트 해야지\n",
            "A: 저 는 좋아요 \n",
            "\n",
            "\n",
            "Q: 만나자고 약속 잡는 것 도 귀찮아\n",
            "A: 병원 가세 요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9656\n",
            "Epoch 1: loss improved from 0.14365 to 0.14255, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.1425 - acc: 0.9655\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9659\n",
            "Epoch 2: loss improved from 0.14255 to 0.14044, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1404 - acc: 0.9659\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9660\n",
            "Epoch 3: loss improved from 0.14044 to 0.14010, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1401 - acc: 0.9659\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9658\n",
            "Epoch 4: loss improved from 0.14010 to 0.13905, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1390 - acc: 0.9657\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9663\n",
            "Epoch 5: loss improved from 0.13905 to 0.13714, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1371 - acc: 0.9663\n",
            "Epoch 6/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1399 - acc: 0.9660\n",
            "Epoch 6: loss did not improve from 0.13714\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1399 - acc: 0.9660\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1386 - acc: 0.9657\n",
            "Epoch 7: loss did not improve from 0.13714\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1386 - acc: 0.9657\n",
            "Epoch 8/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9669\n",
            "Epoch 8: loss improved from 0.13714 to 0.13359, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1336 - acc: 0.9668\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9672\n",
            "Epoch 9: loss improved from 0.13359 to 0.13250, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1325 - acc: 0.9670\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1304 - acc: 0.9672\n",
            "Epoch 10: loss improved from 0.13250 to 0.13042, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1304 - acc: 0.9672\n",
            "Q: 나중 에 뭐 할까 고민 이야\n",
            "A: 지금 도 멋져요 \n",
            "\n",
            "\n",
            "Q: 단순하다\n",
            "A: 요즘 예민한 가봐요 \n",
            "\n",
            "\n",
            "Q: 내 남자친구 아이돌 이면 좋겠다\n",
            "A: 그러게 말 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "123/125 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9675\n",
            "Epoch 1: loss improved from 0.13042 to 0.12919, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.1292 - acc: 0.9676\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1279 - acc: 0.9678\n",
            "Epoch 2: loss improved from 0.12919 to 0.12793, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1279 - acc: 0.9678\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9682\n",
            "Epoch 3: loss improved from 0.12793 to 0.12723, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1272 - acc: 0.9681\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9685\n",
            "Epoch 4: loss improved from 0.12723 to 0.12443, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1244 - acc: 0.9683\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9683\n",
            "Epoch 5: loss improved from 0.12443 to 0.12438, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1244 - acc: 0.9683\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9685\n",
            "Epoch 6: loss improved from 0.12438 to 0.12198, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1220 - acc: 0.9684\n",
            "Epoch 7/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9686\n",
            "Epoch 7: loss did not improve from 0.12198\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1245 - acc: 0.9686\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1250 - acc: 0.9686\n",
            "Epoch 8: loss did not improve from 0.12198\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1250 - acc: 0.9686\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9686\n",
            "Epoch 9: loss did not improve from 0.12198\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1261 - acc: 0.9685\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9692\n",
            "Epoch 10: loss improved from 0.12198 to 0.11896, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1190 - acc: 0.9691\n",
            "Q: 누구 랑 결혼 해야 할까\n",
            "A: 냉장고 파먹기 해보세요 \n",
            "\n",
            "\n",
            "Q: 꽃다발 샀어\n",
            "A: 얼른 끝내시길 기도 할게요 \n",
            "\n",
            "\n",
            "Q: 먼지 가 많나\n",
            "A: 자책 하지 마세요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9700\n",
            "Epoch 1: loss improved from 0.11896 to 0.11640, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1164 - acc: 0.9698\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1152 - acc: 0.9706\n",
            "Epoch 2: loss improved from 0.11640 to 0.11524, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1152 - acc: 0.9706\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1134 - acc: 0.9705\n",
            "Epoch 3: loss improved from 0.11524 to 0.11342, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.1134 - acc: 0.9705\n",
            "Epoch 4/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1105 - acc: 0.9712\n",
            "Epoch 4: loss improved from 0.11342 to 0.11047, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1105 - acc: 0.9712\n",
            "Epoch 5/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.1089 - acc: 0.9712\n",
            "Epoch 5: loss improved from 0.11047 to 0.10886, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1089 - acc: 0.9712\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9718\n",
            "Epoch 6: loss improved from 0.10886 to 0.10728, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1073 - acc: 0.9718\n",
            "Epoch 7/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9722\n",
            "Epoch 7: loss improved from 0.10728 to 0.10420, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1042 - acc: 0.9721\n",
            "Epoch 8/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9725\n",
            "Epoch 8: loss improved from 0.10420 to 0.10317, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1032 - acc: 0.9725\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9727\n",
            "Epoch 9: loss improved from 0.10317 to 0.10257, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1026 - acc: 0.9728\n",
            "Epoch 10/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9732\n",
            "Epoch 10: loss improved from 0.10257 to 0.09964, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0996 - acc: 0.9732\n",
            "Q: 나 많이 기대했는데\n",
            "A: 놀이동산 은 다 좋아할 거 예요 \n",
            "\n",
            "\n",
            "Q: 나중 에 뭐 하고 먹고 사냐\n",
            "A: 진짜 하고 싶은 걸 찾아보세요 \n",
            "\n",
            "\n",
            "Q: 고백 하고 후회 하면 어떡하지\n",
            "A: 후회 는 후회 를 낳을 뿐 이에요 용기 내세 요 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0971 - acc: 0.9739\n",
            "Epoch 1: loss improved from 0.09964 to 0.09708, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 12ms/step - loss: 0.0971 - acc: 0.9739\n",
            "Epoch 2/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9745\n",
            "Epoch 2: loss improved from 0.09708 to 0.09576, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0958 - acc: 0.9745\n",
            "Epoch 3/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9751\n",
            "Epoch 3: loss improved from 0.09576 to 0.09340, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0934 - acc: 0.9750\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9756\n",
            "Epoch 4: loss improved from 0.09340 to 0.09198, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0920 - acc: 0.9757\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9759\n",
            "Epoch 5: loss improved from 0.09198 to 0.08980, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0898 - acc: 0.9760\n",
            "Epoch 6/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9761\n",
            "Epoch 6: loss improved from 0.08980 to 0.08826, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0883 - acc: 0.9761\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0870 - acc: 0.9760\n",
            "Epoch 7: loss improved from 0.08826 to 0.08695, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0870 - acc: 0.9760\n",
            "Epoch 8/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0853 - acc: 0.9766\n",
            "Epoch 8: loss improved from 0.08695 to 0.08534, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0853 - acc: 0.9766\n",
            "Epoch 9/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9767\n",
            "Epoch 9: loss did not improve from 0.08534\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0857 - acc: 0.9767\n",
            "Epoch 10/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0861 - acc: 0.9764\n",
            "Epoch 10: loss did not improve from 0.08534\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.0861 - acc: 0.9764\n",
            "Q: 목 이 뻐근해\n",
            "A: 가끔 씩 스트레칭 을 해주세요 \n",
            "\n",
            "\n",
            "Q: 말투 가 별로 야\n",
            "A: 그러게 말 이에요 \n",
            "\n",
            "\n",
            "Q: 머리 깎아야지\n",
            "A: 예쁘게 깎아요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0853 - acc: 0.9770\n",
            "Epoch 1: loss improved from 0.08534 to 0.08526, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0853 - acc: 0.9770\n",
            "Epoch 2/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0818 - acc: 0.9778\n",
            "Epoch 2: loss improved from 0.08526 to 0.08180, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0818 - acc: 0.9778\n",
            "Epoch 3/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0791 - acc: 0.9789\n",
            "Epoch 3: loss improved from 0.08180 to 0.07905, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0791 - acc: 0.9789\n",
            "Epoch 4/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9790\n",
            "Epoch 4: loss improved from 0.07905 to 0.07607, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0761 - acc: 0.9790\n",
            "Epoch 5/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9797\n",
            "Epoch 5: loss improved from 0.07607 to 0.07515, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0752 - acc: 0.9797\n",
            "Epoch 6/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9797\n",
            "Epoch 6: loss improved from 0.07515 to 0.07454, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0745 - acc: 0.9797\n",
            "Epoch 7/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0743 - acc: 0.9803\n",
            "Epoch 7: loss improved from 0.07454 to 0.07430, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0743 - acc: 0.9803\n",
            "Epoch 8/10\n",
            "124/125 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9805\n",
            "Epoch 8: loss improved from 0.07430 to 0.07304, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0730 - acc: 0.9805\n",
            "Epoch 9/10\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0710 - acc: 0.9806\n",
            "Epoch 9: loss improved from 0.07304 to 0.07103, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0710 - acc: 0.9806\n",
            "Epoch 10/10\n",
            "121/125 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9824\n",
            "Epoch 10: loss improved from 0.07103 to 0.06683, saving model to ./sample-checkpoint.ckpt\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0668 - acc: 0.9825\n",
            "Q: 그만 먹어야 하는데\n",
            "A: 조금 만 드세요 \n",
            "\n",
            "\n",
            "Q: 렌터카 빌릴까봐\n",
            "A: 순서 는 상관 없죠 \n",
            "\n",
            "\n",
            "Q: 내 잘못 인 거 같은데 어떻게 털어놓지\n",
            "A: 사과 할 타이밍 을 놓치지 마세요 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENxB9VmdxD7"
      },
      "source": [
        "## STEP 8. 예측 (prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcTUOIj1_Mc"
      },
      "source": [
        "### 문제 23. `make_question` 함수 정의: 자연어 입력을 데이터 전처리 파이프라인 수행 후 전처리 진행하는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuZ3q1NxdxD8"
      },
      "source": [
        "# 자연어 (질문 입력) 대한 전처리 함수\n",
        "def make_question(sentence):\n",
        "    sentence = clean_and_morph(sentence)\n",
        "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "    return question_padded"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQKCJwtGdxD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa533d6-2b5d-4500-ac59-3e6b4e8c5501"
      },
      "source": [
        "make_question('1 지망 학교 떨어졌어')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1609, 2495, 2496, 1610,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA1iY9q0dxD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcccc14c-56b6-48dc-c8e4-f7a4724818f8"
      },
      "source": [
        "make_question('커피 마시고 싶다.')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   1, 105,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AijEFgfF2Mty"
      },
      "source": [
        "### 문제 24. `run_chatbot`함수를 정의합니다. 질문을 `make_question` 함수 전처리 수행후 모델 예측하여 결과를 다시 자연어로 변환하는 작업을 거치게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q9NlZ8qdxD8"
      },
      "source": [
        "def run_chatbot(question):\n",
        "    question_inputs = make_question(question)\n",
        "    results = make_prediction(seq2seq, question_inputs)\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    return results"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZREkyB-mdxD8"
      },
      "source": [
        "## STEP 9. 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcypsmo2eiI"
      },
      "source": [
        "### 문제 25. 사용자의 입력을 받아 대답을 출력해 주는 프로그램을 완성합니다.\n",
        "\n",
        "유저로부터 Text 입력 값을 받아 답변을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input('\\n말을 걸어 주세요\\n')\n",
        "  if user_input == 'q':\n",
        "    break\n",
        "  answer = run_chatbot(user_input)\n",
        "  print(f'챗봇 응답: {answer}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts3DLKVR68Ve",
        "outputId": "1653c405-db79-4b2b-af19-729efa75a05a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "말을 걸어 주세요\n",
            "배고파\n",
            "챗봇 응답: 밥 사줄 친구 를 찾아 보세요 \n",
            "\n",
            "\n",
            "말을 걸어 주세요\n",
            "딥러닝은 재밌어\n",
            "챗봇 응답: 거짓말 은 할수록 늘어요 \n",
            "\n",
            "\n",
            "말을 걸어 주세요\n",
            "맛있는게 뭐가 있을까\n",
            "챗봇 응답: 제 선물 사오세요 \n",
            "\n",
            "\n",
            "말을 걸어 주세요\n",
            "날씨가 너무 좋아\n",
            "챗봇 응답: 하늘 보고 한 번 웃어 봐요 기분 이 바뀔 거 예요 \n",
            "\n",
            "\n",
            "말을 걸어 주세요\n",
            "이제 운동하러 가야겠어\n",
            "챗봇 응답: 먼저 사람 들 이랑 연락 해서 놀러 가는 건 어때요 \n",
            "\n",
            "\n",
            "말을 걸어 주세요\n",
            "q\n"
          ]
        }
      ]
    }
  ]
}